---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Classification

-----

## Packages used in this chapter

```{r}
library(tidyverse)
library(tidymodels)
library(knitr)
library(kableExtra)
library(skimr)
```

Linear regression in chapter 3 was concerned with predicting a quantitative response variable. What if the response variable is *qualitative*? Eye color is an example of a qualitative variable,  which takes discrete value such as `blue`, `brown`, `green`. These are also referred to as *categorical*.

The approach of predicting qualitative responses is known as *classification*. Often, we predict the probability of the occurences of each category of a qualitative variable, and then make a decision based off of that.

In this chapter we discuss three of the most widely-used classifiers:

* [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)
* [linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)
* [*k*-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)

We discuss more computer-intensive methods in later chapters.

## An Overview of Classification

Classification is a common scenario.

1. Person arrives at ER exhibiting particular symptoms. What illness does he have?
2. Money is wired to an external account at a bank. Is this fraud?
3. Email is sent to your account. Is it legit, or spam?

Similar to regression, we have a set of training observations that use to build a classifier. We also want the classifier to perform well on both training and test observations.

We will use the dataset `ISLR::Default`. First, let's convert it to tidy format.

```{r}
default <- ISLR::Default %>% as_tibble()
```

We are interested in the ability to predict whether an individual will default on their credit card payment, based on their credit card `balance` and annual income.

If we look at the summary statistics, we see the data is clean, and that very few people default on their balances.

```{r}
default %>% skimr::skim()
```

The scatterplot signals a strong relationship between `balance` and `default`.

```{r}
default %>%
  ggplot(aes(x = balance, y = income, fill = default)) + 
  geom_hex(alpha = 2/3)
```

The boxplot captures the stark difference in `balance` between those who default and do not.

```{r}
default %>%
  ggplot(aes(y = balance, fill = default)) +
  geom_boxplot()
```

## Why Not Linear Regression?

Imagine we were trying to predict the medical outcome of a patient on the basis of their symptoms. Let's say there are three possible diagnoses: `stroke`, `overdose`, and `seizure`. We could encode these into a quantitative variable $Y$.  that takes values from 1 to 3. Using least squares, we could then fit a regression model to predict $Y$.

Unfortunately, this coding implies an ordering of the outcomes. It also insists that the difference between levels is quantitative, and equivalent across all sequences of levels.

Thus, changing the order of encodings would change relationship among the conditions, producing fundamentally different linear models.

There could be a case where a response variables took on a natural ordering, such as `mild`, `moderate`, `severe`. We would also need to believe that the gap between each level is equivalent. Unfortunately, there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is appropriate for linear regression.

For cases of *binary* qualitative response, we can utilize the dummy variable solution seen in Chapter 3. In this case, the order of the encodings is arbitrary.

```{r}
# TODO add latex for encoding
```

Linear regression does work for this binary response scenario. However, it is possible for linear regression to produce estimates outside of the `[0, 1]` interval, which affects their interpretability as probabilities.

When the qualitative response has more than two levels, we need to use classification methods that are appropriate.

## Logistic Regression

Let's consider the `default` dataset. Rather than modeling this response $Y$ directly, logistic regression models the *probability* that $Y$ belongs to a particular category.

If we estimate using linear regression, we see that some estimated probabilities are negative. We are using the `tidymodels` package.

```{r}
default <- default %>%
  mutate(default_bool = if_else(default == "Yes", 1, 0))
lm_default <- linear_reg() %>%
  fit(data = default, default_bool ~ balance)

default %>%
  bind_cols(predict(lm_default, default)) %>%
  ggplot(aes(x = balance)) +
  geom_line(aes(y =  .pred)) +
  geom_point(aes(y = default_bool, colour = default_bool)) +
  guides(colour=FALSE)
```

Below is the classification using logistic regression, where are probabilities fall between `0` and `1`.

```{r}
logi_default <- logistic_reg(mode = "classification") %>%
  fit(data = default, as.factor(default_bool) ~ balance)

default %>%
  bind_cols(predict(logi_default, default, type = "prob")) %>%
  ggplot(aes(x = balance)) +
  geom_line(aes(y =  .pred_1)) +
  geom_point(aes(y = default_bool, colour = default_bool)) +
  guides(colour=FALSE)
```

Logistic regression in this example is modelling the probability of default, given the value of `balance`.

<div>
<p style="text-align:center">Pr(`default` = `Yes`|`balance`)</p>
</div>

These values, which we abbreviate as *p*(`balance`), range between `0` and `1`. Logistic regression will always produce an *S-shaped* curve. Regardless of the value of $X$, we will receive a sensible prediction.

From this, we can make a classification prediction for `default`. Depending how conservative we are, the threshold for this could vary. Depending on the domain and context of the classification, a decision boundary around `0.5` or `0.1` might be appropriate.

### The Logistic Model

The problem of using a linear regression model is evident in the chart above, where probabilities can fall below `0` or greater than `1`.

To avoid this, we must model $p(X)$ using a function that gives outputs between `0` and `1` for all values of $X$. In logistic regression, we use the *logistic function*,

<div>
<p style="text-align:center">$p(X) = \frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}$</p>
<p class="vocab" style="text-align:right">*logistic function*</p>
</div>

To fit the model, we use a method called *maximum likelihood*. 

If we manipulate the logistic function, we find that

<div>
<p style="text-align:center">$\frac{p(X)}{1-p(X)} = e^{\beta_0+\beta_1X}$</p>
<p class="vocab" style="text-align:right">*odds*</p>
</div>

This is called the *odds*, and takes any value from $0$ to $\infty$. This is the same type of odds used in sporting events ("9:1 odds to win this match", etc). If $p(X) = 0.9$, then odds are $\frac{0.9}{1-0.9} = 9$.

If we take the logarithm of the odds, we arrive at

<div>
<p style="text-align:center">$log(\frac{p(X)}{1-p(X)}) = \beta_0+\beta_1X$</p>
<p class="vocab" style="text-align:right">*log-odds \ logit*</p>
</div>

The left-hande side is called the *log-odds* or *logit*. The logistic regression model has a logit that is linear in $X$.

The contrast to linear regression is that increasing $X$ by one-unit changes the log odds by $\beta_1$ (or the odds by $e^{\beta_1}$. However, since $p(X)$ and $X$ relationship is not a straight line (see plot above), $\beta_1$ does not correspond to the the change in $p(X)$ associated with a one-unit increase in $X$. The amount that $p(X)$ changes depends on the current value of $X$. See how the slope approaches `0` more and more slowly as `balance` increases. 

Regardless of how much $p(X)$ moves, if $\beta_1$ is positive then increasing $X$ will be associated with increasing $p(X)$. The opposite is also true. 

### Estimating the Regression Coefficients

The coefficients in the logistic regression equation must be estimated used training data. Linear regression used the least squares approach to estimate the coefficients. It is possible to use non-linear least squares to fit the model, but *maximum likelihood* is preferred. 

Maximum likelihood seeks to to find estimates for $\beta_0$ and $\beta_1$ such that the predicted probability $\hat{p}(x_i)$ of default for each individual corresponds as closely as possible to to the individual's observed default status. We want estimates that produce low probabilities for individuals who did not default, and high probabilities for those who did.

We can formalize this with a *likelihood function*:

```{r}
#TODO add likelihood function, links, etc
```

We can examine the coefficients and other information from our logistic regression model. 

```{r}
logi_default %>% broom::tidy()
```

If we look at the terms of our logistic regression, we see that the coefficient for `balance` is positive. This means that higher `balance` increases $p(Default)$. A one-unit increase in `balance` will increase the log odds of defaulting by ~0.0055. 

The test-statistic also behaves similarly. Coefficients with large statistics indicate evidence against the null hypothesis $H_0: \beta_1 = 0$. For logistic regression, the null hypothesis implies that $p(X) = \frac{e^{\beta_0}}{1+e^{\beta_0}}$, which means that the probability of defaulting does not depend on `balance.`

Given the miniscule p-value associated with our `balance` coefficient, we can confidently reject $H_0$. The intercept ($\beta_0$) is typically not of interest; it's main purpose is to adjust the average fitted probabilities to the proportion of ones in the data.

### Making Predictions

Once we have the coefficients, we simply compute the probability of `default` for any given observation.

Let's take an individual with a `balance` of `$1000`. Using our model terms, we can compute the probability. Let's extract the terms from the model and plug in a `balance` of `$1000`.

```{r}
logi_coef <- logi_default %>%
  broom::tidy() %>%
  # widen it and clean up names
  select(term, estimate) %>%
  pivot_wider(names_from = term, values_from = estimate) %>%
  janitor::clean_names()

logi_coef %>%
  mutate(prob_1000 = exp(intercept + balance * 1000) /
           (1 + exp(intercept + balance * 1000)))
```

We find the probability to be less than `1%`.

We can also incorporate qualitative predictors with the logistic regression model. Here we encode `student` in to the model.

```{r}
logi_default_student <- logistic_reg(mode = "classification") %>%
  fit(data = default, as.factor(default_bool) ~ student)

logi_default_student %>% broom::tidy()
```

This model indicates that students have a higher rate of defaulting compared to non-students.

### Multiple Logistic Regression

We now consider the scenario of multiple predictors.

We can rewrite $p(X)$ as

<div>
<p style="text-align:center">$p(X) = \frac{e^{\beta_0+\beta_1X_1+...+\beta_pX_p}}{1+e^{\beta_0+\beta_1X_1+...+\beta_pX_p}}$</p>
<p class="vocab" style="text-align:right">/p>
</div>

And again use the maximum likelihood method to estimate the coefficients.

Let's estimate `balance` using `balance`, `income` and `student`.

```{r}
multiple_logi_default<- logistic_reg(mode = "classification") %>%
  fit(data = default, as.factor(default_bool) ~ balance + student + income)

multiple_logi_default %>% broom::tidy()
```

Notice that being a student now *decreases* the chances of default, whereas in our previous model (which only contained `student` as a predictor), it increased the chances.

Why is that? This model is showing that, for a fixed value of `income` and `balance`, students actually default less. This is because `student` and `balance` are correlated.

```{r}
default %>%
  ggplot(aes(y = balance, fill = student)) +
  geom_boxplot()
```

If we plot the distribution of `balance` across `student`, we see that students tend to carry larger credit card balances. 

This example illustrates the dangers of drawing insights from single predictor regressions when other predictors may be relevant. The results from using one predictor can be substantially different compared to using multiple predictors. This phenomenon is known as *confounding*.

### Logistic Regression for >2 Response Classes

Sometimes we wish to classify a response variable that has more than two classes. This could be the medical example where a patient outcomes falls into `stroke`, `overdose`, and `seizure`. It is possible to extend the two-class logistic regression model into multiple-class, but this is not used often in practice.

A method that is popular for multi-class classification is *discriminant analysis*.

## Linear Discriminant Analysis

Logistic regression models the distribution of response $Y$ given the predictor(s) $X$. In discriminant analysis, we model the distribution of the predictors $X$ in each of the response classes, and then use Bayes' theorem to flip these around into estimates for $Pr(Y = k|X = x)$. 

Why do we need this method?

* Well-separated classes produce unstable parameter estimates for logistic regression models

* If $n$ is small and distribution of predictors $X$ is normall across the classes, the linear discriminant model is more stable than logistic regression

### Using Bayes' Theorem for Classification

Consider the scenario where we want to classify an observation into one of $K$ classes, where $K >= 2$. 

* Let $\pi_k$ represent the overall or *prior* probability that a randomly chosen observation comes from the $k$th class
* Let $f_k(x) = Pr(X = x|Y = k)$ denote the *density function* of $X$ for an observation that comes from the $k$th class.

In other words, $f_k(x)$ being large means that there is a high probability that an observation in the $k$th class has $X \approx x$.

We can use Bayes' theorem

$$
\operatorname{Pr}(Y=k | X=x)=\frac{\pi_{k} f_{k}(x)}{\sum_{l=1}^{K} \pi_{l} f_{l}(x)}
$$

And call the left-hand side $p_k(X)$. We can plug in estimates of $\pi_k$ and $f_k(X)$ into Bayes' theorem above to get the probability of a certain class, given an observation.

* Solving for $\pi_k$ is easy if we have a random sample of $Y$s from the population. We simply calculate the fraction of observations that fall into a $k$ class.
* Estimating $f_k(X)$ is more challenging unless we assume simple forms for these densities

We refer to $p_k(x)$ as the posterior probability that an observation $X = x$ belongs to the $k$th class. This is the probability that the observation belongs to the $k$th class, *given* the predictor value for that observation.

The Bayes' classifier classifies an observation to the class for which $p_k(X)$ is largest. If we can find a way to estimate $f_k(X)$, we can develop a classifier that approximates the Bayes classifier.

### Linear Discriminant Analysis for p = 1

Let's assume we have one predictor. We need to obtain an estimate for $f_k(x)$ (the density function for $X$ given a class $k$). This will obtain a value for $p_k(x)$. We will then classify this observation for which $p_k(x)$ is greatest.

To estimate $f_k(x)$, we need to make some assumptions about its form.

Let's assume $f_k(x)$ is *normal* or *Gaussian*. The normal density takes the form

$$
f_{k}(x)=\frac{1}{\sqrt{2 \pi} \sigma_{k}} \exp \left(-\frac{1}{2 \sigma_{k}^{2}}\left(x-\mu_{k}\right)^{2}\right)
$$

Plugging this back in to $p_k(x)$, we obtain

$$
p_{k}(x)=\frac{\pi_{k} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2 \sigma^{2}}\left(x-\mu_{k}\right)^{2}\right)}{\sum_{l=1}^{K} \pi_{l} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2 \sigma^{2}}\left(x-\mu_{l}\right)^{2}\right)}
$$

Taking the log and rearranging results in 

$$
\delta_{k}(x)=x \cdot \frac{\mu_{k}}{\sigma^{2}}-\frac{\mu_{k}^{2}}{2 \sigma^{2}}+\log \left(\pi_{k}\right)
$$

In this case, the Bayes decision boundary corresponds to 

$$
x=\frac{\mu_{1}^{2}-\mu_{2}^{2}}{2\left(\mu_{1}-\mu_{2}\right)}=\frac{\mu_{1}+\mu_{2}}{2}
$$

We can simulate some data to show a simple example.

In this data we have two classes:

* $\mu_1 = -1.25, \mu_2 = 1.25, \sigma_1^2 = \sigma_2^2 = 1$

```{r}


var_1 = 1
var_2 = var_1
f_1 = tibble(fun = "f_1", x = rnorm(n = 10000, mean = -1.25, sd = var_1))
f_2 = tibble(fun = "f_2", x = rnorm(n = 10000, mean = 1.25, sd = var_2))
f_x = bind_rows(f_1, f_2)

# add summary statistics

f_x <- f_x %>%
  group_by(fun) %>%
  mutate(pi = n(),
         var = var(x),
         mu = mean(x)) %>%
  ungroup() %>%
  mutate(pi = pi / n())

decision_boundary <- f_x %>%
  group_by(fun) %>%
  summarise(mu = mean(x)) %>%
  summarise(decision_boundary = sum(mu) / 2) %>%
  pull()
f_x %>%
  ggplot(aes(x = x, colour = fun)) +
  geom_density() +
  geom_vline(xintercept = decision_boundary, linetype = "dashed")
```

These two densities overlap, and so given $X = x$, we still have uncertaintly about which class the observation belongs to. If both classes are equally likely for a random observation $\pi_1 = \pi_2$, then we see the Bayes classifier assigns the observation to class 1 if $x < 0$ and class 2 otherwise.

Even if we are sure that $X$ is drawn from a Gaussian distribution within each class, we still need to estimate $\mu_1,...,\mu_k$, $\pi_1,...,\pi_k$, and $\sigma^2$. The *linear discriminant analysis* method approximates these by plugging in estimates as follows

<div>
<p style="text-align:center">$\hat{\mu}_k = \frac{1}{n_k}\sum_{i:y_i=k}{x_i}$</p>
<p class="vocab" style="text-align:right">*</p>
</div>

<div>
<p style="text-align:center">$\hat{\sigma}^2 = \frac{1}{n-K}\sum_{k=1}^{K}\sum_{i:y_i=k}{(x_i-\hat{\mu}_k)^2}$</p>
<p class="vocab" style="text-align:right">*</p>
</div>

The estimate for $\hat{\mu}_k$ is the average of all training observations from the $k$th class. The estimate for $\hat{\sigma}^2$ is the weighted average of the sample variances for each of the K classes.

To estimate $\hat{\pi}_k$, we simply take the proportion of training observations that belong to the $k$th class

<div>
<p style="text-align:center">$\hat{\pi}_k = n_k/n$</p>
<p class="vocab" style="text-align:right">*</p>
</div>

From these estimates, we can achieve a decision boundary

$$
\hat{\delta}_{k}(x)=x \cdot \frac{\hat{\mu}_{k}}{\hat{\sigma}^{2}}-\frac{\hat{\mu}_{k}^{2}}{2 \hat{\sigma}^{2}}+\log \left(\hat{\pi}_{k}\right)
$$

This classifier has *linear* in the name due to the fact that the *discriminant function* above are linear functions of $x$.

Let's take a sample from our earlier distribution and see how it performs.

```{r}
library(discrim)
f_sample = f_x %>% sample_frac(size = 0.01)

lda_f <- discrim::discrim_linear() %>%
  fit(data = f_sample, as.factor(fun) ~ x)

preds <- predict(lda_f, f_sample, type = "class")

f_sample <- f_sample %>% bind_cols(preds)
# TODO figure out how to truly extract decision boundary from MASS::lda
est_decision <- f_sample %>% arrange(x) %>% filter(.pred_class == 'f_2') %>%
  slice(1) %>% pull(x)

ggplot(f_sample, aes(x = x, fill = fun)) +
  geom_histogram() +
  geom_vline(xintercept = est_decision, linetype = "dashed") +
  geom_vline(xintercept = 0)
```

Notice the estimated decision boundary (dashed line) being very close to the Bayes decision boundary.

#### Measuring Performance

```{r, eval = FALSE}
# TODO show lda performance compared to true value
```

### Linear Discriminant Analysis for p > 1

We can extend LDA classifier to multiple predictors.

The multivariate Gaussian distribution assumes that each predictor follows a one-dimensional normal distribution, with some correlation between each pair of predictors.

* [Andrew Ng on Multivariate Gaussian Distribution](https://www.youtube.com/watch?v=JjB58InuTqM)

To indicate that a $p$-dimensional random variable $X$ has a multi-variate Gaussian distribution, we write $ X \sim N(\mu, \Sigma)$

* $E(X) = \mu$ is the mean of $X$ (a vector with $p$ components)
* $Cov(X) = \Sigma$ is the $p*p$ covariance matrix of $X$.

The multivariate Gaussian density is defined as

$$
f(x)=\frac{1}{(2 \pi)^{p / 2}|\mathbf{\Sigma}|^{1 / 2}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \mathbf{\Sigma}^{-1}(x-\mu)\right)
$$

In the case of $p>1$ predictors, the LDA classifier assumes that the observations in the $k$th class are drawn from a multivariate Gaussian distribution $N(\mu_k, \Sigma)$, where $\mu_k$ is a class-specific mean vector, and $\Sigma$ is the covariance matrix that is common to all $K$ classes.

Plugging the density function for the $k$th class, $f_k(X = x)$, into 

$$
\operatorname{Pr}(Y=k | X=x)=\frac{\pi_{k} f_{k}(x)}{\sum_{l=1}^{K} \pi_{l} f_{l}(x)}
$$

and performing some algebra reveals that the Bayes classifier will assign observation $X = x$ by identifying the class for which

$$
\delta_{k}(x)=x^{T} \boldsymbol{\Sigma}^{-1} \mu_{k}-\frac{1}{2} \mu_{k}^{T} \boldsymbol{\Sigma}^{-1} \mu_{k}+\log \pi_{k}
$$

is largest.

#### Performing LDA on Default data

If we run an LDA model on our `default` dataset, predicting the probability of `default` based off of `student` and `balance`, we achieve a respectable `3.0%` error rate.

```{r}
set.seed(1)
default_split <- initial_split(default, prop = 3/4)
train_default <- training(default_split)
test_default <- testing(default_split)

lda_default <- discrim::discrim_linear() %>%
  fit(data = train_default, default ~ student + balance)

preds <- predict(lda_default, test_default, type = "class")

# error rate
test_default %>%
  bind_cols(preds) %>%
  metrics(truth = default, estimate = .pred_class)
```

While this may seem impressive, let's remember that only `3.6%` of observations in the dataset end up in default. This means that if we assigned a *null* classifier, which simply predicted every observation to not end in default, our error rate would be `3.6%`. This is worse, but not by much, compared to our LDA error rate.

```{r}
# null error rate
test_default %>%
  group_by(default) %>%
  count() %>%
  ungroup() %>%
  mutate(prop = n / sum(n))
```

Binary decision makers can make to types of errors:

* Incorrectly assigning an individual who defaults to the "no default" category
* Incorrectly assigning an individual who doesn't default to the "default" category.

We can identify the breakdown by using a *confusion matrix*

```{r}
cm <- test_default %>%
  bind_cols(preds) %>%
  conf_mat(truth = default, estimate = .pred_class)

cm
```

We see that our LDA only predicted `31` people to default. Of these, `23` actually defaulted. So, only `8` of out of the `~7500` people who did not default were incorrectly labeled.

However, of the `90` people in our test set who defaulted, we only predicted this correctly for `23` of them. That means `~75%` of individuals who default were incorrectly classified. Having an error rate this high for the problematic class (those who default) is unacceptable.

Class-specific performance is an important concept. *Sensitivity* and *specificity* characterize the performance of a classifier or screening test. In this case, the sensitivity is the percentage of true defaults who are identified (a low `~25%`). The specificity is the percentage of non-defaulters who are correctly identified (`7492/7500 ~ 99.9%`).

Remember that LDA is trying to approximate the Bayes classifier, which has the lowest *total* error rate out of all classifiers (assuming Gaussian assumption is correct). The classifier will yield the smallest total number of misclassifications, regardless of which class the errors came from. In this credit card scenario, the credit card company might wish to avoid incorrectly misclassifying a user who defaults. In this case, they value sensitivity. For them, the cost of misclassifying a defaulter is higher than the cost of misclassifying a non-defaulter (which they still desire to avoid).

It's possible to modify LDA for such circumstances. Given the Bayes classifier works by assigning an observation to a class in which the posterior probability $p_k(X)$ is greatest (in the two-class scenario, this decision boundary is at `0.5`), we can modify the probability threshold to suit our needs. If we wish to increase our sensitivity, we can lower this threshold.

Imagine we lowered the threshold to `0.2`. Sure, we would classify more people as defaulters than before (decreasing our specificity) but we would also catch more defaulters we previously missed (increasing our sensitivity).

```{r}
preds <- predict(lda_default, test_default, type = "prob")

# error rate
test_default %>%
  bind_cols(preds) %>%
  mutate(.pred_class = as.factor(if_else(.pred_Yes > 0.2, "Yes", "No"))) %>%
  conf_mat(truth = default, estimate = .pred_class)
```

Now our sensitivy has increased. Of the `90` people who defaulted, we correctly identified `53, or ~58.8%` of them (up from `~25%` previously).

This came at a cost, as our specificity decreased. This time, we predicted `106` people to default. Of those, `53` actually defaulted. This means that `53` of the `7500` people who didn't default were incorrectly labelled. This gives us a specificity of (`7447/7500 ~ 99.2%`)

Despite the overall increase in error rate, the lower threshold may be chosen, depending on the context of the problem. To make a decision, an extensive amount of *domain knowledge* is required.

The *ROC curve* is a popular graphic for displaying the two types of errors for all possible thresholds. "ROC" stands for *receiver operating characteristics*.

The overall performance of a classifier, summarized over all possible thresholds, is given by the *area under the (ROC) curve* (AUC). An ideal ROC curve will hug the top left corner. Think of it this way: ideal ROC curves are able to increase sensitivity at a much higher rate than reduction in specificity.

We can use `yardstick::` (part of `tidymodels::`) to plot an ROC curve.

```{r}
test_default %>%
  bind_cols(preds) %>%
  roc_curve(default, .pred_Yes) %>%
  autoplot()
```

We can think of the *sensitivity* as the true positive, and *1 - specificity* as the false positive.

### Quadratic Discriminant Analysis

LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution, with a class-specific mean vector and a covariance matrix that is common to all $K$ classes. *Quadratic discriminant analysis* (QDA) assumes that class has its own covariance matrix. 

It assumes that each observation from the $k$th class has the form $X \sim N(\mu_k, \Sigma_k)$, where $\Sigma_k$ is a covariance matrix for the $k$th class. Under this assumption, the Bayes classifier assigns an observation $X=x$ to the class for which

$$
\begin{aligned} \delta_{k}(x) &=-\frac{1}{2}\left(x-\mu_{k}\right)^{T} \boldsymbol{\Sigma}_{k}^{-1}\left(x-\mu_{k}\right)-\frac{1}{2} \log \left|\boldsymbol{\Sigma}_{k}\right|+\log \pi_{k} \\ &=-\frac{1}{2} x^{T} \boldsymbol{\Sigma}_{k}^{-1} x+x^{T} \boldsymbol{\Sigma}_{k}^{-1} \mu_{k}-\frac{1}{2} \mu_{k}^{T} \boldsymbol{\Sigma}_{k}^{-1} \mu_{k}-\frac{1}{2} \log \left|\boldsymbol{\Sigma}_{k}\right|+\log \pi_{k} \end{aligned}
$$

is largest. In this case, we plug in estimates for $\Sigma_k$, $\mu_k$, and $\pi_k$. Notice the quantity $x$ appears as a quadratic function, hence the name.

So why would one prefer LDA to QDA, or vice-versa? We again approach the bias-variance trade-off. With $p$ predictors, estimating a class-independent covariance matrix requires estimating $p(p+1)/2$ parameters. For example, a covariance matrix with `4` predictors would require estimating `4(4+1)/2 = 10` parameters. To estimate a covariance matrix for each class, the number of parameters is $Kp(p+1)/2$ paramters. With `50` predictors, this becomes some multiple of `1,275`, depending on $K$. The assumption of the common covariance matrix in LDA causes the model to become linear in $x$, which means there are $Kp$ linear coefficients to estimate. As a result, LDA is much less flexible clasifier than QDA, and has lower variance.

```{r, eval = FALSE}
# TODO understand the parameter calculation
```

The consequence of this is that if LDA's assumption of a common covariance matrix is significantly off, the LDA can suffer from high bias. In general, LDA tends to be a better bet than QDA when there are relatively few training observations and so reduction of variance is crucial. In contrast, with large data sets, QDA can be recommended as the variance of the classifier is not a major concern, or the assumption of a common covariance matrix for the $K$ classes is clearly not correct.

Breaking the assumption of a common covariance matrix can "curve" the decision boundary, and so the use of a more flexible model (QDA) could yield better results.

```{r}
# TODO add QDA example
```

## A Comparison of Classification Methods

Let's discuss the classification methods we have considered and the scenarios for which one might be superior.

* Logistic regression
* LDA
* QDA
* K-nearest neighbors

There is a connection between LDA and logistic regression, particularyly in the two-class setting with $p=1$ predictor. The difference being that logistic regression estimates coefficients via maximum likelihood, and LDA uses the estimated mean and variance from a normal distribution.

The similarity in fitting procedure means that LDA and logistic regression often give similar results. When the assumption that observations are drawn from a Gaussian distribution with a common covariance matrix in each class are in fact true, the LDA can perform better than logistic regression. If the assumptions are in fact false, logistic regression can outperform LDA.

KNN, on the other hand, is completely non-parametric. KNN looks at observations "closest" to $x$, and assigns it to the class to which the plurality of these observations belong. No assumptions are made about the shape of the decision boundary. We can expect KNN to outperform both LDA and logistic regression when the decision boundary is highly non-linear. A downside of KNN, even when it does outperform, is its lack of interpretability. KNN does not tell us which predictors are important.

QDA serves as a compromise between the non-parametric KNN method and the linear LDA and logistic regression approaches. The assumption of quadratic decision boundary allows it to accurately model a wider range of problems. It's reduced flexibility compared to KNN allows it to produce a lower variance with a limited number of training observations due to it making some assumptions about the form of the decision boundary.

```{r}
# TODO compare all on different dataset using tidymodels and broom
```

## Lab: Logistic Regression, LDA, QDA, and KNN

### The Stock Market Data

We will look at `ISLR::Smarket` dataset, which consists of S&P 500 returns from 2001 through 2005.

```{r}
smarket <- ISLR::Smarket %>% as_tibble() %>% janitor::clean_names()
smarket
```

```{r}
smarket %>% skimr::skim()
```

### Logistic Regression

Let's run a logistic regression to predict `direction` using `lag1` through `lag5` and `volume`.

Unlike ISLR, we will use the `parsnip::logistic_reg` function over `glm` due to its API design and machine learning workflow provided by its parent package, `tidymodels`. Models in the `{parsnip}` package also allow for choice of different computational engines. This reduces cognitive overhead by standardizing the high-level arguments for training a model without rembembering the specifications of different engine. In our case, we will be using the `glm` engine.

```
logistic_reg() is a way to generate a specification of a model before fitting and allows the model to be created using different packages in R, Stan, keras, or via Spark.
```

```{r}
set.seed(1)
logi_market <- logistic_reg(mode = "classification") %>%
  fit(data = smarket %>% select(-year, -today), direction ~ .)

logi_market %>% broom::tidy() %>% arrange(p.value)
```

The model shows that the smallest p-value is associated with `lag` (although the p-value is still relatively large). If we had to interpret the likely faulty model, the negative coefficient suggests that if the market was up yesterday, it's less likely to go up today. 

We can still use the `predict()` function with our `{tidymodels}` workflow. The `type` parameter specifies whether we want probabilities or classifications returned. The object returned is a tibble with columns of the predicted probability of the observation being in each class.

```{r}
probs <- predict(object = logi_market, new_data = smarket, type = "prob")
smarket %>%
  bind_cols(probs)
```

ISLR recommends creating classification labels by hand, but we can also utilize `predict()` for this.

```{r}
preds <- predict(object = logi_market, new_data = smarket, type = "class")
smarket %>%
  bind_cols(preds)
```

Again, we can produce a confusion matrix using `conf_mat()` function of the `{yardstick}` package (used for measuring model performance, also part of `{tidymodels}`).

We tell `conf_mat()` that the `direction` column is our source of truth, and our classifications are contained in the `.pred_class` column.

```{r}
cm_market <- smarket %>%
  bind_cols(preds) %>%
  conf_mat(truth = direction, estimate = .pred_class)

cm_market
```

`conf_mat` objects also have a `summary()` method that computes various classification metrics.

```{r}
summary(cm_market) %>%
  filter(.metric %in% c('accuracy', 'sens', 'spec'))
```

Our overall accuracy is around `~52%`. This may seem decent, but if we look at the original `smarket` data, we see that the `direction` value is evenly split (market goes up `51.8%` of the time). Still, it suggests that we are slightly better off than random guessing.

However, we need to consider that this accuracy is on the *training data*, which is often overly optimistic. To get a better sense of our model performance, we need to train on a subset of the data, and test on the remaining subset.

We will train on data from `2001` to `2004` and then test on data from `2005`.

```{r}
set.seed(40)
# prepare training/test splits
smarket_split <- initial_split(smarket, prop = 3/4)
train_smarket <- training(smarket_split)
test_smarket <- testing(smarket_split)

# fit model and grab predictions
logi_market <- logistic_reg(mode = "classification") %>%
  fit(data = train_smarket %>% select(-year, -today), direction ~ .)
preds <- predict(object = logi_market, new_data = test_smarket, type = "class")

# build confusion matrix on test set
test_cm <- test_smarket %>%
  bind_cols(preds) %>%
  conf_mat(truth = direction, estimate = .pred_class)

# summary statistics
summary(test_cm) %>%
  filter(.metric %in% c('accuracy', 'sens', 'spec'))
```

In this scenario, the accuracy is `47.8%`. Certain iterations of this *could* produce a lower test error due to randomness + small sample size, but each iteration tends to be lower than the previous, no-split example. Obviously, this example shows that applying classification models doesn't always yield a robust model. Having the ability to predict the stock market with a dozen lines of code isn't realistic.

### Linear Discriminant Analysis

