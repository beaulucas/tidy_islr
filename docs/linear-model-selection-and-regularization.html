<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Linear Model Selection And Regularization | A Tidy Introduction To Statistical Learning</title>
  <meta name="description" content="Chapter 6 Linear Model Selection And Regularization | A Tidy Introduction To Statistical Learning" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Linear Model Selection And Regularization | A Tidy Introduction To Statistical Learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Linear Model Selection And Regularization | A Tidy Introduction To Statistical Learning" />
  
  
  

<meta name="author" content="Beau Lucas" />


<meta name="date" content="2020-03-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="resampling-methods.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Tidy Introduction To Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#an-overview-of-statistical-learning"><i class="fa fa-check"></i><b>1.1</b> An Overview of Statistical Learning</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#data-sets-used-in-labs-and-exercises"><i class="fa fa-check"></i><b>1.2</b> Data Sets Used in Labs and Exercises</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#book-resources"><i class="fa fa-check"></i><b>1.3</b> Book Resources:</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-learning.html"><a href="statistical-learning.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#packages-used-in-this-chapter"><i class="fa fa-check"></i><b>2.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#what-is-statistical-learning"><i class="fa fa-check"></i><b>2.2</b> What is Statistical Learning?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#why-estimate-f"><i class="fa fa-check"></i><b>2.2.1</b> Why Estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#how-do-we-estimate-f"><i class="fa fa-check"></i><b>2.2.2</b> How do we estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-trade-off-between-prediction-accuracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.2.3</b> The Trade-Off Between Prediction Accuracy and Model Interpretability</a></li>
<li class="chapter" data-level="2.2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#supervised-versus-unsupervised-learning"><i class="fa fa-check"></i><b>2.2.4</b> Supervised Versus Unsupervised Learning</a></li>
<li class="chapter" data-level="2.2.5" data-path="statistical-learning.html"><a href="statistical-learning.html#regression-versus-classification-problems"><i class="fa fa-check"></i><b>2.2.5</b> Regression Versus Classification Problems</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.3</b> Assessing Model Accuracy</a><ul>
<li class="chapter" data-level="2.3.1" data-path="statistical-learning.html"><a href="statistical-learning.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.3.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.3.2" data-path="statistical-learning.html"><a href="statistical-learning.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>2.3.2</b> The Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="2.3.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-classification-setting"><i class="fa fa-check"></i><b>2.3.3</b> The Classification Setting</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#lab-introduction-to-r"><i class="fa fa-check"></i><b>2.4</b> Lab: Introduction to R</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-learning.html"><a href="statistical-learning.html#exercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#packages-used-in-this-chapter-1"><i class="fa fa-check"></i><b>3.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>3.2.1</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimate"><i class="fa fa-check"></i><b>3.2.2</b> Assessing the Accuracy of the Coefficient Estimate</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-model"><i class="fa fa-check"></i><b>3.2.3</b> Assessing the Accuracy of the Model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>3.3.1</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-regression.html"><a href="linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>3.3.2</b> Some Important Questions</a></li>
<li class="chapter" data-level="3.3.3" data-path="linear-regression.html"><a href="linear-regression.html#other-considerations-in-the-regression-model"><i class="fa fa-check"></i><b>3.3.3</b> Other Considerations in the Regression Model</a></li>
<li class="chapter" data-level="3.3.4" data-path="linear-regression.html"><a href="linear-regression.html#qualitative-predictors-with-more-than-two-levels"><i class="fa fa-check"></i><b>3.3.4</b> Qualitative Predictors with More than Two Levels</a></li>
<li class="chapter" data-level="3.3.5" data-path="linear-regression.html"><a href="linear-regression.html#extensions-of-the-linear-model"><i class="fa fa-check"></i><b>3.3.5</b> Extensions of the Linear Model</a></li>
<li class="chapter" data-level="3.3.6" data-path="linear-regression.html"><a href="linear-regression.html#potential-problems"><i class="fa fa-check"></i><b>3.3.6</b> Potential Problems</a></li>
<li class="chapter" data-level="3.3.7" data-path="linear-regression.html"><a href="linear-regression.html#the-marketing-plan"><i class="fa fa-check"></i><b>3.3.7</b> The Marketing Plan</a></li>
<li class="chapter" data-level="3.3.8" data-path="linear-regression.html"><a href="linear-regression.html#comparison-of-linear-regression-with-k-nearest-neighbors"><i class="fa fa-check"></i><b>3.3.8</b> Comparison of Linear Regression with <em>K</em>-Nearest Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-regression.html"><a href="linear-regression.html#lab-linear-regression"><i class="fa fa-check"></i><b>3.4</b> Lab: Linear Regression</a><ul>
<li class="chapter" data-level="3.4.1" data-path="linear-regression.html"><a href="linear-regression.html#fitting-a-linear-regression"><i class="fa fa-check"></i><b>3.4.1</b> Fitting a linear regression</a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression-1"><i class="fa fa-check"></i><b>3.4.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="3.4.3" data-path="linear-regression.html"><a href="linear-regression.html#interaction-terms"><i class="fa fa-check"></i><b>3.4.3</b> Interaction Terms</a></li>
<li class="chapter" data-level="3.4.4" data-path="linear-regression.html"><a href="linear-regression.html#non-linear-transformations-of-the-predictors"><i class="fa fa-check"></i><b>3.4.4</b> Non-linear Transformations of the Predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="linear-regression.html"><a href="linear-regression.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>4</b> Classification</a><ul>
<li class="chapter" data-level="4.1" data-path="classification.html"><a href="classification.html#packages-used-in-this-chapter-2"><i class="fa fa-check"></i><b>4.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="4.2" data-path="classification.html"><a href="classification.html#an-overview-of-classification"><i class="fa fa-check"></i><b>4.2</b> An Overview of Classification</a></li>
<li class="chapter" data-level="4.3" data-path="classification.html"><a href="classification.html#why-not-linear-regression"><i class="fa fa-check"></i><b>4.3</b> Why Not Linear Regression?</a></li>
<li class="chapter" data-level="4.4" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>4.4</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.4.1" data-path="classification.html"><a href="classification.html#the-logistic-model"><i class="fa fa-check"></i><b>4.4.1</b> The Logistic Model</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification.html"><a href="classification.html#estimating-the-regression-coefficients-1"><i class="fa fa-check"></i><b>4.4.2</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="4.4.3" data-path="classification.html"><a href="classification.html#making-predictions"><i class="fa fa-check"></i><b>4.4.3</b> Making Predictions</a></li>
<li class="chapter" data-level="4.4.4" data-path="classification.html"><a href="classification.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>4.4.4</b> Multiple Logistic Regression</a></li>
<li class="chapter" data-level="4.4.5" data-path="classification.html"><a href="classification.html#logistic-regression-for-2-response-classes"><i class="fa fa-check"></i><b>4.4.5</b> Logistic Regression for &gt;2 Response Classes</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>4.5</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="4.5.1" data-path="classification.html"><a href="classification.html#using-bayes-theorem-for-classification"><i class="fa fa-check"></i><b>4.5.1</b> Using Bayes’ Theorem for Classification</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1"><i class="fa fa-check"></i><b>4.5.2</b> Linear Discriminant Analysis for p = 1</a></li>
<li class="chapter" data-level="4.5.3" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1-1"><i class="fa fa-check"></i><b>4.5.3</b> Linear Discriminant Analysis for p &gt; 1</a></li>
<li class="chapter" data-level="4.5.4" data-path="classification.html"><a href="classification.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>4.5.4</b> Quadratic Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="classification.html"><a href="classification.html#a-comparison-of-classification-methods"><i class="fa fa-check"></i><b>4.6</b> A Comparison of Classification Methods</a></li>
<li class="chapter" data-level="4.7" data-path="classification.html"><a href="classification.html#lab-logistic-regression-lda-qda-and-knn"><i class="fa fa-check"></i><b>4.7</b> Lab: Logistic Regression, LDA, QDA, and KNN</a><ul>
<li class="chapter" data-level="4.7.1" data-path="classification.html"><a href="classification.html#churn-dataset"><i class="fa fa-check"></i><b>4.7.1</b> Churn Dataset</a></li>
<li class="chapter" data-level="4.7.2" data-path="classification.html"><a href="classification.html#logistic-regression-1"><i class="fa fa-check"></i><b>4.7.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.7.3" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-1"><i class="fa fa-check"></i><b>4.7.3</b> Linear discriminant analysis</a></li>
<li class="chapter" data-level="4.7.4" data-path="classification.html"><a href="classification.html#k-nearest-neighbors-1"><i class="fa fa-check"></i><b>4.7.4</b> K-Nearest Neighbors</a></li>
<li class="chapter" data-level="4.7.5" data-path="classification.html"><a href="classification.html#choosing-the-model"><i class="fa fa-check"></i><b>4.7.5</b> Choosing the model</a></li>
<li class="chapter" data-level="4.7.6" data-path="classification.html"><a href="classification.html#evaluating-the-threshold"><i class="fa fa-check"></i><b>4.7.6</b> Evaluating the threshold</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="classification.html"><a href="classification.html#conclusion"><i class="fa fa-check"></i><b>4.8</b> Conclusion</a></li>
<li class="chapter" data-level="4.9" data-path="classification.html"><a href="classification.html#exercises-2"><i class="fa fa-check"></i><b>4.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="resampling-methods.html"><a href="resampling-methods.html"><i class="fa fa-check"></i><b>5</b> Resampling Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation"><i class="fa fa-check"></i><b>5.1</b> Cross-Validation</a><ul>
<li class="chapter" data-level="5.1.1" data-path="resampling-methods.html"><a href="resampling-methods.html#the-validation-set-approach"><i class="fa fa-check"></i><b>5.1.1</b> The Validation Set Approach</a></li>
<li class="chapter" data-level="5.1.2" data-path="resampling-methods.html"><a href="resampling-methods.html#leave-one-out-cross-validation."><i class="fa fa-check"></i><b>5.1.2</b> Leave-One-Out Cross-Validation.</a></li>
<li class="chapter" data-level="5.1.3" data-path="resampling-methods.html"><a href="resampling-methods.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.1.3</b> k-Fold Cross-Validation</a></li>
<li class="chapter" data-level="5.1.4" data-path="resampling-methods.html"><a href="resampling-methods.html#bias-variance-trade-off-for-k-fold-cross-validation"><i class="fa fa-check"></i><b>5.1.4</b> Bias-Variance Trade-Off for <em>k</em>-fold Cross-Validation</a></li>
<li class="chapter" data-level="5.1.5" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation-on-classification-problems"><i class="fa fa-check"></i><b>5.1.5</b> Cross-Validation on Classification Problems</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="resampling-methods.html"><a href="resampling-methods.html#the-bootstrap"><i class="fa fa-check"></i><b>5.2</b> The Bootstrap</a></li>
<li class="chapter" data-level="5.3" data-path="resampling-methods.html"><a href="resampling-methods.html#lab"><i class="fa fa-check"></i><b>5.3</b> Lab</a><ul>
<li class="chapter" data-level="5.3.1" data-path="resampling-methods.html"><a href="resampling-methods.html#the-validation-set-approach-1"><i class="fa fa-check"></i><b>5.3.1</b> The Validation Set Approach</a></li>
<li class="chapter" data-level="5.3.2" data-path="resampling-methods.html"><a href="resampling-methods.html#loocv"><i class="fa fa-check"></i><b>5.3.2</b> LOOCV</a></li>
<li class="chapter" data-level="5.3.3" data-path="resampling-methods.html"><a href="resampling-methods.html#k-fold-cross-validation-1"><i class="fa fa-check"></i><b>5.3.3</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation</a></li>
<li class="chapter" data-level="5.3.4" data-path="resampling-methods.html"><a href="resampling-methods.html#the-bootstrap-1"><i class="fa fa-check"></i><b>5.3.4</b> The Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="resampling-methods.html"><a href="resampling-methods.html#exercises-3"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html"><i class="fa fa-check"></i><b>6</b> Linear Model Selection And Regularization</a><ul>
<li class="chapter" data-level="6.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#subset-selection"><i class="fa fa-check"></i><b>6.1</b> Subset Selection</a><ul>
<li class="chapter" data-level="6.1.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#best-subset-selection"><i class="fa fa-check"></i><b>6.1.1</b> Best Subset Selection</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#stepwise-selection"><i class="fa fa-check"></i><b>6.2</b> Stepwise Selection</a><ul>
<li class="chapter" data-level="6.2.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#forward-stepwise-selection"><i class="fa fa-check"></i><b>6.2.1</b> Forward Stepwise Selection</a></li>
<li class="chapter" data-level="6.2.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#backward-stepwise-selection"><i class="fa fa-check"></i><b>6.2.2</b> Backward Stepwise Selection</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#choosing-the-optimal-model"><i class="fa fa-check"></i><b>6.3</b> Choosing the Optimal Model</a><ul>
<li class="chapter" data-level="6.3.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#c_p-aic-bic-and-adjusted-r2."><i class="fa fa-check"></i><b>6.3.1</b> <span class="math inline">\(C_p\)</span>, AIC, BIC, and Adjusted <span class="math inline">\(R^2\)</span>.</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#validation-and-cross-validation"><i class="fa fa-check"></i><b>6.3.2</b> Validation and Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#shrinkage-methods"><i class="fa fa-check"></i><b>6.4</b> Shrinkage Methods</a><ul>
<li class="chapter" data-level="6.4.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#ridge-regression"><i class="fa fa-check"></i><b>6.4.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="6.4.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#the-lasso"><i class="fa fa-check"></i><b>6.4.2</b> The Lasso</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/beaulucas/tidy_islr" target="blank">GitHub Repository</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tidy Introduction To Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-model-selection-and-regularization" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Linear Model Selection And Regularization</h1>
<hr />
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb189-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb189-2" data-line-number="2"><span class="kw">library</span>(knitr)</a>
<a class="sourceLine" id="cb189-3" data-line-number="3"><span class="kw">library</span>(skimr)</a>
<a class="sourceLine" id="cb189-4" data-line-number="4"><span class="kw">library</span>(ISLR)</a>
<a class="sourceLine" id="cb189-5" data-line-number="5"><span class="kw">library</span>(tidymodels)</a>
<a class="sourceLine" id="cb189-6" data-line-number="6"><span class="kw">library</span>(workflows)</a>
<a class="sourceLine" id="cb189-7" data-line-number="7"><span class="kw">library</span>(tune)</a>
<a class="sourceLine" id="cb189-8" data-line-number="8"><span class="kw">library</span>(leaps) <span class="co"># best subset selection</span></a></code></pre></div>
<p>Before moving on to the non-linear world in further chapters, let’s discuss in some ways in which the simple linear model can be improved, by replacing plain least squares fitting with some alternative fitting procedures.</p>
<p>Why should we explore alternative fitting procedures? We will see that alternative fitting procedures can yield better prediction accuracy and model interpretability.</p>
<ul>
<li><p><em>Prediction Accuracy</em>: Provided the relationship between the response and its predictors is approximately linear, then least squares estimates will have low bias. If <span class="math inline">\(n &gt;&gt; p\)</span>, meaning that the number of observations <span class="math inline">\(n\)</span> is much larger than the number of predictors <span class="math inline">\(p\)</span>, then the least squares estimates tend to also have low variance. As <span class="math inline">\(p\)</span> approaches <span class="math inline">\(n\)</span>, there can be a lot of variability in the least squares fit, which could result in overfitting and poor predictions on future observations. If <span class="math inline">\(p\)</span> &gt; <span class="math inline">\(n\)</span>, there is no longer a unique least squares coefficient estimate; the method doesn’t work. By <em>constraining</em> or <em>shrinking</em> the estimated coefficients, we can significantly reduce the variance at the cost of a negligible increase in bias.</p></li>
<li><p><em>Model Interpretability</em>: It is common for predictor variables used in a multiple regression model to not be associated with the response. Including these <em>irrelevant</em> variables leads to unnecessary complexity in the resulting model. If we could remove these variables by setting their coefficients equal to zero, we can obtain a simpler, more interpretable model. The chance of least squares yielding a zero coefficient is quite low. We will explore some approaches for <em>feature selection</em>.</p></li>
</ul>
<p>We will discuss three important classes of methods:</p>
<ol style="list-style-type: decimal">
<li><em>Subset selection.</em> This approach involves identifying a subset of the <span class="math inline">\(p\)</span> predictors that we believe to be related to the response.</li>
<li><em>Shrinkage.</em> This approach involves fitting a model of all <span class="math inline">\(p\)</span> predictors, but shrinking (also known as regularizing) the coefficients of some predictors towards zero. This can also result in variable selection when coefficients are shrunk towards exactly zero.</li>
<li><em>Dimension Reduction</em>. This approach involes projecting the <span class="math inline">\(p\)</span> predictors into a <span class="math inline">\(M\)</span>-dimensional subspace, where <span class="math inline">\(M &lt; p\)</span>. This is achieved by computing <span class="math inline">\(M\)</span> different <em>linear combinations</em>, or <em>projections</em>, of the variables. Then, we use these <span class="math inline">\(M\)</span> projections as predictors.</li>
</ol>
<div id="subset-selection" class="section level2">
<h2><span class="header-section-number">6.1</span> Subset Selection</h2>
<div id="best-subset-selection" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Best Subset Selection</h3>
<p>We fit a separate least squares regression for each possible combination of the <span class="math inline">\(p\)</span> predictors. That is, we fit all <span class="math inline">\(p\)</span> models that contain exactly one predictor, all <span class="math inline">\(\binom{p}{2}\)</span> that contain exactly two predictors, and so forth. Once we fit all of them, we identify the one that is best.</p>
<p>Here are the steps:</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(M_0\)</span> denote the <em>null model</em> of no predictors. This is simply the sample mean.</li>
<li>For <span class="math inline">\(k = 1,2,...p\)</span>:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Fit all <span class="math inline">\(\binom{p}{k}\)</span> models that contain exactly <span class="math inline">\(k\)</span> predictors.</li>
<li>Pick the best among these <span class="math inline">\(\binom{p}{k}\)</span> models via largest <span class="math inline">\(R^2\)</span>.</li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li>Select a single best model from <span class="math inline">\(M_0,...,M_p\)</span> using cross-validation, <span class="math inline">\(C_p\)</span> (AIC), BIC, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<p>Once we complete step 2, we reduce the problem from one of <span class="math inline">\(2^p\)</span> possible models to one of <span class="math inline">\(p+1\)</span> possible models. To complete step 3, we can’t use a metric such as <span class="math inline">\(R^2\)</span> anymore. Remember that <span class="math inline">\(R^2\)</span> increases monotonically as the number of features included in the models increases. Therefore, we need to pick the model with the lowest estimated <em>test</em> error.</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb190-1" data-line-number="1">credit &lt;-<span class="st"> </span>ISLR<span class="op">::</span>Credit <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span>janitor<span class="op">::</span><span class="kw">clean_names</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb190-2" data-line-number="2"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>id)</a>
<a class="sourceLine" id="cb190-3" data-line-number="3">regfit_full =<span class="st"> </span><span class="kw">regsubsets</span>(balance <span class="op">~</span>., <span class="dt">data =</span> credit)</a>
<a class="sourceLine" id="cb190-4" data-line-number="4">reg_summary &lt;-<span class="st"> </span><span class="kw">summary</span>(regfit_full)</a>
<a class="sourceLine" id="cb190-5" data-line-number="5"><span class="kw">plot</span>(reg_summary<span class="op">$</span>rsq, <span class="dt">xlab =</span> <span class="st">&quot;Number of Predictors&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;RSquared&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>)</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-129-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>As said above, <span class="math inline">\(R^2\)</span> will always increase as add more predictors. In this case, it ramps up through three predictors before flattening out. We could apply the same idea to other types of models such as logistic regression. Instead of ordering by <span class="math inline">\(R^2\)</span>, we could sort by <em>deviance</em>, a measure that plays the role of <span class="math inline">\(R^2\)</span> for a broader class of models. Deviance is negative two times the maximized log-likelihood; the smaller the deviance, the better the fit.</p>
<p>One problem with best subset selection is the computational cost. Fitting <span class="math inline">\(2^p\)</span> possible model quickly grows prohibitively expensive.</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb191-1" data-line-number="1">cpu_cost &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">preds =</span> <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">50</span>))</a>
<a class="sourceLine" id="cb191-2" data-line-number="2">cpu_cost &lt;-<span class="st"> </span>cpu_cost <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">time =</span> <span class="dv">2</span><span class="op">^</span>preds<span class="op">/</span><span class="dv">1000000</span>)</a>
<a class="sourceLine" id="cb191-3" data-line-number="3"><span class="kw">ggplot</span>(cpu_cost, <span class="kw">aes</span>(<span class="dt">x =</span> preds, <span class="dt">y =</span> time)) <span class="op">+</span></a>
<a class="sourceLine" id="cb191-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb191-5" data-line-number="5"><span class="st">  </span><span class="kw">scale_y_log10</span>(<span class="dt">label=</span>scales<span class="op">::</span>comma)</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-130-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>In this made-up example, fitting best subset selection for <code>20</code> predictors would take less than ten seconds, which is reasonable. At <code>40</code> predictors, that number is arround <code>100,000</code> seconds, more than a full day. Since it is common for a dataset to contain hundreds, if not thousands, of predictors, we need to explore more computationaly efficient alternatives.</p>
</div>
</div>
<div id="stepwise-selection" class="section level2">
<h2><span class="header-section-number">6.2</span> Stepwise Selection</h2>
<p>Due to computational costs, best subset selection cannot be applied with very large <span class="math inline">\(p\)</span>. There are also statistical problems when <span class="math inline">\(p\)</span> is large. The larger the search space, the higher chance of finding models that look good on the training data. An enormous search space can lead to overfitting and high variance of the coefficient estimates.</p>
<p>For both of these reasons, <em>stepwise</em> methods, which explore a more restricted set of models, are attractive alternatives.</p>
<div id="forward-stepwise-selection" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Forward Stepwise Selection</h3>
<p><em>Forward stepwise selection</em> is a computationally efficient alternative to best subset selection. It begins with a model containing no predictors, and then adds predictors one-at-a-time, until all of the predictors are in the model. At each step, the variable that gives the greatest <em>additional</em> improvement to the fit is added to the model.</p>
<p>Here are the steps:</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(M_0\)</span> denote the <em>null model</em> of no predictors. This is simply the sample mean.</li>
<li>For <span class="math inline">\(k = 0,1,...p-1\)</span>:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Consider all <span class="math inline">\(p-k\)</span> models that augment the predictors in <span class="math inline">\(M_k\)</span> with one additional predictors.</li>
<li>Choose the <em>best</em> among these <span class="math inline">\(p - k\)</span> models, and call it <span class="math inline">\(M_{k+1}\)</span>. <em>Best</em> is defined as having the highest <span class="math inline">\(R^2\)</span>.</li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li>Select a single best model from <span class="math inline">\(M_0,...,M_p\)</span> using cross-validation, <span class="math inline">\(C_p\)</span> (AIC), BIC, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<p>Forward stepwise selection fits <span class="math inline">\(1+p(p+1)/2\)</span> models, which we can approximate to <span class="math inline">\(p^2\)</span>. Computationally, this scales much better as <span class="math inline">\(p\)</span> grows.</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb192-1" data-line-number="1">cpu_cost &lt;-<span class="st"> </span>cpu_cost <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">time_fss =</span> preds<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb192-2" data-line-number="2"><span class="kw">ggplot</span>(cpu_cost, <span class="kw">aes</span>(<span class="dt">x =</span> preds)) <span class="op">+</span></a>
<a class="sourceLine" id="cb192-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y=</span>time)) <span class="op">+</span></a>
<a class="sourceLine" id="cb192-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y=</span>time_fss)) <span class="op">+</span></a>
<a class="sourceLine" id="cb192-5" data-line-number="5"><span class="st">  </span><span class="kw">scale_y_log10</span>(<span class="dt">labels=</span>scales<span class="op">::</span>comma)</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-131-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Choosing the best model of those that augment <span class="math inline">\(M_k\)</span> with one additional predictor is simple. We can simply used the <span class="math inline">\(R^2\)</span> value. Step 3, in which we have to evaluate which model among our selected models is best, requires using a method that can estimate test error.</p>
<p>The trade-off of being computationally cheap compared to best subset selection is that it will not always find the best model out of the <span class="math inline">\(2^p\)</span> possible models. Imagine a dataset containing <span class="math inline">\(p=3\)</span> predictors. The best possible one-variable model contains <span class="math inline">\(X_1\)</span>, and the best possible two-variable model contains <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_3\)</span>. Forward stepwise selection will never produce the ideal two-variable model, since it will always fit <span class="math inline">\(X_1\)</span> to the one-variable model. Thus, <span class="math inline">\(M_2\)</span> will always contain <span class="math inline">\(X_1\)</span>.</p>
</div>
<div id="backward-stepwise-selection" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Backward Stepwise Selection</h3>
<p><em>Backward stepwise selection</em> is similar to forward stepwise selection, but starts with the full least squares model containing all <span class="math inline">\(p\)</span> predictors. It then iteratively removes the least useful predictor, one-at-a-time.</p>
<p>Here are the steps:</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(M_0\)</span> denote the <em>full model</em> of all <span class="math inline">\(p\)</span> predictors.</li>
<li>For <span class="math inline">\(k = p, p-1,...,1\)</span>:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Consider all <span class="math inline">\(k\)</span> models that contain all but one of the predictors in <span class="math inline">\(M_k\)</span>, for a total of <span class="math inline">\(k-1\)</span> predictors.</li>
<li>Choose the <em>best</em> among these <span class="math inline">\(k\)</span> models, and call it <span class="math inline">\(M_{k-1}\)</span>. <em>Best</em> is defined as having the highest <span class="math inline">\(R^2\)</span>.</li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li>Select a single best model from <span class="math inline">\(M_0,...,M_p\)</span> using cross-validation, <span class="math inline">\(C_p\)</span> (AIC), BIC, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<p>Like forward stepwise selection, we search through approximately <span class="math inline">\(p^2\)</span> models. Also like forward stepwise selection, we are not guaranteed to yield the best model containing a subset of the <span class="math inline">\(p\)</span> predictors. Backward stepwise selection requires <span class="math inline">\(n\)</span> to be larger than <span class="math inline">\(p\)</span> in order to fit the full model.</p>
<div id="hybrid-approaches" class="section level4">
<h4><span class="header-section-number">6.2.2.1</span> Hybrid Approaches</h4>
<p>Another alternative is a hybrid approach. Variables can be added to the model sequentially, as in forward selection. However, after adding each new variable, the method may also remove any variables that no longer provide an improvement in model fit. Such an approach attempts to mimic best subset selection while retaining the computational advantages of forward and backward stepwise selection.</p>
</div>
</div>
</div>
<div id="choosing-the-optimal-model" class="section level2">
<h2><span class="header-section-number">6.3</span> Choosing the Optimal Model</h2>
<div id="c_p-aic-bic-and-adjusted-r2." class="section level3">
<h3><span class="header-section-number">6.3.1</span> <span class="math inline">\(C_p\)</span>, AIC, BIC, and Adjusted <span class="math inline">\(R^2\)</span>.</h3>
<p>In general, the training set <span class="math inline">\(MSE\)</span> is an underestimate of the test <span class="math inline">\(MSE\)</span>. When we fit a model to the training data using least squares, we specifically estimate the regression coefficients such that the training RSS is as small as possible. Training error will always decrease as we add more variables to the model, but the test error may not. Therefore, we cannot use metrics such as <span class="math inline">\(R^2\)</span> to select from models containing different numbers of variables.</p>
<p>We do have a number of techniques for <em>adjusting</em> the training error. We will now consider four such approaches: <span class="math inline">\(C_p\)</span>, <em>Akaike information criterion (AIC)</em>, <em>Bayesian information criterion (BIC)</em>, and <em>adjusted</em> <span class="math inline">\(R^2\)</span>.</p>
<p>For a fitted least squares model containing <span class="math inline">\(d\)</span> predictors, the <span class="math inline">\(C_p\)</span> estimate of test MSE is computed using the equation</p>
<p><span class="math display">\[
C_{p}=\frac{1}{n}\left(\mathrm{RSS}+2 d \hat{\sigma}^{2}\right)
\]</span></p>
<p>where <span class="math inline">\(\hat\sigma^2\)</span> is an estimate of the variance of the error <span class="math inline">\(\epsilon\)</span> associated with each response measurement. Typically, <span class="math inline">\(\hat\sigma^2\)</span> is estimated using the full model with all predictors. The <span class="math inline">\(C_p\)</span> statistic essentially adds a penalty of <span class="math inline">\(2d\hat\sigma^2\)</span> to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error. The more predictors, the higher the penalty.</p>
<p>The AIC criterion is defined for a large class of models fit by maximum likelihood. In the case of the model with Gaussian errors, maximum likelihood and least squares are the same thing.</p>
<p><span class="math display">\[
AIC=\frac{1}{n\hat\sigma^2}\left(\mathrm{RSS}+2 d \hat{\sigma}^{2}\right)
\]</span></p>
<p>For least squares, <span class="math inline">\(AIC\)</span> and <span class="math inline">\(C_P\)</span> are proportional to eachother.</p>
<p>BIC looks similar but can place a higher penalty on models with many variables.</p>
<p><span class="math display">\[
BIC=\frac{1}{n\hat\sigma^2}\left(\mathrm{RSS}+log(n){\sigma}^{2}\right)
\]</span></p>
<p><span class="math inline">\(BIC\)</span> replaces the <span class="math inline">\(2d\hat\sigma^2\)</span> used by <span class="math inline">\(C_p\)</span> with a <span class="math inline">\(log(n)\hat\sigma^2\)</span>. Since <span class="math inline">\(log(n) &gt; 2\)</span> for any <span class="math inline">\(n &gt; 7\)</span>, <span class="math inline">\(BIC\)</span> places a heavier penalty on models with many variables and tends to select smaller models than <span class="math inline">\(C_p\)</span>.</p>
<p>Adjusted <span class="math inline">\(R^2\)</span> is another approach for selecting among a set of models. Remember that <span class="math inline">\(R^2\)</span> is defined as <span class="math inline">\(1 - RSS/TSS\)</span>. Since <span class="math inline">\(RSS\)</span> can only decrease as more variables are added to the model, <span class="math inline">\(R^2\)</span> always increases as more variables are added. For a least squares model with <span class="math inline">\(d\)</span> variables, adjusted <span class="math inline">\(R^2\)</span> is calculated as</p>
<p>$$
Adjusted R^2=1-</p>
<p>$$</p>
<p>Maximizing the adjusted <span class="math inline">\(R^2\)</span> is equivalent to minimizing the numerator, <span class="math inline">\(RSS/(n-d-1)\)</span>. The idea behind adjusted <span class="math inline">\(R^2\)</span> is that additional noise variables will increase <span class="math inline">\(RSS\)</span> but also increase <span class="math inline">\(d\)</span>, which can ultimately increase <span class="math inline">\(RSS/(n-d-1)\)</span>. Adjusted <span class="math inline">\(R^2\)</span> pays a price for the inclusion of unnecessary variables in the model.</p>
<p><span class="math inline">\(C_p\)</span>, AIC, and BIC all have rigorous theoretical justifications that are beyond the scope of this book. AIC, BIC, and <span class="math inline">\(C_p\)</span> can also be defined for more general types of models.</p>
</div>
<div id="validation-and-cross-validation" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Validation and Cross-Validation</h3>
<p>We can directly estimate the test error using the validation set and cross-validation methods.</p>
<p>This procedure has an advantage over AIC, BIC, adjusted <span class="math inline">\(R^2\)</span> and <span class="math inline">\(C_p\)</span> in that it provides a direct estimate of the test error and makes fewer assumptions about the underlying model. It can also be used in cases in which it is hard to estimate <span class="math inline">\(\sigma^2\)</span> and/or the number of degrees of freedom is not known.</p>
<p>Cross-validation has become a more attractive approach as computing power has increased.</p>
<p>Let’s use cross-validation to determine the best of the <span class="math inline">\(d\)</span>-variable models for the <code>Credit</code> dataset.</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb193-1" data-line-number="1"><span class="co"># </span><span class="al">TODO</span><span class="co"> solve this</span></a>
<a class="sourceLine" id="cb193-2" data-line-number="2"><span class="co"># bic from forward selection</span></a>
<a class="sourceLine" id="cb193-3" data-line-number="3">forward_fit &lt;-<span class="st"> </span><span class="kw">regsubsets</span>(<span class="dt">data =</span> credit, balance <span class="op">~</span><span class="st"> </span>., <span class="dt">nbest=</span><span class="dv">1</span>, <span class="dt">nvmax=</span><span class="dv">8</span>, <span class="dt">method=</span><span class="st">&quot;forward&quot;</span>)</a>
<a class="sourceLine" id="cb193-4" data-line-number="4">forward_summary &lt;-<span class="st"> </span><span class="kw">summary</span>(forward_fit)</a>
<a class="sourceLine" id="cb193-5" data-line-number="5">forward_tibble &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">preds =</span> <span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(forward_summary<span class="op">$</span>bic)),</a>
<a class="sourceLine" id="cb193-6" data-line-number="6">                         <span class="dt">bic =</span> forward_summary<span class="op">$</span>bic)</a>
<a class="sourceLine" id="cb193-7" data-line-number="7"></a>
<a class="sourceLine" id="cb193-8" data-line-number="8"><span class="co"># validation with 3/4 training set</span></a>
<a class="sourceLine" id="cb193-9" data-line-number="9">credit_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(credit, <span class="dt">prop =</span> <span class="dv">3</span><span class="op">/</span><span class="dv">4</span>)</a>
<a class="sourceLine" id="cb193-10" data-line-number="10">credit_training &lt;-<span class="st"> </span><span class="kw">training</span>(credit_split)</a>
<a class="sourceLine" id="cb193-11" data-line-number="11">credit_test &lt;-<span class="st"> </span><span class="kw">testing</span>(credit_split)</a>
<a class="sourceLine" id="cb193-12" data-line-number="12"></a>
<a class="sourceLine" id="cb193-13" data-line-number="13"><span class="co">## find best model from null to full</span></a>
<a class="sourceLine" id="cb193-14" data-line-number="14"><span class="co"># 10 pred cols </span></a>
<a class="sourceLine" id="cb193-15" data-line-number="15"><span class="co"># </span><span class="al">TODO</span><span class="co"> tidy way?</span></a>
<a class="sourceLine" id="cb193-16" data-line-number="16">num_preds &lt;-<span class="st"> </span><span class="kw">ncol</span>(credit <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>balance))</a>
<a class="sourceLine" id="cb193-17" data-line-number="17"><span class="co"># cross-validation with k=10</span></a>
<a class="sourceLine" id="cb193-18" data-line-number="18"><span class="co">#lm(data = credit, balance ~ credit[seq(1,num_preds)])</span></a></code></pre></div>
</div>
</div>
<div id="shrinkage-methods" class="section level2">
<h2><span class="header-section-number">6.4</span> Shrinkage Methods</h2>
<p>The subset methods above involve fitting a least squares model that contains a subset of the predictors. We can also fit a model containing all <span class="math inline">\(p\)</span> predictors that <em>constraints</em>, or <em>regularizes</em>, the coefficient estimates. Shrinking the coefficient estimates can significantly reduce their variance. The two best-known techniques are <em>ridge regression</em> and the <em>lasso</em>.</p>
<div id="ridge-regression" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Ridge Regression</h3>
<p>Ridge regression is very similar to least squares, except we are minimizing the coefficients by a different quantity.</p>
<p><span class="math display">\[
\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}=\mathrm{RSS}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}
\]</span></p>
<p>Where <span class="math inline">\(\lambda\)</span> is a <em>tuning parameter</em> to be determined separately. Similar to least squares, we seek coefficient estimates that make RSS small. However, we’re adding a second term, <span class="math inline">\(\lambda \sum_{j=1}^{p} \beta_{j}^{2}\)</span>, called a <em>shrinkage penalty</em>. This shrinkage penalty is small when <span class="math inline">\(\beta_1,...,\beta_p\)</span> are close to zero, and thus has the effect of <em>shrinking</em> the estimates of <span class="math inline">\(\beta_j\)</span> towards zero. The penalty, <span class="math inline">\(\beta_{j}^{2}\)</span>, is also known as an L2 penalty. <span class="math inline">\(\lambda\)</span> controls the relative impact of these two terms (at <span class="math inline">\(\lambda=0\)</span>, this is the original least squares estimate). Ridge regression will produce a different set of coefficient estimates for each value of <span class="math inline">\(\lambda\)</span>. Selecting a good value of <span class="math inline">\(\lambda\)</span> is crtical.</p>
<p>Notice the shrinkage is applied not to the intercept <span class="math inline">\(\beta_0\)</span>. If the variables have been centered to have mean zero before ridge regression is performed, then the estimated intercept will take the value of the sample mean.</p>
<div id="an-application-to-the-credit-data" class="section level4">
<h4><span class="header-section-number">6.4.1.1</span> An Application to the Credit Data</h4>
<p>Let’s test out ridge regression on our credit data set.</p>
<p>We again split into training/test splits.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb194-1" data-line-number="1"><span class="co"># </span><span class="al">TODO</span><span class="co"> Find better example set</span></a>
<a class="sourceLine" id="cb194-2" data-line-number="2"><span class="co"># validation with 3/4 training set</span></a>
<a class="sourceLine" id="cb194-3" data-line-number="3">credit_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(credit, <span class="dt">prop =</span> <span class="dv">3</span><span class="op">/</span><span class="dv">4</span>)</a>
<a class="sourceLine" id="cb194-4" data-line-number="4">credit_train &lt;-<span class="st"> </span><span class="kw">training</span>(credit_split)</a>
<a class="sourceLine" id="cb194-5" data-line-number="5">credit_test &lt;-<span class="st"> </span><span class="kw">testing</span>(credit_split)</a></code></pre></div>
<p>Next, let’s specify a recipe in tidymodels. I want to remove zero-variance predictors as well as normalize numeric predictors (this is important when regularization is involved). At this moment, <code>ridge_rec</code> hasn’t performed any calculations. We need to <code>prep()</code> it to do so.</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb195-1" data-line-number="1"><span class="co"># ridge recipe</span></a>
<a class="sourceLine" id="cb195-2" data-line-number="2">ridge_rec &lt;-<span class="st"> </span><span class="kw">recipe</span>(balance <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> credit_train) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb195-3" data-line-number="3"><span class="st">  </span><span class="kw">step_zv</span>(<span class="kw">all_numeric</span>(), <span class="op">-</span><span class="kw">all_outcomes</span>()) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># remove zero variance predictors</span></a>
<a class="sourceLine" id="cb195-4" data-line-number="4"><span class="st">  </span><span class="kw">step_normalize</span>(<span class="kw">all_numeric</span>(), <span class="op">-</span><span class="kw">all_outcomes</span>()) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb195-5" data-line-number="5"><span class="st">  </span><span class="kw">step_dummy</span>(<span class="kw">all_nominal</span>())</a>
<a class="sourceLine" id="cb195-6" data-line-number="6"></a>
<a class="sourceLine" id="cb195-7" data-line-number="7">ridge_prep &lt;-<span class="st"> </span>ridge_rec <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb195-8" data-line-number="8"><span class="st">  </span><span class="kw">prep</span>(<span class="dt">strings_as_factors =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>Now we can specify and fit our model. To specify L2 regularization used in ridge regression, we need to set <code>mixture = 0</code>. Our lambda value (<code>penalty</code>) is undecided, so we plug in a placeholder for now.</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb196-1" data-line-number="1">ridge_spec &lt;-<span class="st"> </span><span class="kw">linear_reg</span>(<span class="dt">penalty =</span> <span class="kw">tune</span>(), <span class="dt">mixture =</span> <span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># mixture = 0 meaning no L1 penalty </span></a>
<a class="sourceLine" id="cb196-2" data-line-number="2"><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</a>
<a class="sourceLine" id="cb196-3" data-line-number="3"></a>
<a class="sourceLine" id="cb196-4" data-line-number="4">wf &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb196-5" data-line-number="5"><span class="st">  </span><span class="kw">add_recipe</span>(ridge_rec)</a>
<a class="sourceLine" id="cb196-6" data-line-number="6"></a>
<a class="sourceLine" id="cb196-7" data-line-number="7">ridge_fit &lt;-<span class="st"> </span>wf <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb196-8" data-line-number="8"><span class="st">  </span><span class="kw">add_model</span>(ridge_spec) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb196-9" data-line-number="9"><span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> credit_train)</a>
<a class="sourceLine" id="cb196-10" data-line-number="10"></a>
<a class="sourceLine" id="cb196-11" data-line-number="11"><span class="co"># workflow</span></a>
<a class="sourceLine" id="cb196-12" data-line-number="12">ridge_fit <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb196-13" data-line-number="13"><span class="st">  </span><span class="kw">pull_workflow_fit</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb196-14" data-line-number="14"><span class="st">  </span><span class="kw">tidy</span>()</a></code></pre></div>
<pre><code>## # A tibble: 1,200 x 5
##   term         step  estimate  lambda dev.ratio
##   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)     1  5.21e+ 2 400049.  3.60e-36
## 2 income          1  2.19e-34 400049.  3.60e-36
## 3 limit           1  4.04e-34 400049.  3.60e-36
## 4 rating          1  4.05e-34 400049.  3.60e-36
## 5 cards           1  5.77e-35 400049.  3.60e-36
## 6 age             1 -6.15e-38 400049.  3.60e-36
## # … with 1,194 more rows</code></pre>
<div id="tuning-lambda" class="section level5">
<h5><span class="header-section-number">6.4.1.1.1</span> Tuning Lambda</h5>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb198-1" data-line-number="1"><span class="co"># </span><span class="al">TODO</span><span class="co"> clean up</span></a>
<a class="sourceLine" id="cb198-2" data-line-number="2"><span class="kw">library</span>(tune)</a>
<a class="sourceLine" id="cb198-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">40</span>)</a>
<a class="sourceLine" id="cb198-4" data-line-number="4">credit_boot &lt;-<span class="st"> </span><span class="kw">bootstraps</span>(credit_train)</a>
<a class="sourceLine" id="cb198-5" data-line-number="5"></a>
<a class="sourceLine" id="cb198-6" data-line-number="6">tune_spec &lt;-<span class="st"> </span><span class="kw">linear_reg</span>(<span class="dt">penalty =</span> <span class="kw">tune</span>(), <span class="dt">mixture =</span> <span class="dv">0</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb198-7" data-line-number="7"><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">&quot;glmnet&quot;</span>)</a>
<a class="sourceLine" id="cb198-8" data-line-number="8"></a>
<a class="sourceLine" id="cb198-9" data-line-number="9">lambda_grid &lt;-<span class="st"> </span><span class="kw">grid_regular</span>(<span class="kw">range_set</span>(<span class="kw">penalty</span>(), <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>)), <span class="dt">levels =</span> <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb198-10" data-line-number="10"></a>
<a class="sourceLine" id="cb198-11" data-line-number="11">doParallel<span class="op">::</span><span class="kw">registerDoParallel</span>()</a>
<a class="sourceLine" id="cb198-12" data-line-number="12"><span class="kw">set.seed</span>(<span class="dv">40</span>)</a>
<a class="sourceLine" id="cb198-13" data-line-number="13">ridge_grid &lt;-<span class="st"> </span><span class="kw">tune_grid</span>(</a>
<a class="sourceLine" id="cb198-14" data-line-number="14">  wf <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">add_model</span>(tune_spec),</a>
<a class="sourceLine" id="cb198-15" data-line-number="15">  <span class="dt">resamples =</span> credit_boot,</a>
<a class="sourceLine" id="cb198-16" data-line-number="16">  <span class="dt">grid =</span> lambda_grid</a>
<a class="sourceLine" id="cb198-17" data-line-number="17">)</a>
<a class="sourceLine" id="cb198-18" data-line-number="18"></a>
<a class="sourceLine" id="cb198-19" data-line-number="19">ridge_grid <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb198-20" data-line-number="20"><span class="st">  </span><span class="kw">collect_metrics</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb198-21" data-line-number="21"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> penalty, <span class="dt">y =</span> mean, <span class="dt">colour =</span> .metric)) <span class="op">+</span></a>
<a class="sourceLine" id="cb198-22" data-line-number="22"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>.metric, <span class="dt">scales=</span><span class="st">&quot;free&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb198-23" data-line-number="23"><span class="st">  </span><span class="kw">geom_line</span>()</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-136-1.png" width="576" style="display: block; margin: auto;" /></p>
<p><code>penalty = 0</code> is the original least squares fit. As lambda increases, the coefficients are being shrunk towards zero and we approach the <em>null model</em>. At this point, the decrease in variance (always predicting the sample mean) isn’t worth the increase in bias, and <code>rmse</code> begins to shoot up. Ridge regression doesn’t help our credit data set much.</p>
<p>It’s important to note that standard least squares is <em>scale equivariant</em>, multiplying <span class="math inline">\(X_j\)</span> by a constant <span class="math inline">\(c\)</span> simply scales the least squares coefficient estimates by <span class="math inline">\(1/c\)</span>. In ridge regression, in which the penalty involves summing up <em>all</em> the coefficients (<span class="math inline">\(\lambda \sum_{j=1}^{p} \beta_{j}^{2}\)</span>), scale plays a big role. A predictor with a scale much larger than others can significantly affect the ridge regression. That is why it’s important to <em>standardize</em> the predictors prior to applying ridge regression.</p>
</div>
</div>
<div id="why-does-ridge-regression-improve-over-least-squares" class="section level4">
<h4><span class="header-section-number">6.4.1.2</span> Why Does Ridge Regression Improve Over Least Squares?</h4>
<p>We again go back to the <em>bias-variance tradeoff</em>. As <span class="math inline">\(\lambda\)</span> increases and coefficients are shrunk, the flexibility of the fit decreases, leading to reduced variance but increased bias. If <span class="math inline">\(\lambda\)</span> gets too large and coefficients are shrunk further, they tend to be underestimated, and the variance reduction slows down while the bias increase speeds up.</p>
<p>When <span class="math inline">\(n\)</span> is close to <span class="math inline">\(p\)</span>, the least squares estimate will be extremely variable. Ridge regression works best when the least squares estimates have high variance.</p>
<p>Computationally, ridge regression is much faster than best subset selection, which searches through <span class="math inline">\(2^p\)</span> models. In contrast, ridge regression fits a single model for each <span class="math inline">\(\lambda\)</span> value it checks.</p>
</div>
</div>
<div id="the-lasso" class="section level3">
<h3><span class="header-section-number">6.4.2</span> The Lasso</h3>
<p>One disadvantage of ridge regression is that it never fully shrinks coefficients towards zero. This may not be a problem for prediction accuracy, but could certainly affect model interpretability.</p>
<p>If we want to build a <em>sparse</em> model involving fewer predictors, then we need to explore a different shrinkage method.</p>
<p>The <em>lasso</em> is an alternative to ridge regression. The lasso coefficients, <span class="math inline">\(\hat\beta_{j}^{L}\)</span>, minimize the quantity</p>
<p><span class="math display">\[
\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p} |\beta_{j}|=\mathrm{RSS}+\lambda \sum_{j=1}^{p} |\beta_{j}|
\]</span></p>
<p>The only difference between this and the ridge regression is the penalty coefficient. The lasso uses an L1 penalty instead of an L2 penalty.</p>
<p>The lasso also shrinks the coefficients towards zero. But due to the L1 penalty, the lasso can force some coefficients to be <em>exactly</em> zero when <span class="math inline">\(\lambda\)</span> is sufficiently large. The lasso thus has the ability to perform <em>variable selection</em>, producing <em>sparse</em> models which involve a subset of the original predictors.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb199-1" data-line-number="1"><span class="co"># </span><span class="al">TODO</span><span class="co"> coefficient plot of lasso</span></a></code></pre></div>
<p>Another way to think of the penalties is to imagine it as a constraint, or budget. We can think of ridge regression and lasso as fitting least squares with an additional <em>constraint</em> component, defined by the L1/L2 regularization. The sum of the squared coefficients (L2) or the sum of the magnitude of the coefficients (L1) can only be so large.</p>
<div id="the-variable-selection-property-of-the-lasso" class="section level4">
<h4><span class="header-section-number">6.4.2.1</span> The Variable Selection Property of The Lasso</h4>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="resampling-methods.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["tidy_islr.pdf", "tidy_islr.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
