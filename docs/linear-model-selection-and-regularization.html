<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Linear Model Selection And Regularization | A Tidy Introduction To Statistical Learning</title>
  <meta name="description" content="Chapter 6 Linear Model Selection And Regularization | A Tidy Introduction To Statistical Learning" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Linear Model Selection And Regularization | A Tidy Introduction To Statistical Learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Linear Model Selection And Regularization | A Tidy Introduction To Statistical Learning" />
  
  
  

<meta name="author" content="Beau Lucas" />


<meta name="date" content="2020-03-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="resampling-methods.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Tidy Introduction To Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#an-overview-of-statistical-learning"><i class="fa fa-check"></i><b>1.1</b> An Overview of Statistical Learning</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#data-sets-used-in-labs-and-exercises"><i class="fa fa-check"></i><b>1.2</b> Data Sets Used in Labs and Exercises</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#book-resources"><i class="fa fa-check"></i><b>1.3</b> Book Resources:</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-learning.html"><a href="statistical-learning.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#packages-used-in-this-chapter"><i class="fa fa-check"></i><b>2.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#what-is-statistical-learning"><i class="fa fa-check"></i><b>2.2</b> What is Statistical Learning?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#why-estimate-f"><i class="fa fa-check"></i><b>2.2.1</b> Why Estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#how-do-we-estimate-f"><i class="fa fa-check"></i><b>2.2.2</b> How do we estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-trade-off-between-prediction-accuracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.2.3</b> The Trade-Off Between Prediction Accuracy and Model Interpretability</a></li>
<li class="chapter" data-level="2.2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#supervised-versus-unsupervised-learning"><i class="fa fa-check"></i><b>2.2.4</b> Supervised Versus Unsupervised Learning</a></li>
<li class="chapter" data-level="2.2.5" data-path="statistical-learning.html"><a href="statistical-learning.html#regression-versus-classification-problems"><i class="fa fa-check"></i><b>2.2.5</b> Regression Versus Classification Problems</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.3</b> Assessing Model Accuracy</a><ul>
<li class="chapter" data-level="2.3.1" data-path="statistical-learning.html"><a href="statistical-learning.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.3.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.3.2" data-path="statistical-learning.html"><a href="statistical-learning.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>2.3.2</b> The Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="2.3.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-classification-setting"><i class="fa fa-check"></i><b>2.3.3</b> The Classification Setting</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#lab-introduction-to-r"><i class="fa fa-check"></i><b>2.4</b> Lab: Introduction to R</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-learning.html"><a href="statistical-learning.html#exercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#packages-used-in-this-chapter-1"><i class="fa fa-check"></i><b>3.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>3.2.1</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimate"><i class="fa fa-check"></i><b>3.2.2</b> Assessing the Accuracy of the Coefficient Estimate</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-model"><i class="fa fa-check"></i><b>3.2.3</b> Assessing the Accuracy of the Model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>3.3.1</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-regression.html"><a href="linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>3.3.2</b> Some Important Questions</a></li>
<li class="chapter" data-level="3.3.3" data-path="linear-regression.html"><a href="linear-regression.html#other-considerations-in-the-regression-model"><i class="fa fa-check"></i><b>3.3.3</b> Other Considerations in the Regression Model</a></li>
<li class="chapter" data-level="3.3.4" data-path="linear-regression.html"><a href="linear-regression.html#qualitative-predictors-with-more-than-two-levels"><i class="fa fa-check"></i><b>3.3.4</b> Qualitative Predictors with More than Two Levels</a></li>
<li class="chapter" data-level="3.3.5" data-path="linear-regression.html"><a href="linear-regression.html#extensions-of-the-linear-model"><i class="fa fa-check"></i><b>3.3.5</b> Extensions of the Linear Model</a></li>
<li class="chapter" data-level="3.3.6" data-path="linear-regression.html"><a href="linear-regression.html#potential-problems"><i class="fa fa-check"></i><b>3.3.6</b> Potential Problems</a></li>
<li class="chapter" data-level="3.3.7" data-path="linear-regression.html"><a href="linear-regression.html#the-marketing-plan"><i class="fa fa-check"></i><b>3.3.7</b> The Marketing Plan</a></li>
<li class="chapter" data-level="3.3.8" data-path="linear-regression.html"><a href="linear-regression.html#comparison-of-linear-regression-with-k-nearest-neighbors"><i class="fa fa-check"></i><b>3.3.8</b> Comparison of Linear Regression with <em>K</em>-Nearest Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-regression.html"><a href="linear-regression.html#lab-linear-regression"><i class="fa fa-check"></i><b>3.4</b> Lab: Linear Regression</a><ul>
<li class="chapter" data-level="3.4.1" data-path="linear-regression.html"><a href="linear-regression.html#fitting-a-linear-regression"><i class="fa fa-check"></i><b>3.4.1</b> Fitting a linear regression</a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression-1"><i class="fa fa-check"></i><b>3.4.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="3.4.3" data-path="linear-regression.html"><a href="linear-regression.html#interaction-terms"><i class="fa fa-check"></i><b>3.4.3</b> Interaction Terms</a></li>
<li class="chapter" data-level="3.4.4" data-path="linear-regression.html"><a href="linear-regression.html#non-linear-transformations-of-the-predictors"><i class="fa fa-check"></i><b>3.4.4</b> Non-linear Transformations of the Predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="linear-regression.html"><a href="linear-regression.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>4</b> Classification</a><ul>
<li class="chapter" data-level="4.1" data-path="classification.html"><a href="classification.html#packages-used-in-this-chapter-2"><i class="fa fa-check"></i><b>4.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="4.2" data-path="classification.html"><a href="classification.html#an-overview-of-classification"><i class="fa fa-check"></i><b>4.2</b> An Overview of Classification</a></li>
<li class="chapter" data-level="4.3" data-path="classification.html"><a href="classification.html#why-not-linear-regression"><i class="fa fa-check"></i><b>4.3</b> Why Not Linear Regression?</a></li>
<li class="chapter" data-level="4.4" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>4.4</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.4.1" data-path="classification.html"><a href="classification.html#the-logistic-model"><i class="fa fa-check"></i><b>4.4.1</b> The Logistic Model</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification.html"><a href="classification.html#estimating-the-regression-coefficients-1"><i class="fa fa-check"></i><b>4.4.2</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="4.4.3" data-path="classification.html"><a href="classification.html#making-predictions"><i class="fa fa-check"></i><b>4.4.3</b> Making Predictions</a></li>
<li class="chapter" data-level="4.4.4" data-path="classification.html"><a href="classification.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>4.4.4</b> Multiple Logistic Regression</a></li>
<li class="chapter" data-level="4.4.5" data-path="classification.html"><a href="classification.html#logistic-regression-for-2-response-classes"><i class="fa fa-check"></i><b>4.4.5</b> Logistic Regression for &gt;2 Response Classes</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>4.5</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="4.5.1" data-path="classification.html"><a href="classification.html#using-bayes-theorem-for-classification"><i class="fa fa-check"></i><b>4.5.1</b> Using Bayes’ Theorem for Classification</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1"><i class="fa fa-check"></i><b>4.5.2</b> Linear Discriminant Analysis for p = 1</a></li>
<li class="chapter" data-level="4.5.3" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1-1"><i class="fa fa-check"></i><b>4.5.3</b> Linear Discriminant Analysis for p &gt; 1</a></li>
<li class="chapter" data-level="4.5.4" data-path="classification.html"><a href="classification.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>4.5.4</b> Quadratic Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="classification.html"><a href="classification.html#a-comparison-of-classification-methods"><i class="fa fa-check"></i><b>4.6</b> A Comparison of Classification Methods</a></li>
<li class="chapter" data-level="4.7" data-path="classification.html"><a href="classification.html#lab-logistic-regression-lda-qda-and-knn"><i class="fa fa-check"></i><b>4.7</b> Lab: Logistic Regression, LDA, QDA, and KNN</a><ul>
<li class="chapter" data-level="4.7.1" data-path="classification.html"><a href="classification.html#churn-dataset"><i class="fa fa-check"></i><b>4.7.1</b> Churn Dataset</a></li>
<li class="chapter" data-level="4.7.2" data-path="classification.html"><a href="classification.html#logistic-regression-1"><i class="fa fa-check"></i><b>4.7.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.7.3" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-1"><i class="fa fa-check"></i><b>4.7.3</b> Linear discriminant analysis</a></li>
<li class="chapter" data-level="4.7.4" data-path="classification.html"><a href="classification.html#k-nearest-neighbors-1"><i class="fa fa-check"></i><b>4.7.4</b> K-Nearest Neighbors</a></li>
<li class="chapter" data-level="4.7.5" data-path="classification.html"><a href="classification.html#choosing-the-model"><i class="fa fa-check"></i><b>4.7.5</b> Choosing the model</a></li>
<li class="chapter" data-level="4.7.6" data-path="classification.html"><a href="classification.html#evaluating-the-threshold"><i class="fa fa-check"></i><b>4.7.6</b> Evaluating the threshold</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="classification.html"><a href="classification.html#conclusion"><i class="fa fa-check"></i><b>4.8</b> Conclusion</a></li>
<li class="chapter" data-level="4.9" data-path="classification.html"><a href="classification.html#exercises-2"><i class="fa fa-check"></i><b>4.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="resampling-methods.html"><a href="resampling-methods.html"><i class="fa fa-check"></i><b>5</b> Resampling Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation"><i class="fa fa-check"></i><b>5.1</b> Cross-Validation</a><ul>
<li class="chapter" data-level="5.1.1" data-path="resampling-methods.html"><a href="resampling-methods.html#the-validation-set-approach"><i class="fa fa-check"></i><b>5.1.1</b> The Validation Set Approach</a></li>
<li class="chapter" data-level="5.1.2" data-path="resampling-methods.html"><a href="resampling-methods.html#leave-one-out-cross-validation."><i class="fa fa-check"></i><b>5.1.2</b> Leave-One-Out Cross-Validation.</a></li>
<li class="chapter" data-level="5.1.3" data-path="resampling-methods.html"><a href="resampling-methods.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.1.3</b> k-Fold Cross-Validation</a></li>
<li class="chapter" data-level="5.1.4" data-path="resampling-methods.html"><a href="resampling-methods.html#bias-variance-trade-off-for-k-fold-cross-validation"><i class="fa fa-check"></i><b>5.1.4</b> Bias-Variance Trade-Off for <em>k</em>-fold Cross-Validation</a></li>
<li class="chapter" data-level="5.1.5" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation-on-classification-problems"><i class="fa fa-check"></i><b>5.1.5</b> Cross-Validation on Classification Problems</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="resampling-methods.html"><a href="resampling-methods.html#the-bootstrap"><i class="fa fa-check"></i><b>5.2</b> The Bootstrap</a></li>
<li class="chapter" data-level="5.3" data-path="resampling-methods.html"><a href="resampling-methods.html#lab"><i class="fa fa-check"></i><b>5.3</b> Lab</a><ul>
<li class="chapter" data-level="5.3.1" data-path="resampling-methods.html"><a href="resampling-methods.html#the-validation-set-approach-1"><i class="fa fa-check"></i><b>5.3.1</b> The Validation Set Approach</a></li>
<li class="chapter" data-level="5.3.2" data-path="resampling-methods.html"><a href="resampling-methods.html#loocv"><i class="fa fa-check"></i><b>5.3.2</b> LOOCV</a></li>
<li class="chapter" data-level="5.3.3" data-path="resampling-methods.html"><a href="resampling-methods.html#k-fold-cross-validation-1"><i class="fa fa-check"></i><b>5.3.3</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation</a></li>
<li class="chapter" data-level="5.3.4" data-path="resampling-methods.html"><a href="resampling-methods.html#the-bootstrap-1"><i class="fa fa-check"></i><b>5.3.4</b> The Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="resampling-methods.html"><a href="resampling-methods.html#exercises-3"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html"><i class="fa fa-check"></i><b>6</b> Linear Model Selection And Regularization</a><ul>
<li class="chapter" data-level="6.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#subset-selection"><i class="fa fa-check"></i><b>6.1</b> Subset Selection</a><ul>
<li class="chapter" data-level="6.1.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#best-subset-selection"><i class="fa fa-check"></i><b>6.1.1</b> Best Subset Selection</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#stepwise-selection"><i class="fa fa-check"></i><b>6.2</b> Stepwise Selection</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/beaulucas/tidy_islr" target="blank">GitHub Repository</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tidy Introduction To Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-model-selection-and-regularization" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Linear Model Selection And Regularization</h1>
<hr />
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb189-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb189-2" data-line-number="2"><span class="kw">library</span>(knitr)</a>
<a class="sourceLine" id="cb189-3" data-line-number="3"><span class="kw">library</span>(skimr)</a>
<a class="sourceLine" id="cb189-4" data-line-number="4"><span class="kw">library</span>(ISLR)</a>
<a class="sourceLine" id="cb189-5" data-line-number="5"><span class="kw">library</span>(tidymodels)</a>
<a class="sourceLine" id="cb189-6" data-line-number="6"><span class="kw">library</span>(leaps) <span class="co"># best subset selection</span></a></code></pre></div>
<p>Before moving on to the non-linear world in further chapters, let’s discuss in some ways in which the simple linear model can be improved, by replacing plain least squares fitting with some alternative fitting procedures.</p>
<p>Why should we explore alternative fitting procedures? We will see that alternative fitting procedures can yield better prediction accuracy and model interpretability.</p>
<ul>
<li><p><em>Prediction Accuracy</em>: Provided the relationship between the response and its predictors is approximately linear, then least squares estimates will have low bias. If <span class="math inline">\(n &gt;&gt; p\)</span>, meaning that the number of observations <span class="math inline">\(n\)</span> is much larger than the number of predictors <span class="math inline">\(p\)</span>, then the least squares estimates tend to also have low variance. As <span class="math inline">\(p\)</span> approaches <span class="math inline">\(n\)</span>, there can be a lot of variability in the least squares fit, which could result in overfitting and poor predictions on future observations. If <span class="math inline">\(p\)</span> &gt; <span class="math inline">\(n\)</span>, there is no longer a unique least squares coefficient estimate; the method doesn’t work. By <em>constraining</em> or <em>shrinking</em> the estimated coefficients, we can significantly reduce the variance at the cost of a negligible increase in bias.</p></li>
<li><p><em>Model Interpretability</em>: It is common for predictor variables used in a multiple regression model to not be associated with the response. Including these <em>irrelevant</em> variables leads to unnecessary complexity in the resulting model. If we could remove these variables by setting their coefficients equal to zero, we can obtain a simpler, more interpretable model. The chance of least squares yielding a zero coefficient is quite low. We will explore some approaches for <em>feature selection</em>.</p></li>
</ul>
<p>We will discuss three important classes of methods:</p>
<ol style="list-style-type: decimal">
<li><em>Subset selection.</em> This approach involves identifying a subset of the <span class="math inline">\(p\)</span> predictors that we believe to be related to the response.</li>
<li><em>Shrinkage.</em> This approach involves fitting a model of all <span class="math inline">\(p\)</span> predictors, but shrinking (also known as regularizing) the coefficients of some predictors towards zero. This can also result in variable selection when coefficients are shrunk towards exactly zero.</li>
<li><em>Dimension Reduction</em>. This approach involes projecting the <span class="math inline">\(p\)</span> predictors into a <span class="math inline">\(M\)</span>-dimensional subspace, where <span class="math inline">\(M &lt; p\)</span>. This is achieved by computing <span class="math inline">\(M\)</span> different <em>linear combinations</em>, or <em>projections</em>, of the variables. Then, we use these <span class="math inline">\(M\)</span> projections as predictors.</li>
</ol>
<div id="subset-selection" class="section level2">
<h2><span class="header-section-number">6.1</span> Subset Selection</h2>
<div id="best-subset-selection" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Best Subset Selection</h3>
<p>We fit a separate least squares regression for each possible combination of the <span class="math inline">\(p\)</span> predictors. That is, we fit all <span class="math inline">\(p\)</span> models that contain exactly one predictor, all <span class="math inline">\(\binom{p}{2}\)</span> that contain exactly two predictors, and so forth. Once we fit all of them, we identify the one that is best.</p>
<p>Here are the steps:</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(M_0\)</span> denote the <em>null model</em> of no predictors. This is simply the sample mean.</li>
<li>For <span class="math inline">\(k = 1,2,...p\)</span>:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Fit all <span class="math inline">\(\binom{p}{k}\)</span> models that contain exactly <span class="math inline">\(k\)</span> predictors.</li>
<li>Pick the best among these <span class="math inline">\(\binom{p}{k}\)</span> models via largest <span class="math inline">\(R^2\)</span>.</li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li>Select a single best model from <span class="math inline">\(M_0,...,M_p\)</span> using cross-validation, <span class="math inline">\(C_p\)</span> (AIC), BIC, or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<p>Once we complete step 2, we reduce the problem from one of <span class="math inline">\(2^p\)</span> possible models to one of <span class="math inline">\(p+1\)</span> possible models. To complete step 3, we can’t use a metric such as <span class="math inline">\(R^2\)</span> anymore. Remember that <span class="math inline">\(R^2\)</span> increases monotonically as the number of features included in the models increases. Therefore, we need to pick the model with the lowest estimated <em>test</em> error.</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb190-1" data-line-number="1">credit &lt;-<span class="st"> </span>ISLR<span class="op">::</span>Credit <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span>janitor<span class="op">::</span><span class="kw">clean_names</span>()</a>
<a class="sourceLine" id="cb190-2" data-line-number="2">regfit_full =<span class="st"> </span><span class="kw">regsubsets</span>(balance <span class="op">~</span>., <span class="dt">data =</span> credit)</a>
<a class="sourceLine" id="cb190-3" data-line-number="3">reg_summary &lt;-<span class="st"> </span><span class="kw">summary</span>(regfit_full)</a>
<a class="sourceLine" id="cb190-4" data-line-number="4"><span class="kw">plot</span>(reg_summary<span class="op">$</span>rsq, <span class="dt">xlab =</span> <span class="st">&quot;Number of Predictors&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;RSquared&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>)</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-129-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>As said above, <span class="math inline">\(R^2\)</span> will always increase as add more predictors. In this case, it ramps up through three predictors before flattening out. We could apply the same idea to other types of models such as logistic regression. Instead of ordering by <span class="math inline">\(R^2\)</span>, we could sort by <em>deviance</em>, a measure that plays the role of <span class="math inline">\(R^2\)</span> for a broader class of models. Deviance is negative two times the maximized log-likelihood; the smaller the deviance, the better the fit.</p>
<p>One problem with best subset selection is the computational cost. Fitting <span class="math inline">\(2^p\)</span> possible model quickly grows prohibitively expensive.</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb191-1" data-line-number="1">cpu_cost &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">preds =</span> <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">50</span>))</a>
<a class="sourceLine" id="cb191-2" data-line-number="2">cpu_cost &lt;-<span class="st"> </span>cpu_cost <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">time =</span> <span class="dv">2</span><span class="op">^</span>preds<span class="op">/</span><span class="dv">1000000</span>)</a>
<a class="sourceLine" id="cb191-3" data-line-number="3"><span class="kw">ggplot</span>(cpu_cost, <span class="kw">aes</span>(<span class="dt">x =</span> preds, <span class="dt">y =</span> time)) <span class="op">+</span></a>
<a class="sourceLine" id="cb191-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb191-5" data-line-number="5"><span class="st">  </span><span class="kw">scale_y_log10</span>(<span class="dt">label=</span>scales<span class="op">::</span>comma)</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-130-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>In this made-up example, fitting best subset selection for <code>20</code> predictors would take less than ten seconds, which is reasonable. At <code>40</code> predictors, that number is arround <code>100,000</code> seconds, more than a full day. Given that a large number of predictors, sometimes in the thousands, is a common occurrence, we need to explore more computationaly efficient alternatives.</p>
</div>
</div>
<div id="stepwise-selection" class="section level2">
<h2><span class="header-section-number">6.2</span> Stepwise Selection</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="resampling-methods.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["tidy_islr.pdf", "tidy_islr.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
