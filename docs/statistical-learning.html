<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Statistical Learning | A Tidy Introduction To Statistical Learning</title>
  <meta name="description" content="Chapter 2 Statistical Learning | A Tidy Introduction To Statistical Learning" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Statistical Learning | A Tidy Introduction To Statistical Learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Statistical Learning | A Tidy Introduction To Statistical Learning" />
  
  
  

<meta name="author" content="Beau Lucas" />


<meta name="date" content="2019-12-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html">
<link rel="next" href="linear-regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Tidy Introduction To Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#an-overview-of-statistical-learning"><i class="fa fa-check"></i><b>1.1</b> An Overview of Statistical Learning</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#data-sets-used-in-labs-and-exercises"><i class="fa fa-check"></i><b>1.2</b> Data Sets Used in Labs and Exercises</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#book-resources"><i class="fa fa-check"></i><b>1.3</b> Book Resources:</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-learning.html"><a href="statistical-learning.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#packages-used-in-this-chapter"><i class="fa fa-check"></i><b>2.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#what-is-statistical-learning"><i class="fa fa-check"></i><b>2.2</b> What is Statistical Learning?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#why-estimate-f"><i class="fa fa-check"></i><b>2.2.1</b> Why Estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#how-do-we-estimate-f"><i class="fa fa-check"></i><b>2.2.2</b> How do we estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-trade-off-between-prediction-accuracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.2.3</b> The Trade-Off Between Prediction Accuracy and Model Interpretability</a></li>
<li class="chapter" data-level="2.2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#supervised-versus-unsupervised-learning"><i class="fa fa-check"></i><b>2.2.4</b> Supervised Versus Unsupervised Learning</a></li>
<li class="chapter" data-level="2.2.5" data-path="statistical-learning.html"><a href="statistical-learning.html#regression-versus-classification-problems"><i class="fa fa-check"></i><b>2.2.5</b> Regression Versus Classification Problems</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.3</b> Assessing Model Accuracy</a><ul>
<li class="chapter" data-level="2.3.1" data-path="statistical-learning.html"><a href="statistical-learning.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.3.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.3.2" data-path="statistical-learning.html"><a href="statistical-learning.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>2.3.2</b> The Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="2.3.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-classification-setting"><i class="fa fa-check"></i><b>2.3.3</b> The Classification Setting</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#lab-introduction-to-r"><i class="fa fa-check"></i><b>2.4</b> Lab: Introduction to R</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-learning.html"><a href="statistical-learning.html#exercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#packages-used-in-this-chapter-1"><i class="fa fa-check"></i><b>3.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>3.2.1</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimate"><i class="fa fa-check"></i><b>3.2.2</b> Assessing the Accuracy of the Coefficient Estimate</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-model"><i class="fa fa-check"></i><b>3.2.3</b> Assessing the Accuracy of the Model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>3.3.1</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-regression.html"><a href="linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>3.3.2</b> Some Important Questions</a></li>
<li class="chapter" data-level="3.3.3" data-path="linear-regression.html"><a href="linear-regression.html#other-considerations-in-the-regression-model"><i class="fa fa-check"></i><b>3.3.3</b> Other Considerations in the Regression Model</a></li>
<li class="chapter" data-level="3.3.4" data-path="linear-regression.html"><a href="linear-regression.html#qualitative-predictors-with-more-than-two-levels"><i class="fa fa-check"></i><b>3.3.4</b> Qualitative Predictors with More than Two Levels</a></li>
<li class="chapter" data-level="3.3.5" data-path="linear-regression.html"><a href="linear-regression.html#extensions-of-the-linear-model"><i class="fa fa-check"></i><b>3.3.5</b> Extensions of the Linear Model</a></li>
<li class="chapter" data-level="3.3.6" data-path="linear-regression.html"><a href="linear-regression.html#potential-problems"><i class="fa fa-check"></i><b>3.3.6</b> Potential Problems</a></li>
<li class="chapter" data-level="3.3.7" data-path="linear-regression.html"><a href="linear-regression.html#the-marketing-plan"><i class="fa fa-check"></i><b>3.3.7</b> The Marketing Plan</a></li>
<li class="chapter" data-level="3.3.8" data-path="linear-regression.html"><a href="linear-regression.html#comparison-of-linear-regression-with-k-nearest-neighbors"><i class="fa fa-check"></i><b>3.3.8</b> Comparison of Linear Regression with <em>K</em>-Nearest Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-regression.html"><a href="linear-regression.html#lab-linear-regression"><i class="fa fa-check"></i><b>3.4</b> Lab: Linear Regression</a><ul>
<li class="chapter" data-level="3.4.1" data-path="linear-regression.html"><a href="linear-regression.html#fitting-a-linear-regression"><i class="fa fa-check"></i><b>3.4.1</b> Fitting a linear regression</a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression-1"><i class="fa fa-check"></i><b>3.4.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="3.4.3" data-path="linear-regression.html"><a href="linear-regression.html#interaction-terms"><i class="fa fa-check"></i><b>3.4.3</b> Interaction Terms</a></li>
<li class="chapter" data-level="3.4.4" data-path="linear-regression.html"><a href="linear-regression.html#non-linear-transformations-of-the-predictors"><i class="fa fa-check"></i><b>3.4.4</b> Non-linear Transformations of the Predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="linear-regression.html"><a href="linear-regression.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>4</b> Classification</a><ul>
<li class="chapter" data-level="4.1" data-path="classification.html"><a href="classification.html#packages-used-in-this-chapter-2"><i class="fa fa-check"></i><b>4.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="4.2" data-path="classification.html"><a href="classification.html#an-overview-of-classification"><i class="fa fa-check"></i><b>4.2</b> An Overview of Classification</a></li>
<li class="chapter" data-level="4.3" data-path="classification.html"><a href="classification.html#why-not-linear-regression"><i class="fa fa-check"></i><b>4.3</b> Why Not Linear Regression?</a></li>
<li class="chapter" data-level="4.4" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>4.4</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.4.1" data-path="classification.html"><a href="classification.html#the-logistic-model"><i class="fa fa-check"></i><b>4.4.1</b> The Logistic Model</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification.html"><a href="classification.html#estimating-the-regression-coefficients-1"><i class="fa fa-check"></i><b>4.4.2</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="4.4.3" data-path="classification.html"><a href="classification.html#making-predictions"><i class="fa fa-check"></i><b>4.4.3</b> Making Predictions</a></li>
<li class="chapter" data-level="4.4.4" data-path="classification.html"><a href="classification.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>4.4.4</b> Multiple Logistic Regression</a></li>
<li class="chapter" data-level="4.4.5" data-path="classification.html"><a href="classification.html#logistic-regression-for-2-response-classes"><i class="fa fa-check"></i><b>4.4.5</b> Logistic Regression for &gt;2 Response Classes</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>4.5</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="4.5.1" data-path="classification.html"><a href="classification.html#using-bayes-theorem-for-classification"><i class="fa fa-check"></i><b>4.5.1</b> Using Bayes’ Theorem for Classification</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1"><i class="fa fa-check"></i><b>4.5.2</b> Linear Discriminant Analysis for p = 1</a></li>
<li class="chapter" data-level="4.5.3" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1-1"><i class="fa fa-check"></i><b>4.5.3</b> Linear Discriminant Analysis for p &gt; 1</a></li>
<li class="chapter" data-level="4.5.4" data-path="classification.html"><a href="classification.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>4.5.4</b> Quadratic Discriminant Analysis</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/beaulucas/tidy_islr" target="blank">GitHub Repository</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tidy Introduction To Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistical-learning" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Statistical Learning</h1>
<hr />
<div id="packages-used-in-this-chapter" class="section level2">
<h2><span class="header-section-number">2.1</span> Packages used in this chapter</h2>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(knitr)</code></pre>
</div>
<div id="what-is-statistical-learning" class="section level2">
<h2><span class="header-section-number">2.2</span> What is Statistical Learning?</h2>
<p>Methods to estimate functions that connect inputs to outputs.</p>
<p>If there exists a quantitative response variable <span class="math inline">\(Y\)</span> and <span class="math inline">\(p\)</span> different predictors (<span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, …, <span class="math inline">\(X_p\)</span>), we can write this relationship as:</p>
<center>
<span class="math inline">\(Y = f(X) + ε\)</span>
</center>
<div id="why-estimate-f" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Why Estimate <em><span class="math inline">\(f\)</span></em>?</h3>
<div id="prediction" class="section level4">
<h4><span class="header-section-number">2.2.1.1</span> Prediction</h4>
<p>We can predict Y using:</p>
<center>
<span class="math inline">\(\hat{Y} = \hat{f}(X)\)</span>
</center>
<p>Accuracy of <span class="math inline">\(Y\)</span> is dependant on:</p>
<ul>
<li><em>reducible error</em>
<ul>
<li><span class="math inline">\(\hat{f}\)</span> will never be perfect estimate of <span class="math inline">\(f\)</span>, and model can always be potentially improved</li>
<li>Even if <span class="math inline">\(\hat{f} = f\)</span>, prediction would still have some error</li>
</ul></li>
<li><em>irreducible error</em>
<ul>
<li>Because <span class="math inline">\(Y\)</span> is also a function of random <span class="math inline">\(ε\)</span>, there will always be variability</li>
<li>We cannot reduce the error introduced by <span class="math inline">\(ε\)</span></li>
</ul></li>
</ul>
</div>
<div id="inference" class="section level4">
<h4><span class="header-section-number">2.2.1.2</span> Inference</h4>
<p>How does <span class="math inline">\(Y\)</span> respond to changes in <span class="math inline">\(X_1, X_2, ..., X_p\)</span>?</p>
</div>
</div>
<div id="how-do-we-estimate-f" class="section level3">
<h3><span class="header-section-number">2.2.2</span> How do we estimate <em><span class="math inline">\(f\)</span></em>?</h3>
<ul>
<li>Use <em>training data</em> to train method</li>
<li><span class="math inline">\(x_ij\)</span> is value of <span class="math inline">\(j\)</span>th predictor for observation <span class="math inline">\(i\)</span>, <span class="math inline">\(y_i\)</span> is value of response variable
<ul>
<li><span class="math inline">\(i = 1, 2, ..., n\)</span>, <span class="math inline">\(j = 1, 2, ..., p\)</span></li>
</ul></li>
<li>Using training data, apply statistical learning method estimate unknown function <span class="math inline">\(f\)</span></li>
<li>Most statistical learning methods can be characterized as either <em>parametric</em> or <em>non-parametric</em></li>
</ul>
<div id="parametric-methods" class="section level4">
<h4><span class="header-section-number">2.2.2.1</span> Parametric Methods</h4>
<p>Two-step model-based approach:</p>
<ol style="list-style-type: decimal">
<li>Make an assumption about functional form of <span class="math inline">\(f\)</span>, such as “<span class="math inline">\(f\)</span> is linear in <span class="math inline">\(X\)</span>”</li>
<li>Perform procedure that uses training data to train the model
* In case of linear model, this procedure estimates parameters <span class="math inline">\(β_0, β_1, ..., β_p\)</span>
* Most common approach to fit linear model is <em>(ordinary) least squares</em></li>
</ol>
<p>This is <em>parametric</em>, as it reduces the problem of estimating <span class="math inline">\(f\)</span> down to one of estimating a set of parameters. Problems that can arise:</p>
<ul>
<li>Model will not match the true unknown form of <span class="math inline">\(f\)</span></li>
<li>If model is made more <em>flexible</em>, which generally requires estimating a greater number of parameters, <em>overfitting</em> can occur</li>
</ul>
</div>
<div id="non-parametric-methods" class="section level4">
<h4><span class="header-section-number">2.2.2.2</span> Non-parametric Methods</h4>
<p>Non-parametric methods do not make assumptions about the form of <span class="math inline">\(f\)</span>. An advantage of this is that they have the potential to fit a wider range of possible shapes for <span class="math inline">\(f\)</span>. A disadvantage is that, because there are no assumptions about the form of <span class="math inline">\(f\)</span>, the problem of estimating <span class="math inline">\(f\)</span> is not reduced to a set number of parameters. This means more observations are needed compared to a parametric approach to estimate <span class="math inline">\(f\)</span> accurately.</p>
</div>
</div>
<div id="the-trade-off-between-prediction-accuracy-and-model-interpretability" class="section level3">
<h3><span class="header-section-number">2.2.3</span> The Trade-Off Between Prediction Accuracy and Model Interpretability</h3>
<p>Restrictive models are much more intepretable than flexible ones. Flexible approaches can be so complicated that it is hard to understand how predictors affect the response.</p>
<p>If inference is the goal, simple and inflexible methods are easier to interpret. For prediction, accuracy is the biggest concern. However, flexible models are more prone to overfitting.</p>
</div>
<div id="supervised-versus-unsupervised-learning" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Supervised Versus Unsupervised Learning</h3>
<p>Most machine learning methods can be split into <em>supervised</em> or <em>unsupervised</em> categories. Most of this textbook involves supervised learning methods, in which a model that captures the relationship between predictors and response measurements is fitted. The goal is to accurately predict the response variables for future observations, or to understand the relationship between the predictors and response.</p>
<p>Unsupervised learning takes place when we have a set of observations and a vector of measurements <span class="math inline">\(x_i\)</span>, but no response <span class="math inline">\(y_i\)</span>. We can examine the relationship between the variables or between the observations. A popular method of unsupervised learning is <a href="https://en.wikipedia.org/wiki/Cluster_analysis">cluster analysis</a>, in which observations are grouped into distinct groups based on their vector of measurements <span class="math inline">\(x_i\)</span>. An example of this would be a company segmenting survey respondents based on demographic data, in which the goal is to ascertain some idea about potential spending habits without possessing this data.</p>
<p>Clustering has some drawbacks. It works best when the groups are significantly distinct from each other. In reality, it is rare for data to exhibit this characteristic. There is often overlap between observations in different groups, and clustering will inevitably place a number of observations in the wrong groups. Further more, visualization of clusters breaks down as the dimensionality of data increases. Most data contains at least several, if not dozens, of variables.</p>
<p>It is not always clear-cut whether a problem should be handled with supervised or unsupervised learning. There are some scenarios where only a subset of the observations have response measurements. This is a <em>semi-supervised learning</em> problem, in which a statistical learning method that can utilize all observations is needed.</p>
</div>
<div id="regression-versus-classification-problems" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Regression Versus Classification Problems</h3>
<p>Variables can be categorized as either <em>quantitative</em> or <em>qualitative</em>. Both qualitative and quantatitive predictors can be used to predict both types of response variables. The more important part of choosing an appropriate statistical learning method is the type of the response variable.</p>
<hr />
</div>
</div>
<div id="assessing-model-accuracy" class="section level2">
<h2><span class="header-section-number">2.3</span> Assessing Model Accuracy</h2>
<p>Every data set is different and there is no one statistical learning method that works best for all data sets. It is important for any given data set to find the statistical learning method that produces the best results. This section presents some concepts that are part of that decision-making process.</p>
<div id="measuring-the-quality-of-fit" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Measuring the Quality of Fit</h3>
<p>We need to be able to quantify how well a model’s predictions match the observed data. How close are the model’s predicted response values to the true response values?</p>
<p>In regression, <a href="https://en.wikipedia.org/wiki/Mean_squared_error"><em>mean squared error (MSE)</em></a> is the most commonly-used measure. A small MSE indicates the predicted responses are very close to the true ones. MSE used on training data is more accurately referred to as the <em>training MSE</em>.</p>
<p>We are most concerned with the accuracy of the predictions when we apply our methods to <strong>previously unseen data</strong>. If you are trying to predict the value of a stock, your concern is how it performs in the future, not on known data from the past. Thus, the goal is then minimizing the <em>test MSE</em>, which measures the accuracy of a model on <strong>observations that were not used to train the model</strong>. Imagine a set of observations <span class="math inline">\((x_0, y_0)\)</span> that were not used to train the statistical learning method.</p>
<center>
<span class="math inline">\(Ave(y_0 - \hat{f}(x_0))2\)</span>
</center>
<p><br></p>
<p>The goal is to select the model that minimizes the test MSE shown above. How can we do this?</p>
<p>Sometimes, there is an available test data set full of observations that were not used in training the model. The test MSE can be evaluated on these observations, and the learning method which produces the smallest TSE will be chosen. If no test observations are available, picking the method that minimizes the training MSE might seem to be a good idea. However, there is no guarantee that a model with the lowest training MSE also has the lowest test MSE. Models often work in minimizing the training MSE, and can end up with large test MSE.</p>
<p>There is a tradeoff in model flexibility, training MSE, and test MSE. A model that is too flexible can closely match the training data, but perform poorly on the test data. There is a sweet spot to find between model flexibility, training MSE, and test MSE that varies for each unique data set.</p>
<p><em>Degrees of freedom</em> is a quantity that summarizes the flexibility of a curve, discused more fully in Chapter 7. The more inflexible a model is, the fewer degrees of freedom.</p>
<p>As model flexibility increases, training MSE will inevitably decrease, but test MSE may plateau or even rise. A model with a small training MSE and large test MSE is <em>overfitting the data</em>, picking up patterns on the training data that don’t exist in the test data. Since we expect the training MSE to almost always be lower than the test MSE, overfitting is a specific case when there exists a less flexible model with a smaller test MSE.</p>
</div>
<div id="the-bias-variance-trade-off" class="section level3">
<h3><span class="header-section-number">2.3.2</span> The Bias-Variance Trade-Off</h3>
<p>The expected test MSE can be broken down into the sum of three quantities:</p>
<ol style="list-style-type: decimal">
<li>the <em>variance</em> of <span class="math inline">\(\hat{f}(x_0)\)</span></li>
<li>the squared <em>bias</em> of <span class="math inline">\(\hat{f}(x_0)\)</span></li>
<li>the variance of the error terms ε</li>
</ol>
<center>
<span class="math inline">\(E(y_0 - \hat{f}(x_0)^2 = Var(\hat{f}(x_0)) + [Bias(\hat{f}(x_0))]^2+Var(ε)\)</span>
</center>
<p><br></p>
<p>The formula above defines the <em>expected test MSE</em>, which can be thought of the average test MSE that would be obtained if we repeatedly estimated <span class="math inline">\(f\)</span> and tested each at <span class="math inline">\(x_0\)</span>. To minimize expected test MSE, we need to choose a statistical learning method that achieves both low variance and low bias. Since variance and squared bias are nonnegative, the expected test MSE can never be lower than <span class="math inline">\(Var(ε)\)</span>, the irreducible error.</p>
<p><em>Variance</em> refers to how much <span class="math inline">\(\hat{f}\)</span> would change if repeatedly estimated with different training data sets. Methods with high variance can produce large changes in <span class="math inline">\(\hat{f}\)</span> through small changes in the training data. Generally, the more flexible a model it is, the higher the variance. Following the observations so closely can cause changes in just a single observation of the training data to result in significant changes to <span class="math inline">\(\hat{f}\)</span>. More inflexible models, such as linear regression, are less susceptible to the effects of changing a single observation.</p>
<p><em>Bias</em> is the error introduced from approximating a complicated problem by a much simpler model. Fitting a linear regression to data that is not linear will always lead to high bias, no matter how many observations are in the training set. More flexible models tend to result in less bias.</p>
<p>More flexible methods lead to higher variance and lower bias. The rate of change between the quantities determines at which point the test MSE is minimized. Bias tends to decrease at a faster rate in the beginning, causing the test MSE to decline. However, when flexibility reaches a certain point, variance will begin to increase faster than bias is decreasing, causing test MSE to rise.</p>
<p>This relationship between bias, variance, and test MSE is known as the <em>bias-variance tradeoff</em>. Here is a good article on it: <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">Understanding the Bias-Variance Tradeoff</a></p>
<p>In real-life scenarios where <span class="math inline">\(f\)</span> is unknown, we cannot explicitly compute the test MSE, bias, or variance. However, there are methods to estimate this, such as <em>cross-validation</em>, which will be discussed in Chapter 5.</p>
</div>
<div id="the-classification-setting" class="section level3">
<h3><span class="header-section-number">2.3.3</span> The Classification Setting</h3>
<p>For classification problems where <span class="math inline">\(y_i,...,y_n\)</span> are qualitative, we can quantify the accuracy of our estimate by using the <em>training error rate</em>, the proportion of mistakes that are made when applying our model <span class="math inline">\(\hat{f}\)</span> to the training observations.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(1/n\sum_{i=1}^nI(y_i \neq \hat{y_i})\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>training error rate</em>
</p>
</div>
<p>Breaking the formula above down.</p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> is the predicted class label for the <span class="math inline">\(i\)</span>th observation using <span class="math inline">\(\hat{f}\)</span></li>
<li><span class="math inline">\(I(y_i \neq \hat{y_i})\)</span> is an <em>indicator variable</em> that equals 1 if <span class="math inline">\(y_i \neq \hat{y_i}\)</span>, and 0 if <span class="math inline">\(y_i = \hat{y_i}\)</span></li>
<li>If <span class="math inline">\(I(y_i \neq \hat{y_i})\)</span> = 0, then the <span class="math inline">\(i\)</span>th observation was classified correctly</li>
</ul>
<p>Similar to our regression problems, we are more interested in the model’s performance on test observations not used in training. The formula below gives us the <em>test error rate</em> for a set of observations of the form <span class="math inline">\((x_0, y_0)\)</span>.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Ave(I(y_0 \neq \hat{y_0}))\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>test error rate</em>
</p>
</div>
<p>A good classifier will minimize the above.</p>
<div id="the-bayes-classifier" class="section level4">
<h4><span class="header-section-number">2.3.3.1</span> The Bayes Classifier</h4>
<p>The <em>test error rate</em> is minimized by the classifier that assigns each observation to the most likely class, given its predictor values. Our decision is then based on finding the value at which the formula below is largest.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Pr(Y = j|X = x_0)\)</span>
</p>
</div>
<p>If the response values are binomial (let’s call them A and B) the classifier simplifies to:</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Pr(Y = A|X = x_0) &gt; 0.5\:then\:A,\:else\:B\)</span>
</p>
</div>
<p>The <em>Bayes decision boundary</em> is the point where the probabilities are equal for both groups. Points on either side of this line are assigned to the group predicted by the classifier. The <em>Bayes error rate</em> averaged over all possible values of <span class="math inline">\(X\)</span> is below.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(1-E(max_jPr(Y = j|X))\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>Bayes error rate</em>
</p>
</div>
<p>The <em>Bayes error rate</em> is often greater than zero, as observations between classes overlap in real-world data.</p>
</div>
<div id="k-nearest-neighbors" class="section level4">
<h4><span class="header-section-number">2.3.3.2</span> K-Nearest Neighbors</h4>
<p>Since the true conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> cannot be known in real data, the Bayes classifier is used as a “gold standard” to compare other models to. Many methods attempt to estimate this conditional distribution, and then classify an observation based on the estimated probability. A common method is <em>K-nearest neighbors (KNN)</em>. Given a positive integer <span class="math inline">\(K\)</span> and a test observation <span class="math inline">\(x_0\)</span>, KNN then does the following:</p>
<ol style="list-style-type: decimal">
<li>identifies the <span class="math inline">\(K\)</span> points in the training data that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(N_0\)</span></li>
<li>estimates conditional probability for class <span class="math inline">\(j\)</span> as the fraction of the points in <span class="math inline">\(N_0\)</span> whose response values equal <span class="math inline">\(j\)</span>:</li>
</ol>
<div>
<p style="text-align:center">
<span class="math inline">\(Pr(Y = j| X = x_0) = 1/K\sum_{i\in N_0}I(y_i = j)\)</span>
</p>
</div>
<ol start="3" style="list-style-type: decimal">
<li>applies Bayes rule and classifies test observation <span class="math inline">\(x_0\)</span> to class with largest probability</li>
</ol>
<p>KNN can be surprisingly robust to the optimal Bayes classifier. The choice in <span class="math inline">\(K\)</span> makes a huge difference. For example, a <span class="math inline">\(K\)</span> = 1 is highly flexible, classifying observations based off of the closest nearby training observation. <span class="math inline">\(K\)</span> = 100 would do the opposite, basing its classification off a large pool of training observations compared to the <span class="math inline">\(K\)</span> = 1 version. The higher <span class="math inline">\(K\)</span> value produces a more linear model. The trade-off between flexibility, training error rate, and test error rate applies to both classification and regression problems.</p>
<hr />
</div>
</div>
</div>
<div id="lab-introduction-to-r" class="section level2">
<h2><span class="header-section-number">2.4</span> Lab: Introduction to R</h2>
<p>Finally we get to some <code>R</code> code. This chapter of ISLR introduces basic <code>R</code> syntax, and most of it is unchanged in my version. This should all be familiar to anyone who has used R before.</p>
<p>We are going to be working with <code>tibbles</code> as our primary data structure throughout this book. Please read here: <a href="http://r4ds.had.co.nz/tibbles.html">tibbles</a></p>
<div id="basic-commands" class="section level4">
<h4><span class="header-section-number">2.4.0.1</span> Basic Commands</h4>
<p>Skipping this.</p>
</div>
<div id="graphics-plotting" class="section level4">
<h4><span class="header-section-number">2.4.0.2</span> Graphics (Plotting)</h4>
<p>Here we begin to explore the “tidy” approach to R. We will abstain from base R plotting and use <code>ggplot2</code>, which is a more powerful tool. Let’s plot a scatterplot with some basic labels.</p>
<pre class="sourceCode r"><code class="sourceCode r">tbl_rnorm &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">x1 =</span> <span class="kw">rnorm</span>(<span class="dv">100</span>),
  <span class="dt">y1 =</span> <span class="kw">rnorm</span>(<span class="dv">100</span>)
)

<span class="kw">ggplot</span>(tbl_rnorm, <span class="kw">aes</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> y1)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;plot of x1 vs. y1&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;this is the x-axis&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;this is the y-axis&quot;</span>)</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-2-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<div id="indexing-data" class="section level4">
<h4><span class="header-section-number">2.4.0.3</span> Indexing data</h4>
<p>We will skip this.</p>
</div>
<div id="loading-data" class="section level4">
<h4><span class="header-section-number">2.4.0.4</span> Loading data</h4>
<p><em>ISLR</em> mentions insuring proper working directory before loading data. Dealing with working directories in R is a bad idea. Fortunately, it’s easily avoidable through the use of RStudio <em>projects</em>, which keep all files used in analysis together and make your work more robust and reproducible. See the <a href="http://r4ds.had.co.nz/workflow-projects.html">RStudio Projects</a> chapter in <em>r4ds</em> for more information.</p>
<p>We will opt for the <code>readr</code> (part of the <code>tidyverse</code>) package instead of base R. Take a look at this subsection of <em>r4ds</em> for reasons why:
<a href="http://r4ds.had.co.nz/data-import.html"><em>11.2.1 Compared to base R</em></a></p>
<p>Below is a reproducible example in which we create a tibble, save it as a .txt file, and then read it in with <code>write_tsv()</code>. The set of <code>read_*</code> functions in <code>readr</code> will be the standrad way to read local files into R. If you are using RStudio projects, there is no need to worry about working directories.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate dummy data to read in</span>
generic_company_tibble &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">x =</span> <span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dt">mean =</span> <span class="dv">25</span>),
  <span class="dt">y =</span> <span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dt">mean =</span> <span class="dv">50</span>),
  <span class="dt">z =</span> <span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;apple&quot;</span>, <span class="st">&quot;uber&quot;</span>, <span class="st">&quot;facebook&quot;</span>, <span class="st">&quot;twitter&quot;</span>, <span class="st">&quot;tesla&quot;</span>, <span class="st">&quot;google&quot;</span>, <span class="st">&quot;microsoft&quot;</span>), <span class="dv">1</span>)
)

tmp &lt;-<span class="st"> </span><span class="kw">tempfile</span>()
<span class="kw">write_tsv</span>(generic_company_tibble, tmp)
company_data &lt;-<span class="st"> </span><span class="kw">read_tsv</span>(tmp)</code></pre>
<p><code>readr</code> provides a nice summary of the imported tibble. Calling the tibble by name will also give a breakdown of column names, data types, and number of observations.</p>
<pre class="sourceCode r"><code class="sourceCode r">company_data</code></pre>
<pre><code>## # A tibble: 100 x 3
##       x     y z        
##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    
## 1  25.6  50.2 microsoft
## 2  25.8  49.9 microsoft
## 3  25.9  49.6 microsoft
## 4  26.3  48.1 microsoft
## 5  25.7  51.0 microsoft
## 6  24.1  50.2 microsoft
## # … with 94 more rows</code></pre>
</div>
<div id="additional-graphical-and-numerical-summaries" class="section level4">
<h4><span class="header-section-number">2.4.0.5</span> Additional Graphical and Numerical Summaries</h4>
<p>ISLR mentions the <code>attach()</code> function, which allows R to reference column names of dataframes without specifying the dataframe. <code>attach</code> can lead to confusion and errors when working on a project with multiple sources of data. This is a bad practice, and should always be avoided.</p>
<p>The book then goes into some explanation of <code>plot()</code>, which we will not be using.</p>
<hr />
</div>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">2.5</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li>For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method.
<ol style="list-style-type: lower-alpha">
<li>The sample size n is extremely large, and the number of predictors p is small
<ul>
<li><strong>(better)</strong> given large sample size, a flexible model would be able to capture a trend without being influenced too heavily by a small number of observations.</li>
</ul></li>
<li>The number of predictors p is extremely large, and the number of observations n is small.
<ul>
<li><strong>(worse)</strong> given the small sample size, an inflexible model would do better at not overfitting to a small number of observations (capturing patterns in the training data that dont really exist)</li>
</ul></li>
<li>The relationship between the predictors and response is highly non-linear.
<ul>
<li><strong>(better)</strong> highly flexible methods are highly non-linear and can produce better fits on non-linear data compared to inflexible methods such as linear regression</li>
</ul></li>
<li>The variance of the error terms, i.e. σ2 = Var(ε), is extremely high.
<ul>
<li><strong>(worse)</strong> given the high variance in the data, an inflexible method would overfit to the noise</li>
</ul></li>
</ol></li>
<li>Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.
<ul>
<li><strong>(regression; inference)</strong> This is a regression problem with both qualitative and quantitative predictors. Inference is the main goal, as the company probably wants a model that is human-readable in order to understand what determines a CEO’s salary.
<ul>
<li><code>n = 500, p = 3</code></li>
</ul></li>
</ul></li>
<li>We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.
<ul>
<li><strong>(classification; prediction)</strong> The goal is to classify whether a product will be a success or failure. Prediction is the goal, as they want to accurately determine if their product will succeed or fail.
<ul>
<li><code>n = 20, p = 4</code></li>
</ul></li>
</ul></li>
<li>We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.
<ul>
<li><strong>(regression; prediction)</strong> The goal is to predict the % change of the exchange rate.</li>
</ul></li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li>We now revisit the bias-variance decomposition</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.</li>
</ol>
<pre class="sourceCode r"><code class="sourceCode r">bias_variance &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">flexibility =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>),
  <span class="dt">bias =</span> <span class="kw">c</span>(<span class="dv">300</span>,<span class="dv">200</span>,<span class="dv">150</span>,<span class="dv">100</span>,<span class="dv">50</span>),
  <span class="dt">variance =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">25</span>,<span class="dv">125</span>,<span class="dv">250</span>,<span class="dv">500</span>),
  <span class="dt">train_error =</span> <span class="kw">c</span>(<span class="dv">350</span>,<span class="dv">250</span>,<span class="dv">200</span>, <span class="dv">125</span>, <span class="dv">50</span>),
  <span class="dt">irreducible_error =</span> <span class="dv">100</span>,
  <span class="dt">test_error =</span> variance <span class="op">+</span><span class="st"> </span>bias <span class="op">+</span><span class="st"> </span>irreducible_error) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(<span class="st">`</span><span class="dt">bias</span><span class="st">`</span>, <span class="st">`</span><span class="dt">variance</span><span class="st">`</span>, <span class="st">`</span><span class="dt">train_error</span><span class="st">`</span>, <span class="st">`</span><span class="dt">irreducible_error</span><span class="st">`</span>, <span class="st">`</span><span class="dt">test_error</span><span class="st">`</span>,
         <span class="dt">key =</span> <span class="st">&quot;measurement&quot;</span>, <span class="dt">value =</span> <span class="st">&quot;value&quot;</span>)

<span class="kw">ggplot</span>(bias_variance, <span class="kw">aes</span>(<span class="dt">x =</span> flexibility, <span class="dt">y =</span> value, <span class="dt">colour =</span> measurement)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">se =</span> <span class="ot">FALSE</span>, <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x,<span class="dv">3</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>()</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-5-1.png" width="576" style="display: block; margin: auto;" /></p>
<ol start="4" style="list-style-type: decimal">
<li>You will now think of some real-life applications for statistical learning.
<ol style="list-style-type: lower-alpha">
<li>Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.
<ol style="list-style-type: decimal">
<li>predicting diabetes.
<ul>
<li>response: future diabetes</li>
<li>predictors: health and body measurements of patient</li>
<li>goal: prediction, model complexity and human understanding is not important</li>
</ul></li>
<li>demographics that determine future education level
<ul>
<li>response: education level</li>
<li>predictors: demographic data</li>
<li>goal: inference, prediction is important here too but researchers would probably want to understand and share which factors determine the response in order to raise awareness</li>
</ul></li>
<li>faulty parts in manufacturing
<ul>
<li>response: whether or not part is faulty</li>
<li>predictors: various tests on part</li>
<li>goal: prediction, it is most important to have an accurate model, especially if faulty parts can lead to deaths</li>
</ul></li>
</ol></li>
<li>Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.
<ol style="list-style-type: decimal">
<li>number of riders on public transit over time
<ul>
<li>response: how many riders are expected to use public transit</li>
<li>predictors: current transit usage data, local population data, etc.</li>
<li>goal: prediction, it is important to prepare for growth in transit usage so governments have enough time to make necessary changes</li>
</ul></li>
<li>demand for product
<ul>
<li>response: how many units to expect to be sold</li>
<li>predictors: current demand data, revenue growth, business expansion, changing in market trends, economy health</li>
<li>goal: prediction, figure out in X amount of time demand for product in order to upsize/downsize to appropriate level</li>
</ul></li>
<li>determine future salary
<ul>
<li>response: future expeceted salary</li>
<li>predictors: employment history, education, geolocation, etc.</li>
<li>goal: inference, researchers might want to know the most important factors that lead to higher salaries rather than a model that is too complex to understand</li>
</ul></li>
</ol></li>
</ol></li>
<li>What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?</li>
</ol>
<ul>
<li>Very flexible approach allows you to fit a more flexible function to the data. The advantage is that you have the potential to accurately predict data even as it moves away from linearity. The disadvantage are potential overfitting to the training data, increasing variance (individual observations affect the model to higher degree than non-flexible counterpart), and higher computational costs (as well as less human-readable explanations)</li>
<li>When a more flexible approach is preferred: data that is non-linear, large sample size, prediction more important than inference</li>
<li>When a less flexible approach is preferred: data that is more linear, smaller sample size, inference more important than prediction</li>
</ul>
<ol start="6" style="list-style-type: decimal">
<li>Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a nonparametric approach)? What are its disadvantages?</li>
</ol>
<ul>
<li>A parametric approach assumes a form of <span class="math inline">\(f\)</span> and has to estimate an often known number of parameters (for example, linear regression simply requires estimating <span class="math inline">\(p+1\)</span> coefficients)</li>
<li>A non-parametric approach makes no assumptions about the true form of <span class="math inline">\(f\)</span>. They simply want to get as close as possible to <span class="math inline">\(f\)</span>. This allows them to take on a larger variety of shapes, and accomodate a larger variety of patterns.</li>
<li>Advantages of a parametric approach are that they take less observations to generate (the problem is reduced to applying a known form to <span class="math inline">\(f\)</span>, such as linear regression), are less inclined to overfit, and generally less computationally intensive.</li>
<li>Disadvantages of a parametric approach making assumptions about the form of <span class="math inline">\(f\)</span>, which may not match the real form and could lead to a model that doesn’t fit the data well</li>
</ul>
<ol start="7" style="list-style-type: decimal">
<li>The table below provides a training data set containing six observa- tions, three predictors, and one qualitative response variable.</li>
</ol>
<pre class="sourceCode r"><code class="sourceCode r">training_set &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">x1 =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),
  <span class="dt">x2 =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>),
  <span class="dt">x3 =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>),
  <span class="dt">y =</span> <span class="kw">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;red&quot;</span>,<span class="st">&quot;red&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;red&quot;</span>))

<span class="kw">kable</span>(training_set)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">x1</th>
<th align="right">x2</th>
<th align="right">x3</th>
<th align="left">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="left">red</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">red</td>
</tr>
<tr class="odd">
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="left">red</td>
</tr>
<tr class="even">
<td align="right">0</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="left">green</td>
</tr>
<tr class="odd">
<td align="right">-1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">green</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">red</td>
</tr>
</tbody>
</table>
<p>Suppose we wish to use this data set to make a prediction for <span class="math inline">\(Y\)</span> when <span class="math inline">\(X1 = X2 = X3 = 0\)</span> using K-nearest neighbors.</p>
<ol style="list-style-type: lower-alpha">
<li>Compute the Euclidean distance between each observation and the test point, <span class="math inline">\(X1 = X2 = X3 = 0\)</span>.</li>
</ol>
<p>The Euclidean Distance for three dimensions can be written as:</p>
<center>
<span class="math inline">\(d = \sqrt {\left( {x_1 - x_2 } \right)^2 + \left( {y_1 - y_2 } \right)^2 + \left( {z_1 - z_2 } \right)^2 }\)</span>
</center>
<p><br></p>
<p>Let’s write a function that can handle this in R, then use the <code>rowwise()</code> feature of dplyr to apply it across the rows of our tibble.</p>
<pre class="sourceCode r"><code class="sourceCode r">euc_dist &lt;-<span class="st"> </span><span class="cf">function</span>(x1, x2) <span class="kw">sqrt</span>(<span class="kw">sum</span>((x1 <span class="op">-</span><span class="st"> </span>x2) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))
training_set &lt;-<span class="st"> </span>training_set <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">rowwise</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">distance =</span> <span class="kw">euc_dist</span>(<span class="kw">c</span>(x1,x2,x3), <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>()

<span class="kw">kable</span>(training_set)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">x1</th>
<th align="right">x2</th>
<th align="right">x3</th>
<th align="left">y</th>
<th align="right">distance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="left">red</td>
<td align="right">3.000000</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">red</td>
<td align="right">2.000000</td>
</tr>
<tr class="odd">
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="left">red</td>
<td align="right">3.162278</td>
</tr>
<tr class="even">
<td align="right">0</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="left">green</td>
<td align="right">2.236068</td>
</tr>
<tr class="odd">
<td align="right">-1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">green</td>
<td align="right">1.414214</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">red</td>
<td align="right">1.732051</td>
</tr>
</tbody>
</table>
<ol start="2" style="list-style-type: lower-alpha">
<li>What is our prediction with K = 1? Why?</li>
</ol>
<p>Let’s find the <code>y</code> value of the single closest (<code>k = 1</code>) training observation.</p>
<pre class="sourceCode r"><code class="sourceCode r">training_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(distance <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(distance)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>()</code></pre>
<pre><code>## [1] &quot;green&quot;</code></pre>
<p>Since the closest observation in the training data is <code>green</code>, <code>K = 1</code> classifies our test observation as <code>green</code>.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>What is our prediction with K = 3? Why?</li>
</ol>
<p>First we find the three closest values. Then we measure the breakdown of <code>y</code> responses in this group of three observations. We find that two observations have value of <code>red</code>, and one has <code>green</code>. Given <code>red</code> has the highest probability of the two <code>y</code> values, we assign the training observation as <code>red</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">training_set <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">top_n</span>(<span class="dv">3</span>, <span class="op">-</span>distance) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">n =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">prop =</span> <span class="kw">n</span>()<span class="op">/</span><span class="kw">max</span>(n)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(prop <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(prop)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>(y)</code></pre>
<pre><code>## [1] &quot;red&quot;</code></pre>
<ol start="4" style="list-style-type: lower-alpha">
<li>If Bayes decision boundary is highly non-linear, then do we expect the best value of <span class="math inline">\(K\)</span> to be large or small? Why?
- We expect the value of <span class="math inline">\(K\)</span> to decline as the decision boundary grows more non-linear. A smaller value of <span class="math inline">\(K\)</span> is more suspectible to small changes between observations, which is the type of pattern highly non-linear decision boundary would depict.</li>
</ol>
<p>The R exercises are pretty basic after this. I am going to skip them for now.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["tidy_islr.pdf", "tidy_islr.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
