<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Classification | A Tidy Introduction To Statistical Learning</title>
  <meta name="description" content="Chapter 4 Classification | A Tidy Introduction To Statistical Learning" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Classification | A Tidy Introduction To Statistical Learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Classification | A Tidy Introduction To Statistical Learning" />
  
  
  

<meta name="author" content="Beau Lucas" />


<meta name="date" content="2019-12-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-regression.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Tidy Introduction To Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#an-overview-of-statistical-learning"><i class="fa fa-check"></i><b>1.1</b> An Overview of Statistical Learning</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#data-sets-used-in-labs-and-exercises"><i class="fa fa-check"></i><b>1.2</b> Data Sets Used in Labs and Exercises</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#book-resources"><i class="fa fa-check"></i><b>1.3</b> Book Resources:</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-learning.html"><a href="statistical-learning.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#packages-used-in-this-chapter"><i class="fa fa-check"></i><b>2.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#what-is-statistical-learning"><i class="fa fa-check"></i><b>2.2</b> What is Statistical Learning?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#why-estimate-f"><i class="fa fa-check"></i><b>2.2.1</b> Why Estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#how-do-we-estimate-f"><i class="fa fa-check"></i><b>2.2.2</b> How do we estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-trade-off-between-prediction-accuracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.2.3</b> The Trade-Off Between Prediction Accuracy and Model Interpretability</a></li>
<li class="chapter" data-level="2.2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#supervised-versus-unsupervised-learning"><i class="fa fa-check"></i><b>2.2.4</b> Supervised Versus Unsupervised Learning</a></li>
<li class="chapter" data-level="2.2.5" data-path="statistical-learning.html"><a href="statistical-learning.html#regression-versus-classification-problems"><i class="fa fa-check"></i><b>2.2.5</b> Regression Versus Classification Problems</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.3</b> Assessing Model Accuracy</a><ul>
<li class="chapter" data-level="2.3.1" data-path="statistical-learning.html"><a href="statistical-learning.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.3.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.3.2" data-path="statistical-learning.html"><a href="statistical-learning.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>2.3.2</b> The Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="2.3.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-classification-setting"><i class="fa fa-check"></i><b>2.3.3</b> The Classification Setting</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#lab-introduction-to-r"><i class="fa fa-check"></i><b>2.4</b> Lab: Introduction to R</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-learning.html"><a href="statistical-learning.html#exercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#packages-used-in-this-chapter-1"><i class="fa fa-check"></i><b>3.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>3.2.1</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimate"><i class="fa fa-check"></i><b>3.2.2</b> Assessing the Accuracy of the Coefficient Estimate</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-model"><i class="fa fa-check"></i><b>3.2.3</b> Assessing the Accuracy of the Model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>3.3.1</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-regression.html"><a href="linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>3.3.2</b> Some Important Questions</a></li>
<li class="chapter" data-level="3.3.3" data-path="linear-regression.html"><a href="linear-regression.html#other-considerations-in-the-regression-model"><i class="fa fa-check"></i><b>3.3.3</b> Other Considerations in the Regression Model</a></li>
<li class="chapter" data-level="3.3.4" data-path="linear-regression.html"><a href="linear-regression.html#qualitative-predictors-with-more-than-two-levels"><i class="fa fa-check"></i><b>3.3.4</b> Qualitative Predictors with More than Two Levels</a></li>
<li class="chapter" data-level="3.3.5" data-path="linear-regression.html"><a href="linear-regression.html#extensions-of-the-linear-model"><i class="fa fa-check"></i><b>3.3.5</b> Extensions of the Linear Model</a></li>
<li class="chapter" data-level="3.3.6" data-path="linear-regression.html"><a href="linear-regression.html#potential-problems"><i class="fa fa-check"></i><b>3.3.6</b> Potential Problems</a></li>
<li class="chapter" data-level="3.3.7" data-path="linear-regression.html"><a href="linear-regression.html#the-marketing-plan"><i class="fa fa-check"></i><b>3.3.7</b> The Marketing Plan</a></li>
<li class="chapter" data-level="3.3.8" data-path="linear-regression.html"><a href="linear-regression.html#comparison-of-linear-regression-with-k-nearest-neighbors"><i class="fa fa-check"></i><b>3.3.8</b> Comparison of Linear Regression with <em>K</em>-Nearest Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-regression.html"><a href="linear-regression.html#lab-linear-regression"><i class="fa fa-check"></i><b>3.4</b> Lab: Linear Regression</a><ul>
<li class="chapter" data-level="3.4.1" data-path="linear-regression.html"><a href="linear-regression.html#fitting-a-linear-regression"><i class="fa fa-check"></i><b>3.4.1</b> Fitting a linear regression</a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression-1"><i class="fa fa-check"></i><b>3.4.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="3.4.3" data-path="linear-regression.html"><a href="linear-regression.html#interaction-terms"><i class="fa fa-check"></i><b>3.4.3</b> Interaction Terms</a></li>
<li class="chapter" data-level="3.4.4" data-path="linear-regression.html"><a href="linear-regression.html#non-linear-transformations-of-the-predictors"><i class="fa fa-check"></i><b>3.4.4</b> Non-linear Transformations of the Predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="linear-regression.html"><a href="linear-regression.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>4</b> Classification</a><ul>
<li class="chapter" data-level="4.1" data-path="classification.html"><a href="classification.html#packages-used-in-this-chapter-2"><i class="fa fa-check"></i><b>4.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="4.2" data-path="classification.html"><a href="classification.html#an-overview-of-classification"><i class="fa fa-check"></i><b>4.2</b> An Overview of Classification</a></li>
<li class="chapter" data-level="4.3" data-path="classification.html"><a href="classification.html#why-not-linear-regression"><i class="fa fa-check"></i><b>4.3</b> Why Not Linear Regression?</a></li>
<li class="chapter" data-level="4.4" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>4.4</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.4.1" data-path="classification.html"><a href="classification.html#the-logistic-model"><i class="fa fa-check"></i><b>4.4.1</b> The Logistic Model</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification.html"><a href="classification.html#estimating-the-regression-coefficients-1"><i class="fa fa-check"></i><b>4.4.2</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="4.4.3" data-path="classification.html"><a href="classification.html#making-predictions"><i class="fa fa-check"></i><b>4.4.3</b> Making Predictions</a></li>
<li class="chapter" data-level="4.4.4" data-path="classification.html"><a href="classification.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>4.4.4</b> Multiple Logistic Regression</a></li>
<li class="chapter" data-level="4.4.5" data-path="classification.html"><a href="classification.html#logistic-regression-for-2-response-classes"><i class="fa fa-check"></i><b>4.4.5</b> Logistic Regression for &gt;2 Response Classes</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>4.5</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="4.5.1" data-path="classification.html"><a href="classification.html#using-bayes-theorem-for-classification"><i class="fa fa-check"></i><b>4.5.1</b> Using Bayes’ Theorem for Classification</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1"><i class="fa fa-check"></i><b>4.5.2</b> Linear Discriminant Analysis for p = 1</a></li>
<li class="chapter" data-level="4.5.3" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1-1"><i class="fa fa-check"></i><b>4.5.3</b> Linear Discriminant Analysis for p &gt; 1</a></li>
<li class="chapter" data-level="4.5.4" data-path="classification.html"><a href="classification.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>4.5.4</b> Quadratic Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="classification.html"><a href="classification.html#a-comparison-of-classification-methods"><i class="fa fa-check"></i><b>4.6</b> A Comparison of Classification Methods</a></li>
<li class="chapter" data-level="4.7" data-path="classification.html"><a href="classification.html#lab-logistic-regression-lda-qda-and-knn"><i class="fa fa-check"></i><b>4.7</b> Lab: Logistic Regression, LDA, QDA, and KNN</a><ul>
<li class="chapter" data-level="4.7.1" data-path="classification.html"><a href="classification.html#the-stock-market-data"><i class="fa fa-check"></i><b>4.7.1</b> The Stock Market Data</a></li>
<li class="chapter" data-level="4.7.2" data-path="classification.html"><a href="classification.html#logistic-regression-1"><i class="fa fa-check"></i><b>4.7.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.7.3" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-1"><i class="fa fa-check"></i><b>4.7.3</b> Linear Discriminant Analysis</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/beaulucas/tidy_islr" target="blank">GitHub Repository</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tidy Introduction To Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Classification</h1>
<hr />
<div id="packages-used-in-this-chapter-2" class="section level2">
<h2><span class="header-section-number">4.1</span> Packages used in this chapter</h2>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(tidymodels)
<span class="kw">library</span>(knitr)
<span class="kw">library</span>(kableExtra)
<span class="kw">library</span>(skimr)</code></pre>
<p>Linear regression in chapter 3 was concerned with predicting a quantitative response variable. What if the response variable is <em>qualitative</em>? Eye color is an example of a qualitative variable, which takes discrete value such as <code>blue</code>, <code>brown</code>, <code>green</code>. These are also referred to as <em>categorical</em>.</p>
<p>The approach of predicting qualitative responses is known as <em>classification</em>. Often, we predict the probability of the occurences of each category of a qualitative variable, and then make a decision based off of that.</p>
<p>In this chapter we discuss three of the most widely-used classifiers:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">linear discriminant analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"><em>k</em>-nearest neighbors</a></li>
</ul>
<p>We discuss more computer-intensive methods in later chapters.</p>
</div>
<div id="an-overview-of-classification" class="section level2">
<h2><span class="header-section-number">4.2</span> An Overview of Classification</h2>
<p>Classification is a common scenario.</p>
<ol style="list-style-type: decimal">
<li>Person arrives at ER exhibiting particular symptoms. What illness does he have?</li>
<li>Money is wired to an external account at a bank. Is this fraud?</li>
<li>Email is sent to your account. Is it legit, or spam?</li>
</ol>
<p>Similar to regression, we have a set of training observations that use to build a classifier. We also want the classifier to perform well on both training and test observations.</p>
<p>We will use the dataset <code>ISLR::Default</code>. First, let’s convert it to tidy format.</p>
<pre class="sourceCode r"><code class="sourceCode r">default &lt;-<span class="st"> </span>ISLR<span class="op">::</span>Default <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>()</code></pre>
<p>We are interested in the ability to predict whether an individual will default on their credit card payment, based on their credit card <code>balance</code> and annual income.</p>
<p>If we look at the summary statistics, we see the data is clean, and that very few people default on their balances.</p>
<pre class="sourceCode r"><code class="sourceCode r">default <span class="op">%&gt;%</span><span class="st"> </span>skimr<span class="op">::</span><span class="kw">skim</span>()</code></pre>
<pre><code>## Skim summary statistics
##  n obs: 10000 
##  n variables: 4 
## 
## ── Variable type:factor ─────────────────────────────
##  variable missing complete     n n_unique                 top_counts
##   default       0    10000 10000        2  No: 9667, Yes: 333, NA: 0
##   student       0    10000 10000        2 No: 7056, Yes: 2944, NA: 0
##  ordered
##    FALSE
##    FALSE
## 
## ── Variable type:numeric ────────────────────────────
##  variable missing complete     n     mean       sd     p0      p25
##   balance       0    10000 10000   835.37   483.71   0      481.73
##    income       0    10000 10000 33516.98 13336.64 771.97 21340.46
##       p50      p75     p100     hist
##    823.64  1166.31  2654.32 ▅▆▇▆▃▁▁▁
##  34552.64 43807.73 73554.23 ▁▆▆▆▇▅▁▁</code></pre>
<p>The scatterplot signals a strong relationship between <code>balance</code> and <code>default</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">default <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> balance, <span class="dt">y =</span> income, <span class="dt">fill =</span> default)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_hex</span>(<span class="dt">alpha =</span> <span class="dv">2</span><span class="op">/</span><span class="dv">3</span>)</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-73-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The boxplot captures the stark difference in <code>balance</code> between those who default and do not.</p>
<pre class="sourceCode r"><code class="sourceCode r">default <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> balance, <span class="dt">fill =</span> default)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_boxplot</span>()</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-74-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<div id="why-not-linear-regression" class="section level2">
<h2><span class="header-section-number">4.3</span> Why Not Linear Regression?</h2>
<p>Imagine we were trying to predict the medical outcome of a patient on the basis of their symptoms. Let’s say there are three possible diagnoses: <code>stroke</code>, <code>overdose</code>, and <code>seizure</code>. We could encode these into a quantitative variable <span class="math inline">\(Y\)</span>. that takes values from 1 to 3. Using least squares, we could then fit a regression model to predict <span class="math inline">\(Y\)</span>.</p>
<p>Unfortunately, this coding implies an ordering of the outcomes. It also insists that the difference between levels is quantitative, and equivalent across all sequences of levels.</p>
<p>Thus, changing the order of encodings would change relationship among the conditions, producing fundamentally different linear models.</p>
<p>There could be a case where a response variables took on a natural ordering, such as <code>mild</code>, <code>moderate</code>, <code>severe</code>. We would also need to believe that the gap between each level is equivalent. Unfortunately, there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is appropriate for linear regression.</p>
<p>For cases of <em>binary</em> qualitative response, we can utilize the dummy variable solution seen in Chapter 3. In this case, the order of the encodings is arbitrary.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> add latex for encoding</span></code></pre>
<p>Linear regression does work for this binary response scenario. However, it is possible for linear regression to produce estimates outside of the <code>[0, 1]</code> interval, which affects their interpretability as probabilities.</p>
<p>When the qualitative response has more than two levels, we need to use classification methods that are appropriate.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">4.4</span> Logistic Regression</h2>
<p>Let’s consider the <code>default</code> dataset. Rather than modeling this response <span class="math inline">\(Y\)</span> directly, logistic regression models the <em>probability</em> that <span class="math inline">\(Y\)</span> belongs to a particular category.</p>
<p>If we estimate using linear regression, we see that some estimated probabilities are negative. We are using the <code>tidymodels</code> package.</p>
<pre class="sourceCode r"><code class="sourceCode r">default &lt;-<span class="st"> </span>default <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">default_bool =</span> <span class="kw">if_else</span>(default <span class="op">==</span><span class="st"> &quot;Yes&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>))
lm_default &lt;-<span class="st"> </span><span class="kw">linear_reg</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> default, default_bool <span class="op">~</span><span class="st"> </span>balance)

default <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">predict</span>(lm_default, default)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> balance)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span>  .pred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> default_bool, <span class="dt">colour =</span> default_bool)) <span class="op">+</span>
<span class="st">  </span><span class="kw">guides</span>(<span class="dt">colour=</span><span class="ot">FALSE</span>)</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-76-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Below is the classification using logistic regression, where are probabilities fall between <code>0</code> and <code>1</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">logi_default &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>(<span class="dt">mode =</span> <span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> default, <span class="kw">as.factor</span>(default_bool) <span class="op">~</span><span class="st"> </span>balance)

default <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">predict</span>(logi_default, default, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> balance)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span>  .pred_<span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> default_bool, <span class="dt">colour =</span> default_bool)) <span class="op">+</span>
<span class="st">  </span><span class="kw">guides</span>(<span class="dt">colour=</span><span class="ot">FALSE</span>)</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-77-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Logistic regression in this example is modelling the probability of default, given the value of <code>balance</code>.</p>
<div>
<p style="text-align:center">
Pr(<code>default</code> = <code>Yes</code>|<code>balance</code>)
</p>
</div>
<p>These values, which we abbreviate as <em>p</em>(<code>balance</code>), range between <code>0</code> and <code>1</code>. Logistic regression will always produce an <em>S-shaped</em> curve. Regardless of the value of <span class="math inline">\(X\)</span>, we will receive a sensible prediction.</p>
<p>From this, we can make a classification prediction for <code>default</code>. Depending how conservative we are, the threshold for this could vary. Depending on the domain and context of the classification, a decision boundary around <code>0.5</code> or <code>0.1</code> might be appropriate.</p>
<div id="the-logistic-model" class="section level3">
<h3><span class="header-section-number">4.4.1</span> The Logistic Model</h3>
<p>The problem of using a linear regression model is evident in the chart above, where probabilities can fall below <code>0</code> or greater than <code>1</code>.</p>
<p>To avoid this, we must model <span class="math inline">\(p(X)\)</span> using a function that gives outputs between <code>0</code> and <code>1</code> for all values of <span class="math inline">\(X\)</span>. In logistic regression, we use the <em>logistic function</em>,</p>
<div>
<p style="text-align:center">
<span class="math inline">\(p(X) = \frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>logistic function</em>
</p>
</div>
<p>To fit the model, we use a method called <em>maximum likelihood</em>.</p>
<p>If we manipulate the logistic function, we find that</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\frac{p(X)}{1-p(X)} = e^{\beta_0+\beta_1X}\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>odds</em>
</p>
</div>
<p>This is called the <em>odds</em>, and takes any value from <span class="math inline">\(0\)</span> to <span class="math inline">\(\infty\)</span>. This is the same type of odds used in sporting events (“9:1 odds to win this match”, etc). If <span class="math inline">\(p(X) = 0.9\)</span>, then odds are <span class="math inline">\(\frac{0.9}{1-0.9} = 9\)</span>.</p>
<p>If we take the logarithm of the odds, we arrive at</p>
<div>
<p style="text-align:center">
<span class="math inline">\(log(\frac{p(X)}{1-p(X)}) = \beta_0+\beta_1X\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>log-odds  logit</em>
</p>
</div>
<p>The left-hande side is called the <em>log-odds</em> or <em>logit</em>. The logistic regression model has a logit that is linear in <span class="math inline">\(X\)</span>.</p>
<p>The contrast to linear regression is that increasing <span class="math inline">\(X\)</span> by one-unit changes the log odds by <span class="math inline">\(\beta_1\)</span> (or the odds by <span class="math inline">\(e^{\beta_1}\)</span>. However, since <span class="math inline">\(p(X)\)</span> and <span class="math inline">\(X\)</span> relationship is not a straight line (see plot above), <span class="math inline">\(\beta_1\)</span> does not correspond to the the change in <span class="math inline">\(p(X)\)</span> associated with a one-unit increase in <span class="math inline">\(X\)</span>. The amount that <span class="math inline">\(p(X)\)</span> changes depends on the current value of <span class="math inline">\(X\)</span>. See how the slope approaches <code>0</code> more and more slowly as <code>balance</code> increases.</p>
<p>Regardless of how much <span class="math inline">\(p(X)\)</span> moves, if <span class="math inline">\(\beta_1\)</span> is positive then increasing <span class="math inline">\(X\)</span> will be associated with increasing <span class="math inline">\(p(X)\)</span>. The opposite is also true.</p>
</div>
<div id="estimating-the-regression-coefficients-1" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Estimating the Regression Coefficients</h3>
<p>The coefficients in the logistic regression equation must be estimated used training data. Linear regression used the least squares approach to estimate the coefficients. It is possible to use non-linear least squares to fit the model, but <em>maximum likelihood</em> is preferred.</p>
<p>Maximum likelihood seeks to to find estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that the predicted probability <span class="math inline">\(\hat{p}(x_i)\)</span> of default for each individual corresponds as closely as possible to to the individual’s observed default status. We want estimates that produce low probabilities for individuals who did not default, and high probabilities for those who did.</p>
<p>We can formalize this with a <em>likelihood function</em>:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#TODO add likelihood function, links, etc</span></code></pre>
<p>We can examine the coefficients and other information from our logistic regression model.</p>
<pre class="sourceCode r"><code class="sourceCode r">logi_default <span class="op">%&gt;%</span><span class="st"> </span>broom<span class="op">::</span><span class="kw">tidy</span>()</code></pre>
<pre><code>## # A tibble: 2 x 5
##   term         estimate std.error statistic   p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept) -10.7      0.361        -29.5 3.62e-191
## 2 balance       0.00550  0.000220      25.0 1.98e-137</code></pre>
<p>If we look at the terms of our logistic regression, we see that the coefficient for <code>balance</code> is positive. This means that higher <code>balance</code> increases <span class="math inline">\(p(Default)\)</span>. A one-unit increase in <code>balance</code> will increase the log odds of defaulting by ~0.0055.</p>
<p>The test-statistic also behaves similarly. Coefficients with large statistics indicate evidence against the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>. For logistic regression, the null hypothesis implies that <span class="math inline">\(p(X) = \frac{e^{\beta_0}}{1+e^{\beta_0}}\)</span>, which means that the probability of defaulting does not depend on <code>balance.</code></p>
<p>Given the miniscule p-value associated with our <code>balance</code> coefficient, we can confidently reject <span class="math inline">\(H_0\)</span>. The intercept (<span class="math inline">\(\beta_0\)</span>) is typically not of interest; it’s main purpose is to adjust the average fitted probabilities to the proportion of ones in the data.</p>
</div>
<div id="making-predictions" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Making Predictions</h3>
<p>Once we have the coefficients, we simply compute the probability of <code>default</code> for any given observation.</p>
<p>Let’s take an individual with a <code>balance</code> of <code>$1000</code>. Using our model terms, we can compute the probability. Let’s extract the terms from the model and plug in a <code>balance</code> of <code>$1000</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">logi_coef &lt;-<span class="st"> </span>logi_default <span class="op">%&gt;%</span>
<span class="st">  </span>broom<span class="op">::</span><span class="kw">tidy</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># widen it and clean up names</span>
<span class="st">  </span><span class="kw">select</span>(term, estimate) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from =</span> term, <span class="dt">values_from =</span> estimate) <span class="op">%&gt;%</span>
<span class="st">  </span>janitor<span class="op">::</span><span class="kw">clean_names</span>()

logi_coef <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prob_1000 =</span> <span class="kw">exp</span>(intercept <span class="op">+</span><span class="st"> </span>balance <span class="op">*</span><span class="st"> </span><span class="dv">1000</span>) <span class="op">/</span>
<span class="st">           </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(intercept <span class="op">+</span><span class="st"> </span>balance <span class="op">*</span><span class="st"> </span><span class="dv">1000</span>)))</code></pre>
<pre><code>## # A tibble: 1 x 3
##   intercept balance prob_1000
##       &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;
## 1     -10.7 0.00550   0.00575</code></pre>
<p>We find the probability to be less than <code>1%</code>.</p>
<p>We can also incorporate qualitative predictors with the logistic regression model. Here we encode <code>student</code> in to the model.</p>
<pre class="sourceCode r"><code class="sourceCode r">logi_default_student &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>(<span class="dt">mode =</span> <span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> default, <span class="kw">as.factor</span>(default_bool) <span class="op">~</span><span class="st"> </span>student)

logi_default_student <span class="op">%&gt;%</span><span class="st"> </span>broom<span class="op">::</span><span class="kw">tidy</span>()</code></pre>
<pre><code>## # A tibble: 2 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   -3.50     0.0707    -49.6  0       
## 2 studentYes     0.405    0.115       3.52 0.000431</code></pre>
<p>This model indicates that students have a higher rate of defaulting compared to non-students.</p>
</div>
<div id="multiple-logistic-regression" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Multiple Logistic Regression</h3>
<p>We now consider the scenario of multiple predictors.</p>
<p>We can rewrite <span class="math inline">\(p(X)\)</span> as</p>
<div>
<p style="text-align:center">
<span class="math inline">\(p(X) = \frac{e^{\beta_0+\beta_1X_1+...+\beta_pX_p}}{1+e^{\beta_0+\beta_1X_1+...+\beta_pX_p}}\)</span>
</p>
<p class="vocab" style="text-align:right">
/p&gt;
</div>
<p>And again use the maximum likelihood method to estimate the coefficients.</p>
<p>Let’s estimate <code>balance</code> using <code>balance</code>, <code>income</code> and <code>student</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">multiple_logi_default&lt;-<span class="st"> </span><span class="kw">logistic_reg</span>(<span class="dt">mode =</span> <span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> default, <span class="kw">as.factor</span>(default_bool) <span class="op">~</span><span class="st"> </span>balance <span class="op">+</span><span class="st"> </span>student <span class="op">+</span><span class="st"> </span>income)

multiple_logi_default <span class="op">%&gt;%</span><span class="st"> </span>broom<span class="op">::</span><span class="kw">tidy</span>()</code></pre>
<pre><code>## # A tibble: 4 x 5
##   term            estimate  std.error statistic   p.value
##   &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept) -10.9        0.492        -22.1   4.91e-108
## 2 balance       0.00574    0.000232      24.7   4.22e-135
## 3 studentYes   -0.647      0.236         -2.74  6.19e-  3
## 4 income        0.00000303 0.00000820     0.370 7.12e-  1</code></pre>
<p>Notice that being a student now <em>decreases</em> the chances of default, whereas in our previous model (which only contained <code>student</code> as a predictor), it increased the chances.</p>
<p>Why is that? This model is showing that, for a fixed value of <code>income</code> and <code>balance</code>, students actually default less. This is because <code>student</code> and <code>balance</code> are correlated.</p>
<pre class="sourceCode r"><code class="sourceCode r">default <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> balance, <span class="dt">fill =</span> student)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_boxplot</span>()</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-83-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>If we plot the distribution of <code>balance</code> across <code>student</code>, we see that students tend to carry larger credit card balances.</p>
<p>This example illustrates the dangers of drawing insights from single predictor regressions when other predictors may be relevant. The results from using one predictor can be substantially different compared to using multiple predictors. This phenomenon is known as <em>confounding</em>.</p>
</div>
<div id="logistic-regression-for-2-response-classes" class="section level3">
<h3><span class="header-section-number">4.4.5</span> Logistic Regression for &gt;2 Response Classes</h3>
<p>Sometimes we wish to classify a response variable that has more than two classes. This could be the medical example where a patient outcomes falls into <code>stroke</code>, <code>overdose</code>, and <code>seizure</code>. It is possible to extend the two-class logistic regression model into multiple-class, but this is not used often in practice.</p>
<p>A method that is popular for multi-class classification is <em>discriminant analysis</em>.</p>
</div>
</div>
<div id="linear-discriminant-analysis" class="section level2">
<h2><span class="header-section-number">4.5</span> Linear Discriminant Analysis</h2>
<p>Logistic regression models the distribution of response <span class="math inline">\(Y\)</span> given the predictor(s) <span class="math inline">\(X\)</span>. In discriminant analysis, we model the distribution of the predictors <span class="math inline">\(X\)</span> in each of the response classes, and then use Bayes’ theorem to flip these around into estimates for <span class="math inline">\(Pr(Y = k|X = x)\)</span>.</p>
<p>Why do we need this method?</p>
<ul>
<li><p>Well-separated classes produce unstable parameter estimates for logistic regression models</p></li>
<li><p>If <span class="math inline">\(n\)</span> is small and distribution of predictors <span class="math inline">\(X\)</span> is normall across the classes, the linear discriminant model is more stable than logistic regression</p></li>
</ul>
<div id="using-bayes-theorem-for-classification" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Using Bayes’ Theorem for Classification</h3>
<p>Consider the scenario where we want to classify an observation into one of <span class="math inline">\(K\)</span> classes, where <span class="math inline">\(K &gt;= 2\)</span>.</p>
<ul>
<li>Let <span class="math inline">\(\pi_k\)</span> represent the overall or <em>prior</em> probability that a randomly chosen observation comes from the <span class="math inline">\(k\)</span>th class</li>
<li>Let <span class="math inline">\(f_k(x) = Pr(X = x|Y = k)\)</span> denote the <em>density function</em> of <span class="math inline">\(X\)</span> for an observation that comes from the <span class="math inline">\(k\)</span>th class.</li>
</ul>
<p>In other words, <span class="math inline">\(f_k(x)\)</span> being large means that there is a high probability that an observation in the <span class="math inline">\(k\)</span>th class has <span class="math inline">\(X \approx x\)</span>.</p>
<p>We can use Bayes’ theorem</p>
<p><span class="math display">\[
\operatorname{Pr}(Y=k | X=x)=\frac{\pi_{k} f_{k}(x)}{\sum_{l=1}^{K} \pi_{l} f_{l}(x)}
\]</span></p>
<p>And call the left-hand side <span class="math inline">\(p_k(X)\)</span>. We can plug in estimates of <span class="math inline">\(\pi_k\)</span> and <span class="math inline">\(f_k(X)\)</span> into Bayes’ theorem above to get the probability of a certain class, given an observation.</p>
<ul>
<li>Solving for <span class="math inline">\(\pi_k\)</span> is easy if we have a random sample of <span class="math inline">\(Y\)</span>s from the population. We simply calculate the fraction of observations that fall into a <span class="math inline">\(k\)</span> class.</li>
<li>Estimating <span class="math inline">\(f_k(X)\)</span> is more challenging unless we assume simple forms for these densities</li>
</ul>
<p>We refer to <span class="math inline">\(p_k(x)\)</span> as the posterior probability that an observation <span class="math inline">\(X = x\)</span> belongs to the <span class="math inline">\(k\)</span>th class. This is the probability that the observation belongs to the <span class="math inline">\(k\)</span>th class, <em>given</em> the predictor value for that observation.</p>
<p>The Bayes’ classifier classifies an observation to the class for which <span class="math inline">\(p_k(X)\)</span> is largest. If we can find a way to estimate <span class="math inline">\(f_k(X)\)</span>, we can develop a classifier that approximates the Bayes classifier.</p>
</div>
<div id="linear-discriminant-analysis-for-p-1" class="section level3">
<h3><span class="header-section-number">4.5.2</span> Linear Discriminant Analysis for p = 1</h3>
<p>Let’s assume we have one predictor. We need to obtain an estimate for <span class="math inline">\(f_k(x)\)</span> (the density function for <span class="math inline">\(X\)</span> given a class <span class="math inline">\(k\)</span>). This will obtain a value for <span class="math inline">\(p_k(x)\)</span>. We will then classify this observation for which <span class="math inline">\(p_k(x)\)</span> is greatest.</p>
<p>To estimate <span class="math inline">\(f_k(x)\)</span>, we need to make some assumptions about its form.</p>
<p>Let’s assume <span class="math inline">\(f_k(x)\)</span> is <em>normal</em> or <em>Gaussian</em>. The normal density takes the form</p>
<p><span class="math display">\[
f_{k}(x)=\frac{1}{\sqrt{2 \pi} \sigma_{k}} \exp \left(-\frac{1}{2 \sigma_{k}^{2}}\left(x-\mu_{k}\right)^{2}\right)
\]</span></p>
<p>Plugging this back in to <span class="math inline">\(p_k(x)\)</span>, we obtain</p>
<p><span class="math display">\[
p_{k}(x)=\frac{\pi_{k} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2 \sigma^{2}}\left(x-\mu_{k}\right)^{2}\right)}{\sum_{l=1}^{K} \pi_{l} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2 \sigma^{2}}\left(x-\mu_{l}\right)^{2}\right)}
\]</span></p>
<p>Taking the log and rearranging results in</p>
<p><span class="math display">\[
\delta_{k}(x)=x \cdot \frac{\mu_{k}}{\sigma^{2}}-\frac{\mu_{k}^{2}}{2 \sigma^{2}}+\log \left(\pi_{k}\right)
\]</span></p>
<p>In this case, the Bayes decision boundary corresponds to</p>
<p><span class="math display">\[
x=\frac{\mu_{1}^{2}-\mu_{2}^{2}}{2\left(\mu_{1}-\mu_{2}\right)}=\frac{\mu_{1}+\mu_{2}}{2}
\]</span></p>
<p>We can simulate some data to show a simple example.</p>
<p>In this data we have two classes:</p>
<ul>
<li><span class="math inline">\(\mu_1 = -1.25, \mu_2 = 1.25, \sigma_1^2 = \sigma_2^2 = 1\)</span></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r">var_<span class="dv">1</span> =<span class="st"> </span><span class="dv">1</span>
var_<span class="dv">2</span> =<span class="st"> </span>var_<span class="dv">1</span>
f_<span class="dv">1</span> =<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">fun =</span> <span class="st">&quot;f_1&quot;</span>, <span class="dt">x =</span> <span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">10000</span>, <span class="dt">mean =</span> <span class="fl">-1.25</span>, <span class="dt">sd =</span> var_<span class="dv">1</span>))
f_<span class="dv">2</span> =<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">fun =</span> <span class="st">&quot;f_2&quot;</span>, <span class="dt">x =</span> <span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">10000</span>, <span class="dt">mean =</span> <span class="fl">1.25</span>, <span class="dt">sd =</span> var_<span class="dv">2</span>))
f_x =<span class="st"> </span><span class="kw">bind_rows</span>(f_<span class="dv">1</span>, f_<span class="dv">2</span>)

<span class="co"># add summary statistics</span>

f_x &lt;-<span class="st"> </span>f_x <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(fun) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pi =</span> <span class="kw">n</span>(),
         <span class="dt">var =</span> <span class="kw">var</span>(x),
         <span class="dt">mu =</span> <span class="kw">mean</span>(x)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pi =</span> pi <span class="op">/</span><span class="st"> </span><span class="kw">n</span>())

decision_boundary &lt;-<span class="st"> </span>f_x <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(fun) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">mu =</span> <span class="kw">mean</span>(x)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">decision_boundary =</span> <span class="kw">sum</span>(mu) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>()
f_x <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">colour =</span> fun)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> decision_boundary, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>)</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-84-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>These two densities overlap, and so given <span class="math inline">\(X = x\)</span>, we still have uncertaintly about which class the observation belongs to. If both classes are equally likely for a random observation <span class="math inline">\(\pi_1 = \pi_2\)</span>, then we see the Bayes classifier assigns the observation to class 1 if <span class="math inline">\(x &lt; 0\)</span> and class 2 otherwise.</p>
<p>Even if we are sure that <span class="math inline">\(X\)</span> is drawn from a Gaussian distribution within each class, we still need to estimate <span class="math inline">\(\mu_1,...,\mu_k\)</span>, <span class="math inline">\(\pi_1,...,\pi_k\)</span>, and <span class="math inline">\(\sigma^2\)</span>. The <em>linear discriminant analysis</em> method approximates these by plugging in estimates as follows</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat{\mu}_k = \frac{1}{n_k}\sum_{i:y_i=k}{x_i}\)</span>
</p>
<p class="vocab" style="text-align:right">
*
</p>
</div>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat{\sigma}^2 = \frac{1}{n-K}\sum_{k=1}^{K}\sum_{i:y_i=k}{(x_i-\hat{\mu}_k)^2}\)</span>
</p>
<p class="vocab" style="text-align:right">
*
</p>
</div>
<p>The estimate for <span class="math inline">\(\hat{\mu}_k\)</span> is the average of all training observations from the <span class="math inline">\(k\)</span>th class. The estimate for <span class="math inline">\(\hat{\sigma}^2\)</span> is the weighted average of the sample variances for each of the K classes.</p>
<p>To estimate <span class="math inline">\(\hat{\pi}_k\)</span>, we simply take the proportion of training observations that belong to the <span class="math inline">\(k\)</span>th class</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat{\pi}_k = n_k/n\)</span>
</p>
<p class="vocab" style="text-align:right">
*
</p>
</div>
<p>From these estimates, we can achieve a decision boundary</p>
<p><span class="math display">\[
\hat{\delta}_{k}(x)=x \cdot \frac{\hat{\mu}_{k}}{\hat{\sigma}^{2}}-\frac{\hat{\mu}_{k}^{2}}{2 \hat{\sigma}^{2}}+\log \left(\hat{\pi}_{k}\right)
\]</span></p>
<p>This classifier has <em>linear</em> in the name due to the fact that the <em>discriminant function</em> above are linear functions of <span class="math inline">\(x\)</span>.</p>
<p>Let’s take a sample from our earlier distribution and see how it performs.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(discrim)
f_sample =<span class="st"> </span>f_x <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_frac</span>(<span class="dt">size =</span> <span class="fl">0.01</span>)

lda_f &lt;-<span class="st"> </span>discrim<span class="op">::</span><span class="kw">discrim_linear</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> f_sample, <span class="kw">as.factor</span>(fun) <span class="op">~</span><span class="st"> </span>x)

preds &lt;-<span class="st"> </span><span class="kw">predict</span>(lda_f, f_sample, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)

f_sample &lt;-<span class="st"> </span>f_sample <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_cols</span>(preds)
<span class="co"># </span><span class="al">TODO</span><span class="co"> figure out how to truly extract decision boundary from MASS::lda</span>
est_decision &lt;-<span class="st"> </span>f_sample <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(x) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(.pred_class <span class="op">==</span><span class="st"> &#39;f_2&#39;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(x)

<span class="kw">ggplot</span>(f_sample, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">fill =</span> fun)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> est_decision, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">0</span>)</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-85-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Notice the estimated decision boundary (dashed line) being very close to the Bayes decision boundary.</p>
<div id="measuring-performance" class="section level4">
<h4><span class="header-section-number">4.5.2.1</span> Measuring Performance</h4>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> show lda performance compared to true value</span></code></pre>
</div>
</div>
<div id="linear-discriminant-analysis-for-p-1-1" class="section level3">
<h3><span class="header-section-number">4.5.3</span> Linear Discriminant Analysis for p &gt; 1</h3>
<p>We can extend LDA classifier to multiple predictors.</p>
<p>The multivariate Gaussian distribution assumes that each predictor follows a one-dimensional normal distribution, with some correlation between each pair of predictors.</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=JjB58InuTqM">Andrew Ng on Multivariate Gaussian Distribution</a></li>
</ul>
<p>To indicate that a <span class="math inline">\(p\)</span>-dimensional random variable <span class="math inline">\(X\)</span> has a multi-variate Gaussian distribution, we write $ X N(, )$</p>
<ul>
<li><span class="math inline">\(E(X) = \mu\)</span> is the mean of <span class="math inline">\(X\)</span> (a vector with <span class="math inline">\(p\)</span> components)</li>
<li><span class="math inline">\(Cov(X) = \Sigma\)</span> is the <span class="math inline">\(p*p\)</span> covariance matrix of <span class="math inline">\(X\)</span>.</li>
</ul>
<p>The multivariate Gaussian density is defined as</p>
<p><span class="math display">\[
f(x)=\frac{1}{(2 \pi)^{p / 2}|\mathbf{\Sigma}|^{1 / 2}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \mathbf{\Sigma}^{-1}(x-\mu)\right)
\]</span></p>
<p>In the case of <span class="math inline">\(p&gt;1\)</span> predictors, the LDA classifier assumes that the observations in the <span class="math inline">\(k\)</span>th class are drawn from a multivariate Gaussian distribution <span class="math inline">\(N(\mu_k, \Sigma)\)</span>, where <span class="math inline">\(\mu_k\)</span> is a class-specific mean vector, and <span class="math inline">\(\Sigma\)</span> is the covariance matrix that is common to all <span class="math inline">\(K\)</span> classes.</p>
<p>Plugging the density function for the <span class="math inline">\(k\)</span>th class, <span class="math inline">\(f_k(X = x)\)</span>, into</p>
<p><span class="math display">\[
\operatorname{Pr}(Y=k | X=x)=\frac{\pi_{k} f_{k}(x)}{\sum_{l=1}^{K} \pi_{l} f_{l}(x)}
\]</span></p>
<p>and performing some algebra reveals that the Bayes classifier will assign observation <span class="math inline">\(X = x\)</span> by identifying the class for which</p>
<p><span class="math display">\[
\delta_{k}(x)=x^{T} \boldsymbol{\Sigma}^{-1} \mu_{k}-\frac{1}{2} \mu_{k}^{T} \boldsymbol{\Sigma}^{-1} \mu_{k}+\log \pi_{k}
\]</span></p>
<p>is largest.</p>
<div id="performing-lda-on-default-data" class="section level4">
<h4><span class="header-section-number">4.5.3.1</span> Performing LDA on Default data</h4>
<p>If we run an LDA model on our <code>default</code> dataset, predicting the probability of <code>default</code> based off of <code>student</code> and <code>balance</code>, we achieve a respectable <code>3.0%</code> error rate.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
default_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(default, <span class="dt">prop =</span> <span class="dv">3</span><span class="op">/</span><span class="dv">4</span>)
train_default &lt;-<span class="st"> </span><span class="kw">training</span>(default_split)
test_default &lt;-<span class="st"> </span><span class="kw">testing</span>(default_split)

lda_default &lt;-<span class="st"> </span>discrim<span class="op">::</span><span class="kw">discrim_linear</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> train_default, default <span class="op">~</span><span class="st"> </span>student <span class="op">+</span><span class="st"> </span>balance)

preds &lt;-<span class="st"> </span><span class="kw">predict</span>(lda_default, test_default, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)

<span class="co"># error rate</span>
test_default <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(preds) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">metrics</span>(<span class="dt">truth =</span> default, <span class="dt">estimate =</span> .pred_class)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.97 
## 2 kap      binary         0.369</code></pre>
<p>While this may seem impressive, let’s remember that only <code>3.6%</code> of observations in the dataset end up in default. This means that if we assigned a <em>null</em> classifier, which simply predicted every observation to not end in default, our error rate would be <code>3.6%</code>. This is worse, but not by much, compared to our LDA error rate.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># null error rate</span>
test_default <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(default) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prop =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n))</code></pre>
<pre><code>## # A tibble: 2 x 3
##   default     n  prop
##   &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt;
## 1 No       2410 0.964
## 2 Yes        90 0.036</code></pre>
<p>Binary decision makers can make to types of errors:</p>
<ul>
<li>Incorrectly assigning an individual who defaults to the “no default” category</li>
<li>Incorrectly assigning an individual who doesn’t default to the “default” category.</li>
</ul>
<p>We can identify the breakdown by using a <em>confusion matrix</em></p>
<pre class="sourceCode r"><code class="sourceCode r">cm &lt;-<span class="st"> </span>test_default <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(preds) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">conf_mat</span>(<span class="dt">truth =</span> default, <span class="dt">estimate =</span> .pred_class)

cm</code></pre>
<pre><code>##           Truth
## Prediction   No  Yes
##        No  2402   67
##        Yes    8   23</code></pre>
<p>We see that our LDA only predicted <code>31</code> people to default. Of these, <code>23</code> actually defaulted. So, only <code>8</code> of out of the <code>~7500</code> people who did not default were incorrectly labeled.</p>
<p>However, of the <code>90</code> people in our test set who defaulted, we only predicted this correctly for <code>23</code> of them. That means <code>~75%</code> of individuals who default were incorrectly classified. Having an error rate this high for the problematic class (those who default) is unacceptable.</p>
<p>Class-specific performance is an important concept. <em>Sensitivity</em> and <em>specificity</em> characterize the performance of a classifier or screening test. In this case, the sensitivity is the percentage of true defaults who are identified (a low <code>~25%</code>). The specificity is the percentage of non-defaulters who are correctly identified (<code>7492/7500 ~ 99.9%</code>).</p>
<p>Remember that LDA is trying to approximate the Bayes classifier, which has the lowest <em>total</em> error rate out of all classifiers (assuming Gaussian assumption is correct). The classifier will yield the smallest total number of misclassifications, regardless of which class the errors came from. In this credit card scenario, the credit card company might wish to avoid incorrectly misclassifying a user who defaults. In this case, they value sensitivity. For them, the cost of misclassifying a defaulter is higher than the cost of misclassifying a non-defaulter (which they still desire to avoid).</p>
<p>It’s possible to modify LDA for such circumstances. Given the Bayes classifier works by assigning an observation to a class in which the posterior probability <span class="math inline">\(p_k(X)\)</span> is greatest (in the two-class scenario, this decision boundary is at <code>0.5</code>), we can modify the probability threshold to suit our needs. If we wish to increase our sensitivity, we can lower this threshold.</p>
<p>Imagine we lowered the threshold to <code>0.2</code>. Sure, we would classify more people as defaulters than before (decreasing our specificity) but we would also catch more defaulters we previously missed (increasing our sensitivity).</p>
<pre class="sourceCode r"><code class="sourceCode r">preds &lt;-<span class="st"> </span><span class="kw">predict</span>(lda_default, test_default, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)

<span class="co"># error rate</span>
test_default <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(preds) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.pred_class =</span> <span class="kw">as.factor</span>(<span class="kw">if_else</span>(.pred_Yes <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.2</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">conf_mat</span>(<span class="dt">truth =</span> default, <span class="dt">estimate =</span> .pred_class)</code></pre>
<pre><code>##           Truth
## Prediction   No  Yes
##        No  2357   37
##        Yes   53   53</code></pre>
<p>Now our sensitivy has increased. Of the <code>90</code> people who defaulted, we correctly identified <code>53, or ~58.8%</code> of them (up from <code>~25%</code> previously).</p>
<p>This came at a cost, as our specificity decreased. This time, we predicted <code>106</code> people to default. Of those, <code>53</code> actually defaulted. This means that <code>53</code> of the <code>7500</code> people who didn’t default were incorrectly labelled. This gives us a specificity of (<code>7447/7500 ~ 99.2%</code>)</p>
<p>Despite the overall increase in error rate, the lower threshold may be chosen, depending on the context of the problem. To make a decision, an extensive amount of <em>domain knowledge</em> is required.</p>
<p>The <em>ROC curve</em> is a popular graphic for displaying the two types of errors for all possible thresholds. “ROC” stands for <em>receiver operating characteristics</em>.</p>
<p>The overall performance of a classifier, summarized over all possible thresholds, is given by the <em>area under the (ROC) curve</em> (AUC). An ideal ROC curve will hug the top left corner. Think of it this way: ideal ROC curves are able to increase sensitivity at a much higher rate than reduction in specificity.</p>
<p>We can use <code>yardstick::</code> (part of <code>tidymodels::</code>) to plot an ROC curve.</p>
<pre class="sourceCode r"><code class="sourceCode r">test_default <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(preds) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">roc_curve</span>(default, .pred_Yes) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">autoplot</span>()</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-91-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>We can think of the <em>sensitivity</em> as the true positive, and <em>1 - specificity</em> as the false positive.</p>
</div>
</div>
<div id="quadratic-discriminant-analysis" class="section level3">
<h3><span class="header-section-number">4.5.4</span> Quadratic Discriminant Analysis</h3>
<p>LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution, with a class-specific mean vector and a covariance matrix that is common to all <span class="math inline">\(K\)</span> classes. <em>Quadratic discriminant analysis</em> (QDA) assumes that class has its own covariance matrix.</p>
<p>It assumes that each observation from the <span class="math inline">\(k\)</span>th class has the form <span class="math inline">\(X \sim N(\mu_k, \Sigma_k)\)</span>, where <span class="math inline">\(\Sigma_k\)</span> is a covariance matrix for the <span class="math inline">\(k\)</span>th class. Under this assumption, the Bayes classifier assigns an observation <span class="math inline">\(X=x\)</span> to the class for which</p>
<p><span class="math display">\[
\begin{aligned} \delta_{k}(x) &amp;=-\frac{1}{2}\left(x-\mu_{k}\right)^{T} \boldsymbol{\Sigma}_{k}^{-1}\left(x-\mu_{k}\right)-\frac{1}{2} \log \left|\boldsymbol{\Sigma}_{k}\right|+\log \pi_{k} \\ &amp;=-\frac{1}{2} x^{T} \boldsymbol{\Sigma}_{k}^{-1} x+x^{T} \boldsymbol{\Sigma}_{k}^{-1} \mu_{k}-\frac{1}{2} \mu_{k}^{T} \boldsymbol{\Sigma}_{k}^{-1} \mu_{k}-\frac{1}{2} \log \left|\boldsymbol{\Sigma}_{k}\right|+\log \pi_{k} \end{aligned}
\]</span></p>
<p>is largest. In this case, we plug in estimates for <span class="math inline">\(\Sigma_k\)</span>, <span class="math inline">\(\mu_k\)</span>, and <span class="math inline">\(\pi_k\)</span>. Notice the quantity <span class="math inline">\(x\)</span> appears as a quadratic function, hence the name.</p>
<p>So why would one prefer LDA to QDA, or vice-versa? We again approach the bias-variance trade-off. With <span class="math inline">\(p\)</span> predictors, estimating a class-independent covariance matrix requires estimating <span class="math inline">\(p(p+1)/2\)</span> parameters. For example, a covariance matrix with <code>4</code> predictors would require estimating <code>4(4+1)/2 = 10</code> parameters. To estimate a covariance matrix for each class, the number of parameters is <span class="math inline">\(Kp(p+1)/2\)</span> paramters. With <code>50</code> predictors, this becomes some multiple of <code>1,275</code>, depending on <span class="math inline">\(K\)</span>. The assumption of the common covariance matrix in LDA causes the model to become linear in <span class="math inline">\(x\)</span>, which means there are <span class="math inline">\(Kp\)</span> linear coefficients to estimate. As a result, LDA is much less flexible clasifier than QDA, and has lower variance.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> understand the parameter calculation</span></code></pre>
<p>The consequence of this is that if LDA’s assumption of a common covariance matrix is significantly off, the LDA can suffer from high bias. In general, LDA tends to be a better bet than QDA when there are relatively few training observations and so reduction of variance is crucial. In contrast, with large data sets, QDA can be recommended as the variance of the classifier is not a major concern, or the assumption of a common covariance matrix for the <span class="math inline">\(K\)</span> classes is clearly not correct.</p>
<p>Breaking the assumption of a common covariance matrix can “curve” the decision boundary, and so the use of a more flexible model (QDA) could yield better results.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> add QDA example</span></code></pre>
</div>
</div>
<div id="a-comparison-of-classification-methods" class="section level2">
<h2><span class="header-section-number">4.6</span> A Comparison of Classification Methods</h2>
<p>Let’s discuss the classification methods we have considered and the scenarios for which one might be superior.</p>
<ul>
<li>Logistic regression</li>
<li>LDA</li>
<li>QDA</li>
<li>K-nearest neighbors</li>
</ul>
<p>There is a connection between LDA and logistic regression, particularyly in the two-class setting with <span class="math inline">\(p=1\)</span> predictor. The difference being that logistic regression estimates coefficients via maximum likelihood, and LDA uses the estimated mean and variance from a normal distribution.</p>
<p>The similarity in fitting procedure means that LDA and logistic regression often give similar results. When the assumption that observations are drawn from a Gaussian distribution with a common covariance matrix in each class are in fact true, the LDA can perform better than logistic regression. If the assumptions are in fact false, logistic regression can outperform LDA.</p>
<p>KNN, on the other hand, is completely non-parametric. KNN looks at observations “closest” to <span class="math inline">\(x\)</span>, and assigns it to the class to which the plurality of these observations belong. No assumptions are made about the shape of the decision boundary. We can expect KNN to outperform both LDA and logistic regression when the decision boundary is highly non-linear. A downside of KNN, even when it does outperform, is its lack of interpretability. KNN does not tell us which predictors are important.</p>
<p>QDA serves as a compromise between the non-parametric KNN method and the linear LDA and logistic regression approaches. The assumption of quadratic decision boundary allows it to accurately model a wider range of problems. It’s reduced flexibility compared to KNN allows it to produce a lower variance with a limited number of training observations due to it making some assumptions about the form of the decision boundary.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> compare all on different dataset using tidymodels and broom</span></code></pre>
</div>
<div id="lab-logistic-regression-lda-qda-and-knn" class="section level2">
<h2><span class="header-section-number">4.7</span> Lab: Logistic Regression, LDA, QDA, and KNN</h2>
<div id="the-stock-market-data" class="section level3">
<h3><span class="header-section-number">4.7.1</span> The Stock Market Data</h3>
<p>We will look at <code>ISLR::Smarket</code> dataset, which consists of S&amp;P 500 returns from 2001 through 2005.</p>
<pre class="sourceCode r"><code class="sourceCode r">smarket &lt;-<span class="st"> </span>ISLR<span class="op">::</span>Smarket <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span>janitor<span class="op">::</span><span class="kw">clean_names</span>()
smarket</code></pre>
<pre><code>## # A tibble: 1,250 x 9
##    year   lag1   lag2   lag3   lag4   lag5 volume  today direction
##   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    
## 1  2001  0.381 -0.192 -2.62  -1.06   5.01    1.19  0.959 Up       
## 2  2001  0.959  0.381 -0.192 -2.62  -1.06    1.30  1.03  Up       
## 3  2001  1.03   0.959  0.381 -0.192 -2.62    1.41 -0.623 Down     
## 4  2001 -0.623  1.03   0.959  0.381 -0.192   1.28  0.614 Up       
## 5  2001  0.614 -0.623  1.03   0.959  0.381   1.21  0.213 Up       
## 6  2001  0.213  0.614 -0.623  1.03   0.959   1.35  1.39  Up       
## # … with 1,244 more rows</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">smarket <span class="op">%&gt;%</span><span class="st"> </span>skimr<span class="op">::</span><span class="kw">skim</span>()</code></pre>
<pre><code>## Skim summary statistics
##  n obs: 1250 
##  n variables: 9 
## 
## ── Variable type:factor ─────────────────────────────
##   variable missing complete    n n_unique               top_counts ordered
##  direction       0     1250 1250        2 Up: 648, Dow: 602, NA: 0   FALSE
## 
## ── Variable type:numeric ────────────────────────────
##  variable missing complete    n      mean   sd      p0     p25      p50
##      lag1       0     1250 1250    0.0038 1.14   -4.92   -0.64    0.039
##      lag2       0     1250 1250    0.0039 1.14   -4.92   -0.64    0.039
##      lag3       0     1250 1250    0.0017 1.14   -4.92   -0.64    0.038
##      lag4       0     1250 1250    0.0016 1.14   -4.92   -0.64    0.038
##      lag5       0     1250 1250    0.0056 1.15   -4.92   -0.64    0.038
##     today       0     1250 1250    0.0031 1.14   -4.92   -0.64    0.038
##    volume       0     1250 1250    1.48   0.36    0.36    1.26    1.42 
##      year       0     1250 1250 2003.02   1.41 2001    2002    2003    
##      p75    p100     hist
##     0.6     5.73 ▁▁▂▇▅▁▁▁
##     0.6     5.73 ▁▁▂▇▅▁▁▁
##     0.6     5.73 ▁▁▂▇▅▁▁▁
##     0.6     5.73 ▁▁▂▇▅▁▁▁
##     0.6     5.73 ▁▁▂▇▅▁▁▁
##     0.6     5.73 ▁▁▂▇▅▁▁▁
##     1.64    3.15 ▁▁▇▇▂▁▁▁
##  2004    2005    ▇▇▁▇▁▇▁▇</code></pre>
</div>
<div id="logistic-regression-1" class="section level3">
<h3><span class="header-section-number">4.7.2</span> Logistic Regression</h3>
<p>Let’s run a logistic regression to predict <code>direction</code> using <code>lag1</code> through <code>lag5</code> and <code>volume</code>.</p>
<p>Unlike ISLR, we will use the <code>parsnip::logistic_reg</code> function over <code>glm</code> due to its API design and machine learning workflow provided by its parent package, <code>tidymodels</code>. Models in the <code>{parsnip}</code> package also allow for choice of different computational engines. This reduces cognitive overhead by standardizing the high-level arguments for training a model without rembembering the specifications of different engine. In our case, we will be using the <code>glm</code> engine.</p>
<pre><code>logistic_reg() is a way to generate a specification of a model before fitting and allows the model to be created using different packages in R, Stan, keras, or via Spark.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
logi_market &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>(<span class="dt">mode =</span> <span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> smarket <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>year, <span class="op">-</span>today), direction <span class="op">~</span><span class="st"> </span>.)

logi_market <span class="op">%&gt;%</span><span class="st"> </span>broom<span class="op">::</span><span class="kw">tidy</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(p.value)</code></pre>
<pre><code>## # A tibble: 7 x 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 lag1         -0.0731    0.0502    -1.46    0.145
## 2 volume        0.135     0.158      0.855   0.392
## 3 lag2         -0.0423    0.0501    -0.845   0.398
## 4 (Intercept)  -0.126     0.241     -0.523   0.601
## 5 lag3          0.0111    0.0499     0.222   0.824
## 6 lag5          0.0103    0.0495     0.208   0.835
## # … with 1 more row</code></pre>
<p>The model shows that the smallest p-value is associated with <code>lag</code> (although the p-value is still relatively large). If we had to interpret the likely faulty model, the negative coefficient suggests that if the market was up yesterday, it’s less likely to go up today.</p>
<p>We can still use the <code>predict()</code> function with our <code>{tidymodels}</code> workflow. The <code>type</code> parameter specifies whether we want probabilities or classifications returned. The object returned is a tibble with columns of the predicted probability of the observation being in each class.</p>
<pre class="sourceCode r"><code class="sourceCode r">probs &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> logi_market, <span class="dt">new_data =</span> smarket, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)
smarket <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(probs)</code></pre>
<pre><code>## # A tibble: 1,250 x 11
##    year   lag1   lag2   lag3   lag4   lag5 volume  today direction
##   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    
## 1  2001  0.381 -0.192 -2.62  -1.06   5.01    1.19  0.959 Up       
## 2  2001  0.959  0.381 -0.192 -2.62  -1.06    1.30  1.03  Up       
## 3  2001  1.03   0.959  0.381 -0.192 -2.62    1.41 -0.623 Down     
## 4  2001 -0.623  1.03   0.959  0.381 -0.192   1.28  0.614 Up       
## 5  2001  0.614 -0.623  1.03   0.959  0.381   1.21  0.213 Up       
## 6  2001  0.213  0.614 -0.623  1.03   0.959   1.35  1.39  Up       
## # … with 1,244 more rows, and 2 more variables: .pred_Down &lt;dbl&gt;,
## #   .pred_Up &lt;dbl&gt;</code></pre>
<p>ISLR recommends creating classification labels by hand, but we can also utilize <code>predict()</code> for this.</p>
<pre class="sourceCode r"><code class="sourceCode r">preds &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> logi_market, <span class="dt">new_data =</span> smarket, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)
smarket <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(preds)</code></pre>
<pre><code>## # A tibble: 1,250 x 10
##    year   lag1   lag2   lag3   lag4   lag5 volume  today direction
##   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    
## 1  2001  0.381 -0.192 -2.62  -1.06   5.01    1.19  0.959 Up       
## 2  2001  0.959  0.381 -0.192 -2.62  -1.06    1.30  1.03  Up       
## 3  2001  1.03   0.959  0.381 -0.192 -2.62    1.41 -0.623 Down     
## 4  2001 -0.623  1.03   0.959  0.381 -0.192   1.28  0.614 Up       
## 5  2001  0.614 -0.623  1.03   0.959  0.381   1.21  0.213 Up       
## 6  2001  0.213  0.614 -0.623  1.03   0.959   1.35  1.39  Up       
## # … with 1,244 more rows, and 1 more variable: .pred_class &lt;fct&gt;</code></pre>
<p>Again, we can produce a confusion matrix using <code>conf_mat()</code> function of the <code>{yardstick}</code> package (used for measuring model performance, also part of <code>{tidymodels}</code>).</p>
<p>We tell <code>conf_mat()</code> that the <code>direction</code> column is our source of truth, and our classifications are contained in the <code>.pred_class</code> column.</p>
<pre class="sourceCode r"><code class="sourceCode r">cm_market &lt;-<span class="st"> </span>smarket <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(preds) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">conf_mat</span>(<span class="dt">truth =</span> direction, <span class="dt">estimate =</span> .pred_class)

cm_market</code></pre>
<pre><code>##           Truth
## Prediction Down  Up
##       Down  145 141
##       Up    457 507</code></pre>
<p><code>conf_mat</code> objects also have a <code>summary()</code> method that computes various classification metrics.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(cm_market) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(.metric <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;accuracy&#39;</span>, <span class="st">&#39;sens&#39;</span>, <span class="st">&#39;spec&#39;</span>))</code></pre>
<pre><code>## # A tibble: 3 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.522
## 2 sens     binary         0.241
## 3 spec     binary         0.782</code></pre>
<p>Our overall accuracy is around <code>~52%</code>. This may seem decent, but if we look at the original <code>smarket</code> data, we see that the <code>direction</code> value is evenly split (market goes up <code>51.8%</code> of the time). Still, it suggests that we are slightly better off than random guessing.</p>
<p>However, we need to consider that this accuracy is on the <em>training data</em>, which is often overly optimistic. To get a better sense of our model performance, we need to train on a subset of the data, and test on the remaining subset.</p>
<p>We will train on data from <code>2001</code> to <code>2004</code> and then test on data from <code>2005</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">40</span>)
<span class="co"># prepare training/test splits</span>
smarket_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(smarket, <span class="dt">prop =</span> <span class="dv">3</span><span class="op">/</span><span class="dv">4</span>)
train_smarket &lt;-<span class="st"> </span><span class="kw">training</span>(smarket_split)
test_smarket &lt;-<span class="st"> </span><span class="kw">testing</span>(smarket_split)

<span class="co"># fit model and grab predictions</span>
logi_market &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>(<span class="dt">mode =</span> <span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> train_smarket <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>year, <span class="op">-</span>today), direction <span class="op">~</span><span class="st"> </span>.)
preds &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> logi_market, <span class="dt">new_data =</span> test_smarket, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)

<span class="co"># build confusion matrix on test set</span>
test_cm &lt;-<span class="st"> </span>test_smarket <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">bind_cols</span>(preds) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">conf_mat</span>(<span class="dt">truth =</span> direction, <span class="dt">estimate =</span> .pred_class)

<span class="co"># summary statistics</span>
<span class="kw">summary</span>(test_cm) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(.metric <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;accuracy&#39;</span>, <span class="st">&#39;sens&#39;</span>, <span class="st">&#39;spec&#39;</span>))</code></pre>
<pre><code>## # A tibble: 3 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.478
## 2 sens     binary         0.176
## 3 spec     binary         0.791</code></pre>
<p>In this scenario, the accuracy is <code>47.8%</code>. Certain iterations of this <em>could</em> produce a lower test error due to randomness + small sample size, but each iteration tends to be lower than the previous, no-split example. Obviously, this example shows that applying classification models doesn’t always yield a robust model. Having the ability to predict the stock market with a dozen lines of code isn’t realistic.</p>
</div>
<div id="linear-discriminant-analysis-1" class="section level3">
<h3><span class="header-section-number">4.7.3</span> Linear Discriminant Analysis</h3>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regression.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["tidy_islr.pdf", "tidy_islr.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
