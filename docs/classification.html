<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Classification | A Tidy Introduction To Statistical Learning</title>
  <meta name="description" content="Chapter 4 Classification | A Tidy Introduction To Statistical Learning" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Classification | A Tidy Introduction To Statistical Learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Classification | A Tidy Introduction To Statistical Learning" />
  
  
  

<meta name="author" content="Beau Lucas" />


<meta name="date" content="2020-03-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-regression.html"/>
<link rel="next" href="resampling-methods.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Tidy Introduction To Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#an-overview-of-statistical-learning"><i class="fa fa-check"></i><b>1.1</b> An Overview of Statistical Learning</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#data-sets-used-in-labs-and-exercises"><i class="fa fa-check"></i><b>1.2</b> Data Sets Used in Labs and Exercises</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#book-resources"><i class="fa fa-check"></i><b>1.3</b> Book Resources:</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-learning.html"><a href="statistical-learning.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#packages-used-in-this-chapter"><i class="fa fa-check"></i><b>2.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#what-is-statistical-learning"><i class="fa fa-check"></i><b>2.2</b> What is Statistical Learning?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#why-estimate-f"><i class="fa fa-check"></i><b>2.2.1</b> Why Estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#how-do-we-estimate-f"><i class="fa fa-check"></i><b>2.2.2</b> How do we estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-trade-off-between-prediction-accuracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.2.3</b> The Trade-Off Between Prediction Accuracy and Model Interpretability</a></li>
<li class="chapter" data-level="2.2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#supervised-versus-unsupervised-learning"><i class="fa fa-check"></i><b>2.2.4</b> Supervised Versus Unsupervised Learning</a></li>
<li class="chapter" data-level="2.2.5" data-path="statistical-learning.html"><a href="statistical-learning.html#regression-versus-classification-problems"><i class="fa fa-check"></i><b>2.2.5</b> Regression Versus Classification Problems</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.3</b> Assessing Model Accuracy</a><ul>
<li class="chapter" data-level="2.3.1" data-path="statistical-learning.html"><a href="statistical-learning.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.3.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.3.2" data-path="statistical-learning.html"><a href="statistical-learning.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>2.3.2</b> The Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="2.3.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-classification-setting"><i class="fa fa-check"></i><b>2.3.3</b> The Classification Setting</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#lab-introduction-to-r"><i class="fa fa-check"></i><b>2.4</b> Lab: Introduction to R</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-learning.html"><a href="statistical-learning.html#exercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#packages-used-in-this-chapter-1"><i class="fa fa-check"></i><b>3.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>3.2.1</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimate"><i class="fa fa-check"></i><b>3.2.2</b> Assessing the Accuracy of the Coefficient Estimate</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-model"><i class="fa fa-check"></i><b>3.2.3</b> Assessing the Accuracy of the Model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>3.3.1</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-regression.html"><a href="linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>3.3.2</b> Some Important Questions</a></li>
<li class="chapter" data-level="3.3.3" data-path="linear-regression.html"><a href="linear-regression.html#other-considerations-in-the-regression-model"><i class="fa fa-check"></i><b>3.3.3</b> Other Considerations in the Regression Model</a></li>
<li class="chapter" data-level="3.3.4" data-path="linear-regression.html"><a href="linear-regression.html#qualitative-predictors-with-more-than-two-levels"><i class="fa fa-check"></i><b>3.3.4</b> Qualitative Predictors with More than Two Levels</a></li>
<li class="chapter" data-level="3.3.5" data-path="linear-regression.html"><a href="linear-regression.html#extensions-of-the-linear-model"><i class="fa fa-check"></i><b>3.3.5</b> Extensions of the Linear Model</a></li>
<li class="chapter" data-level="3.3.6" data-path="linear-regression.html"><a href="linear-regression.html#potential-problems"><i class="fa fa-check"></i><b>3.3.6</b> Potential Problems</a></li>
<li class="chapter" data-level="3.3.7" data-path="linear-regression.html"><a href="linear-regression.html#the-marketing-plan"><i class="fa fa-check"></i><b>3.3.7</b> The Marketing Plan</a></li>
<li class="chapter" data-level="3.3.8" data-path="linear-regression.html"><a href="linear-regression.html#comparison-of-linear-regression-with-k-nearest-neighbors"><i class="fa fa-check"></i><b>3.3.8</b> Comparison of Linear Regression with <em>K</em>-Nearest Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-regression.html"><a href="linear-regression.html#lab-linear-regression"><i class="fa fa-check"></i><b>3.4</b> Lab: Linear Regression</a><ul>
<li class="chapter" data-level="3.4.1" data-path="linear-regression.html"><a href="linear-regression.html#fitting-a-linear-regression"><i class="fa fa-check"></i><b>3.4.1</b> Fitting a linear regression</a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression-1"><i class="fa fa-check"></i><b>3.4.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="3.4.3" data-path="linear-regression.html"><a href="linear-regression.html#interaction-terms"><i class="fa fa-check"></i><b>3.4.3</b> Interaction Terms</a></li>
<li class="chapter" data-level="3.4.4" data-path="linear-regression.html"><a href="linear-regression.html#non-linear-transformations-of-the-predictors"><i class="fa fa-check"></i><b>3.4.4</b> Non-linear Transformations of the Predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="linear-regression.html"><a href="linear-regression.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>4</b> Classification</a><ul>
<li class="chapter" data-level="4.1" data-path="classification.html"><a href="classification.html#packages-used-in-this-chapter-2"><i class="fa fa-check"></i><b>4.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="4.2" data-path="classification.html"><a href="classification.html#an-overview-of-classification"><i class="fa fa-check"></i><b>4.2</b> An Overview of Classification</a></li>
<li class="chapter" data-level="4.3" data-path="classification.html"><a href="classification.html#why-not-linear-regression"><i class="fa fa-check"></i><b>4.3</b> Why Not Linear Regression?</a></li>
<li class="chapter" data-level="4.4" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>4.4</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.4.1" data-path="classification.html"><a href="classification.html#the-logistic-model"><i class="fa fa-check"></i><b>4.4.1</b> The Logistic Model</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification.html"><a href="classification.html#estimating-the-regression-coefficients-1"><i class="fa fa-check"></i><b>4.4.2</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="4.4.3" data-path="classification.html"><a href="classification.html#making-predictions"><i class="fa fa-check"></i><b>4.4.3</b> Making Predictions</a></li>
<li class="chapter" data-level="4.4.4" data-path="classification.html"><a href="classification.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>4.4.4</b> Multiple Logistic Regression</a></li>
<li class="chapter" data-level="4.4.5" data-path="classification.html"><a href="classification.html#logistic-regression-for-2-response-classes"><i class="fa fa-check"></i><b>4.4.5</b> Logistic Regression for &gt;2 Response Classes</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>4.5</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="4.5.1" data-path="classification.html"><a href="classification.html#using-bayes-theorem-for-classification"><i class="fa fa-check"></i><b>4.5.1</b> Using Bayes’ Theorem for Classification</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1"><i class="fa fa-check"></i><b>4.5.2</b> Linear Discriminant Analysis for p = 1</a></li>
<li class="chapter" data-level="4.5.3" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1-1"><i class="fa fa-check"></i><b>4.5.3</b> Linear Discriminant Analysis for p &gt; 1</a></li>
<li class="chapter" data-level="4.5.4" data-path="classification.html"><a href="classification.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>4.5.4</b> Quadratic Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="classification.html"><a href="classification.html#a-comparison-of-classification-methods"><i class="fa fa-check"></i><b>4.6</b> A Comparison of Classification Methods</a></li>
<li class="chapter" data-level="4.7" data-path="classification.html"><a href="classification.html#lab-logistic-regression-lda-qda-and-knn"><i class="fa fa-check"></i><b>4.7</b> Lab: Logistic Regression, LDA, QDA, and KNN</a><ul>
<li class="chapter" data-level="4.7.1" data-path="classification.html"><a href="classification.html#churn-dataset"><i class="fa fa-check"></i><b>4.7.1</b> Churn Dataset</a></li>
<li class="chapter" data-level="4.7.2" data-path="classification.html"><a href="classification.html#logistic-regression-1"><i class="fa fa-check"></i><b>4.7.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.7.3" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-1"><i class="fa fa-check"></i><b>4.7.3</b> Linear discriminant analysis</a></li>
<li class="chapter" data-level="4.7.4" data-path="classification.html"><a href="classification.html#k-nearest-neighbors-1"><i class="fa fa-check"></i><b>4.7.4</b> K-Nearest Neighbors</a></li>
<li class="chapter" data-level="4.7.5" data-path="classification.html"><a href="classification.html#choosing-the-model"><i class="fa fa-check"></i><b>4.7.5</b> Choosing the model</a></li>
<li class="chapter" data-level="4.7.6" data-path="classification.html"><a href="classification.html#evaluating-the-threshold"><i class="fa fa-check"></i><b>4.7.6</b> Evaluating the threshold</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="classification.html"><a href="classification.html#conclusion"><i class="fa fa-check"></i><b>4.8</b> Conclusion</a></li>
<li class="chapter" data-level="4.9" data-path="classification.html"><a href="classification.html#exercises-2"><i class="fa fa-check"></i><b>4.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="resampling-methods.html"><a href="resampling-methods.html"><i class="fa fa-check"></i><b>5</b> Resampling Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation"><i class="fa fa-check"></i><b>5.1</b> Cross-Validation</a><ul>
<li class="chapter" data-level="5.1.1" data-path="resampling-methods.html"><a href="resampling-methods.html#the-validation-set-approach"><i class="fa fa-check"></i><b>5.1.1</b> The Validation Set Approach</a></li>
<li class="chapter" data-level="5.1.2" data-path="resampling-methods.html"><a href="resampling-methods.html#leave-one-out-cross-validation."><i class="fa fa-check"></i><b>5.1.2</b> Leave-One-Out Cross-Validation.</a></li>
<li class="chapter" data-level="5.1.3" data-path="resampling-methods.html"><a href="resampling-methods.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.1.3</b> k-Fold Cross-Validation</a></li>
<li class="chapter" data-level="5.1.4" data-path="resampling-methods.html"><a href="resampling-methods.html#bias-variance-trade-off-for-k-fold-cross-validation"><i class="fa fa-check"></i><b>5.1.4</b> Bias-Variance Trade-Off for <em>k</em>-fold Cross-Validation</a></li>
<li class="chapter" data-level="5.1.5" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation-on-classification-problems"><i class="fa fa-check"></i><b>5.1.5</b> Cross-Validation on Classification Problems</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="resampling-methods.html"><a href="resampling-methods.html#the-bootstrap"><i class="fa fa-check"></i><b>5.2</b> The Bootstrap</a></li>
<li class="chapter" data-level="5.3" data-path="resampling-methods.html"><a href="resampling-methods.html#lab"><i class="fa fa-check"></i><b>5.3</b> Lab</a><ul>
<li class="chapter" data-level="5.3.1" data-path="resampling-methods.html"><a href="resampling-methods.html#the-validation-set-approach-1"><i class="fa fa-check"></i><b>5.3.1</b> The Validation Set Approach</a></li>
<li class="chapter" data-level="5.3.2" data-path="resampling-methods.html"><a href="resampling-methods.html#loocv"><i class="fa fa-check"></i><b>5.3.2</b> LOOCV</a></li>
<li class="chapter" data-level="5.3.3" data-path="resampling-methods.html"><a href="resampling-methods.html#k-fold-cross-validation-1"><i class="fa fa-check"></i><b>5.3.3</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation</a></li>
<li class="chapter" data-level="5.3.4" data-path="resampling-methods.html"><a href="resampling-methods.html#the-bootstrap-1"><i class="fa fa-check"></i><b>5.3.4</b> The Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="resampling-methods.html"><a href="resampling-methods.html#exercises-3"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html"><i class="fa fa-check"></i><b>6</b> Linear Model Selection And Regularization</a><ul>
<li class="chapter" data-level="6.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#subset-selection"><i class="fa fa-check"></i><b>6.1</b> Subset Selection</a><ul>
<li class="chapter" data-level="6.1.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#best-subset-selection"><i class="fa fa-check"></i><b>6.1.1</b> Best Subset Selection</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#stepwise-selection"><i class="fa fa-check"></i><b>6.2</b> Stepwise Selection</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/beaulucas/tidy_islr" target="blank">GitHub Repository</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tidy Introduction To Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Classification</h1>
<hr />
<div id="packages-used-in-this-chapter-2" class="section level2">
<h2><span class="header-section-number">4.1</span> Packages used in this chapter</h2>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb108-2" data-line-number="2"><span class="kw">library</span>(tidymodels)</a>
<a class="sourceLine" id="cb108-3" data-line-number="3"><span class="kw">library</span>(discrim)</a>
<a class="sourceLine" id="cb108-4" data-line-number="4"><span class="kw">library</span>(kknn)</a>
<a class="sourceLine" id="cb108-5" data-line-number="5"><span class="kw">library</span>(knitr)</a>
<a class="sourceLine" id="cb108-6" data-line-number="6"><span class="kw">library</span>(kableExtra)</a>
<a class="sourceLine" id="cb108-7" data-line-number="7"><span class="kw">library</span>(skimr)</a></code></pre></div>
<p>Linear regression in chapter 3 was concerned with predicting a quantitative response variable. What if the response variable is <em>qualitative</em>? Eye color is an example of a qualitative variable, which takes discrete value such as <code>blue</code>, <code>brown</code>, <code>green</code>. These are also referred to as <em>categorical</em>.</p>
<p>The approach of predicting qualitative responses is known as <em>classification</em>. Often, we predict the probability of the occurences of each category of a qualitative variable, and then make a decision based off of that.</p>
<p>In this chapter we discuss three of the most widely-used classifiers:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis">linear discriminant analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"><em>k</em>-nearest neighbors</a></li>
</ul>
<p>We discuss more computer-intensive methods in later chapters.</p>
</div>
<div id="an-overview-of-classification" class="section level2">
<h2><span class="header-section-number">4.2</span> An Overview of Classification</h2>
<p>Classification is a common scenario.</p>
<ol style="list-style-type: decimal">
<li>Person arrives at ER exhibiting particular symptoms. What illness does he have?</li>
<li>Money is wired to an external account at a bank. Is this fraud?</li>
<li>Email is sent to your account. Is it legit, or spam?</li>
</ol>
<p>Similar to regression, we have a set of training observations that use to build a classifier. We also want the classifier to perform well on both training and test observations.</p>
<p>We will use the dataset <code>ISLR::Default</code>. First, let’s convert it to tidy format.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb109-1" data-line-number="1">default &lt;-<span class="st"> </span>ISLR<span class="op">::</span>Default <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>()</a></code></pre></div>
<p>We are interested in the ability to predict whether an individual will default on their credit card payment, based on their credit card <code>balance</code> and annual income.</p>
<p>If we look at the summary statistics, we see the data is clean, and that very few people default on their balances.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb110-1" data-line-number="1">default <span class="op">%&gt;%</span><span class="st"> </span>skimr<span class="op">::</span><span class="kw">skim</span>()</a></code></pre></div>
<table style='width: auto;'
        class='table table-condensed'>
<caption>
<span id="tab:unnamed-chunk-72">Table 4.1: </span>Data summary
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Name
</td>
<td style="text-align:left;">
Piped data
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of rows
</td>
<td style="text-align:left;">
10000
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of columns
</td>
<td style="text-align:left;">
4
</td>
</tr>
<tr>
<td style="text-align:left;">
_______________________
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Column type frequency:
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
factor
</td>
<td style="text-align:left;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:left;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
________________________
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Group variables
</td>
<td style="text-align:left;">
None
</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left;">
skim_variable
</th>
<th style="text-align:right;">
n_missing
</th>
<th style="text-align:right;">
complete_rate
</th>
<th style="text-align:left;">
ordered
</th>
<th style="text-align:right;">
n_unique
</th>
<th style="text-align:left;">
top_counts
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
default
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
No: 9667, Yes: 333
</td>
</tr>
<tr>
<td style="text-align:left;">
student
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
No: 7056, Yes: 2944
</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left;">
skim_variable
</th>
<th style="text-align:right;">
n_missing
</th>
<th style="text-align:right;">
complete_rate
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
<th style="text-align:right;">
p0
</th>
<th style="text-align:right;">
p25
</th>
<th style="text-align:right;">
p50
</th>
<th style="text-align:right;">
p75
</th>
<th style="text-align:right;">
p100
</th>
<th style="text-align:left;">
hist
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
balance
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
835.37
</td>
<td style="text-align:right;">
483.71
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
481.73
</td>
<td style="text-align:right;">
823.64
</td>
<td style="text-align:right;">
1166.31
</td>
<td style="text-align:right;">
2654.32
</td>
<td style="text-align:left;">
▆▇▅▁▁
</td>
</tr>
<tr>
<td style="text-align:left;">
income
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
33516.98
</td>
<td style="text-align:right;">
13336.64
</td>
<td style="text-align:right;">
771.97
</td>
<td style="text-align:right;">
21340.46
</td>
<td style="text-align:right;">
34552.64
</td>
<td style="text-align:right;">
43807.73
</td>
<td style="text-align:right;">
73554.23
</td>
<td style="text-align:left;">
▂▇▇▅▁
</td>
</tr>
</tbody>
</table>
<p>The scatterplot signals a strong relationship between <code>balance</code> and <code>default</code>.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" data-line-number="1">default <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb111-2" data-line-number="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> balance, <span class="dt">y =</span> income, <span class="dt">fill =</span> default)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb111-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_hex</span>(<span class="dt">alpha =</span> <span class="dv">2</span><span class="op">/</span><span class="dv">3</span>)</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-73-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The boxplot captures the stark difference in <code>balance</code> between those who default and do not.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb112-1" data-line-number="1">default <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb112-2" data-line-number="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> balance, <span class="dt">fill =</span> default)) <span class="op">+</span></a>
<a class="sourceLine" id="cb112-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_boxplot</span>()</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-74-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<div id="why-not-linear-regression" class="section level2">
<h2><span class="header-section-number">4.3</span> Why Not Linear Regression?</h2>
<p>Imagine we were trying to predict the medical outcome of a patient on the basis of their symptoms. Let’s say there are three possible diagnoses: <code>stroke</code>, <code>overdose</code>, and <code>seizure</code>. We could encode these into a quantitative variable <span class="math inline">\(Y\)</span>. that takes values from 1 to 3. Using least squares, we could then fit a regression model to predict <span class="math inline">\(Y\)</span>.</p>
<p>Unfortunately, this coding implies an ordering of the outcomes. It also insists that the difference between levels is quantitative, and equivalent across all sequences of levels.</p>
<p>Thus, changing the order of encodings would change relationship among the conditions, producing fundamentally different linear models.</p>
<p>There could be a case where a response variables took on a natural ordering, such as <code>mild</code>, <code>moderate</code>, <code>severe</code>. We would also need to believe that the gap between each level is equivalent. Unfortunately, there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is appropriate for linear regression.</p>
<p>For cases of <em>binary</em> qualitative response, we can utilize the dummy variable solution seen in Chapter 3. In this case, the order of the encodings is arbitrary.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" data-line-number="1"><span class="co"># </span><span class="al">TODO</span><span class="co"> add latex for encoding</span></a></code></pre></div>
<p>Linear regression does work for this binary response scenario. However, it is possible for linear regression to produce estimates outside of the <code>[0, 1]</code> interval, which affects their interpretability as probabilities.</p>
<p>When the qualitative response has more than two levels, we need to use classification methods that are appropriate.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">4.4</span> Logistic Regression</h2>
<p>Let’s consider the <code>default</code> dataset. Rather than modeling this response <span class="math inline">\(Y\)</span> directly, logistic regression models the <em>probability</em> that <span class="math inline">\(Y\)</span> belongs to a particular category.</p>
<p>If we estimate using linear regression, we see that some estimated probabilities are negative. We are using the <code>tidymodels</code> package.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb114-1" data-line-number="1">default &lt;-<span class="st"> </span>default <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb114-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">default_bool =</span> <span class="kw">if_else</span>(default <span class="op">==</span><span class="st"> &quot;Yes&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>))</a>
<a class="sourceLine" id="cb114-3" data-line-number="3">lm_default &lt;-<span class="st"> </span><span class="kw">linear_reg</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb114-4" data-line-number="4"><span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> default, default_bool <span class="op">~</span><span class="st"> </span>balance)</a>
<a class="sourceLine" id="cb114-5" data-line-number="5"></a>
<a class="sourceLine" id="cb114-6" data-line-number="6">default <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb114-7" data-line-number="7"><span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">predict</span>(lm_default, default)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb114-8" data-line-number="8"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> balance)) <span class="op">+</span></a>
<a class="sourceLine" id="cb114-9" data-line-number="9"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span>  .pred)) <span class="op">+</span></a>
<a class="sourceLine" id="cb114-10" data-line-number="10"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> default_bool, <span class="dt">colour =</span> default_bool)) <span class="op">+</span></a>
<a class="sourceLine" id="cb114-11" data-line-number="11"><span class="st">  </span><span class="kw">guides</span>(<span class="dt">colour=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-76-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Below is the classification using logistic regression, where are probabilities fall between <code>0</code> and <code>1</code>.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" data-line-number="1">logi_default &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>(<span class="dt">mode =</span> <span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb115-2" data-line-number="2"><span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> default, <span class="kw">as.factor</span>(default_bool) <span class="op">~</span><span class="st"> </span>balance)</a>
<a class="sourceLine" id="cb115-3" data-line-number="3"></a>
<a class="sourceLine" id="cb115-4" data-line-number="4">default <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb115-5" data-line-number="5"><span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">predict</span>(logi_default, default, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb115-6" data-line-number="6"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> balance)) <span class="op">+</span></a>
<a class="sourceLine" id="cb115-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span>  .pred_<span class="dv">1</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb115-8" data-line-number="8"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> default_bool, <span class="dt">colour =</span> default_bool)) <span class="op">+</span></a>
<a class="sourceLine" id="cb115-9" data-line-number="9"><span class="st">  </span><span class="kw">guides</span>(<span class="dt">colour=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-77-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Logistic regression in this example is modelling the probability of default, given the value of <code>balance</code>.</p>
<div>
<p style="text-align:center">
Pr(<code>default</code> = <code>Yes</code>|<code>balance</code>)
</p>
</div>
<p>These values, which we abbreviate as <em>p</em>(<code>balance</code>), range between <code>0</code> and <code>1</code>. Logistic regression will always produce an <em>S-shaped</em> curve. Regardless of the value of <span class="math inline">\(X\)</span>, we will receive a sensible prediction.</p>
<p>From this, we can make a classification prediction for <code>default</code>. Depending how conservative we are, the threshold for this could vary. Depending on the domain and context of the classification, a decision boundary around <code>0.5</code> or <code>0.1</code> might be appropriate.</p>
<div id="the-logistic-model" class="section level3">
<h3><span class="header-section-number">4.4.1</span> The Logistic Model</h3>
<p>The problem of using a linear regression model is evident in the chart above, where probabilities can fall below <code>0</code> or greater than <code>1</code>.</p>
<p>To avoid this, we must model <span class="math inline">\(p(X)\)</span> using a function that gives outputs between <code>0</code> and <code>1</code> for all values of <span class="math inline">\(X\)</span>. In logistic regression, we use the <em>logistic function</em>,</p>
<div>
<p style="text-align:center">
<span class="math inline">\(p(X) = \frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>logistic function</em>
</p>
</div>
<p>To fit the model, we use a method called <em>maximum likelihood</em>.</p>
<p>If we manipulate the logistic function, we find that</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\frac{p(X)}{1-p(X)} = e^{\beta_0+\beta_1X}\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>odds</em>
</p>
</div>
<p>This is called the <em>odds</em>, and takes any value from <span class="math inline">\(0\)</span> to <span class="math inline">\(\infty\)</span>. This is the same type of odds used in sporting events (“9:1 odds to win this match”, etc). If <span class="math inline">\(p(X) = 0.9\)</span>, then odds are <span class="math inline">\(\frac{0.9}{1-0.9} = 9\)</span>.</p>
<p>If we take the logarithm of the odds, we arrive at</p>
<div>
<p style="text-align:center">
<span class="math inline">\(log(\frac{p(X)}{1-p(X)}) = \beta_0+\beta_1X\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>log-odds  logit</em>
</p>
</div>
<p>The left-hande side is called the <em>log-odds</em> or <em>logit</em>. The logistic regression model has a logit that is linear in <span class="math inline">\(X\)</span>.</p>
<p>The contrast to linear regression is that increasing <span class="math inline">\(X\)</span> by one-unit changes the log odds by <span class="math inline">\(\beta_1\)</span> (or the odds by <span class="math inline">\(e^{\beta_1}\)</span>. However, since <span class="math inline">\(p(X)\)</span> and <span class="math inline">\(X\)</span> relationship is not a straight line (see plot above), <span class="math inline">\(\beta_1\)</span> does not correspond to the the change in <span class="math inline">\(p(X)\)</span> associated with a one-unit increase in <span class="math inline">\(X\)</span>. The amount that <span class="math inline">\(p(X)\)</span> changes depends on the current value of <span class="math inline">\(X\)</span>. See how the slope approaches <code>0</code> more and more slowly as <code>balance</code> increases.</p>
<p>Regardless of how much <span class="math inline">\(p(X)\)</span> moves, if <span class="math inline">\(\beta_1\)</span> is positive then increasing <span class="math inline">\(X\)</span> will be associated with increasing <span class="math inline">\(p(X)\)</span>. The opposite is also true.</p>
</div>
<div id="estimating-the-regression-coefficients-1" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Estimating the Regression Coefficients</h3>
<p>The coefficients in the logistic regression equation must be estimated used training data. Linear regression used the least squares approach to estimate the coefficients. It is possible to use non-linear least squares to fit the model, but <em>maximum likelihood</em> is preferred.</p>
<p>Maximum likelihood seeks to to find estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that the predicted probability <span class="math inline">\(\hat{p}(x_i)\)</span> of default for each individual corresponds as closely as possible to to the individual’s observed default status. We want estimates that produce low probabilities for individuals who did not default, and high probabilities for those who did.</p>
<p>We can formalize this with a <em>likelihood function</em>:</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" data-line-number="1"><span class="co">#TODO add likelihood function, links, etc</span></a></code></pre></div>
<p>We can examine the coefficients and other information from our logistic regression model.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" data-line-number="1">logi_default <span class="op">%&gt;%</span><span class="st"> </span>broom<span class="op">::</span><span class="kw">tidy</span>()</a></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   term         estimate std.error statistic   p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept) -10.7      0.361        -29.5 3.62e-191
## 2 balance       0.00550  0.000220      25.0 1.98e-137</code></pre>
<p>If we look at the terms of our logistic regression, we see that the coefficient for <code>balance</code> is positive. This means that higher <code>balance</code> increases <span class="math inline">\(p(Default)\)</span>. A one-unit increase in <code>balance</code> will increase the log odds of defaulting by ~0.0055.</p>
<p>The test-statistic also behaves similarly. Coefficients with large statistics indicate evidence against the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>. For logistic regression, the null hypothesis implies that <span class="math inline">\(p(X) = \frac{e^{\beta_0}}{1+e^{\beta_0}}\)</span>, which means that the probability of defaulting does not depend on <code>balance.</code></p>
<p>Given the miniscule p-value associated with our <code>balance</code> coefficient, we can confidently reject <span class="math inline">\(H_0\)</span>. The intercept (<span class="math inline">\(\beta_0\)</span>) is typically not of interest; it’s main purpose is to adjust the average fitted probabilities to the proportion of ones in the data.</p>
</div>
<div id="making-predictions" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Making Predictions</h3>
<p>Once we have the coefficients, we simply compute the probability of <code>default</code> for any given observation.</p>
<p>Let’s take an individual with a <code>balance</code> of <code>$1000</code>. Using our model terms, we can compute the probability. Let’s extract the terms from the model and plug in a <code>balance</code> of <code>$1000</code>.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb119-1" data-line-number="1">logi_coef &lt;-<span class="st"> </span>logi_default <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb119-2" data-line-number="2"><span class="st">  </span>broom<span class="op">::</span><span class="kw">tidy</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb119-3" data-line-number="3"><span class="st">  </span><span class="co"># widen it and clean up names</span></a>
<a class="sourceLine" id="cb119-4" data-line-number="4"><span class="st">  </span><span class="kw">select</span>(term, estimate) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb119-5" data-line-number="5"><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from =</span> term, <span class="dt">values_from =</span> estimate) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb119-6" data-line-number="6"><span class="st">  </span>janitor<span class="op">::</span><span class="kw">clean_names</span>()</a>
<a class="sourceLine" id="cb119-7" data-line-number="7"></a>
<a class="sourceLine" id="cb119-8" data-line-number="8">logi_coef <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb119-9" data-line-number="9"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prob_1000 =</span> <span class="kw">exp</span>(intercept <span class="op">+</span><span class="st"> </span>balance <span class="op">*</span><span class="st"> </span><span class="dv">1000</span>) <span class="op">/</span></a>
<a class="sourceLine" id="cb119-10" data-line-number="10"><span class="st">           </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(intercept <span class="op">+</span><span class="st"> </span>balance <span class="op">*</span><span class="st"> </span><span class="dv">1000</span>)))</a></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   intercept balance prob_1000
##       &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;
## 1     -10.7 0.00550   0.00575</code></pre>
<p>We find the probability to be less than <code>1%</code>.</p>
<p>We can also incorporate qualitative predictors with the logistic regression model. Here we encode <code>student</code> in to the model.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb121-1" data-line-number="1">logi_default_student &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>(<span class="dt">mode =</span> <span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb121-2" data-line-number="2"><span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> default, <span class="kw">as.factor</span>(default_bool) <span class="op">~</span><span class="st"> </span>student)</a>
<a class="sourceLine" id="cb121-3" data-line-number="3"></a>
<a class="sourceLine" id="cb121-4" data-line-number="4">logi_default_student <span class="op">%&gt;%</span><span class="st"> </span>broom<span class="op">::</span><span class="kw">tidy</span>()</a></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   -3.50     0.0707    -49.6  0       
## 2 studentYes     0.405    0.115       3.52 0.000431</code></pre>
<p>This model indicates that students have a higher rate of defaulting compared to non-students.</p>
</div>
<div id="multiple-logistic-regression" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Multiple Logistic Regression</h3>
<p>We now consider the scenario of multiple predictors.</p>
<p>We can rewrite <span class="math inline">\(p(X)\)</span> as</p>
<div>
<p style="text-align:center">
<span class="math inline">\(p(X) = \frac{e^{\beta_0+\beta_1X_1+...+\beta_pX_p}}{1+e^{\beta_0+\beta_1X_1+...+\beta_pX_p}}\)</span>
</p>
<p class="vocab" style="text-align:right">
/p&gt;
</div>
<p>And again use the maximum likelihood method to estimate the coefficients.</p>
<p>Let’s estimate <code>balance</code> using <code>balance</code>, <code>income</code> and <code>student</code>.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb123-1" data-line-number="1">multiple_logi_default&lt;-<span class="st"> </span><span class="kw">logistic_reg</span>(<span class="dt">mode =</span> <span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb123-2" data-line-number="2"><span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> default, <span class="kw">as.factor</span>(default_bool) <span class="op">~</span><span class="st"> </span>balance <span class="op">+</span><span class="st"> </span>student <span class="op">+</span><span class="st"> </span>income)</a>
<a class="sourceLine" id="cb123-3" data-line-number="3"></a>
<a class="sourceLine" id="cb123-4" data-line-number="4">multiple_logi_default <span class="op">%&gt;%</span><span class="st"> </span>broom<span class="op">::</span><span class="kw">tidy</span>()</a></code></pre></div>
<pre><code>## # A tibble: 4 x 5
##   term            estimate  std.error statistic   p.value
##   &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept) -10.9        0.492        -22.1   4.91e-108
## 2 balance       0.00574    0.000232      24.7   4.22e-135
## 3 studentYes   -0.647      0.236         -2.74  6.19e-  3
## 4 income        0.00000303 0.00000820     0.370 7.12e-  1</code></pre>
<p>Notice that being a student now <em>decreases</em> the chances of default, whereas in our previous model (which only contained <code>student</code> as a predictor), it increased the chances.</p>
<p>Why is that? This model is showing that, for a fixed value of <code>income</code> and <code>balance</code>, students actually default less. This is because <code>student</code> and <code>balance</code> are correlated.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb125-1" data-line-number="1">default <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb125-2" data-line-number="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> balance, <span class="dt">fill =</span> student)) <span class="op">+</span></a>
<a class="sourceLine" id="cb125-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_boxplot</span>()</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-83-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>If we plot the distribution of <code>balance</code> across <code>student</code>, we see that students tend to carry larger credit card balances.</p>
<p>This example illustrates the dangers of drawing insights from single predictor regressions when other predictors may be relevant. The results from using one predictor can be substantially different compared to using multiple predictors. This phenomenon is known as <em>confounding</em>.</p>
</div>
<div id="logistic-regression-for-2-response-classes" class="section level3">
<h3><span class="header-section-number">4.4.5</span> Logistic Regression for &gt;2 Response Classes</h3>
<p>Sometimes we wish to classify a response variable that has more than two classes. This could be the medical example where a patient outcomes falls into <code>stroke</code>, <code>overdose</code>, and <code>seizure</code>. It is possible to extend the two-class logistic regression model into multiple-class, but this is not used often in practice.</p>
<p>A method that is popular for multi-class classification is <em>discriminant analysis</em>.</p>
</div>
</div>
<div id="linear-discriminant-analysis" class="section level2">
<h2><span class="header-section-number">4.5</span> Linear Discriminant Analysis</h2>
<p>Logistic regression models the distribution of response <span class="math inline">\(Y\)</span> given the predictor(s) <span class="math inline">\(X\)</span>. In discriminant analysis, we model the distribution of the predictors <span class="math inline">\(X\)</span> in each of the response classes, and then use Bayes’ theorem to flip these around into estimates for <span class="math inline">\(Pr(Y = k|X = x)\)</span>.</p>
<p>Why do we need this method?</p>
<ul>
<li><p>Well-separated classes produce unstable parameter estimates for logistic regression models</p></li>
<li><p>If <span class="math inline">\(n\)</span> is small and distribution of predictors <span class="math inline">\(X\)</span> is normall across the classes, the linear discriminant model is more stable than logistic regression</p></li>
</ul>
<div id="using-bayes-theorem-for-classification" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Using Bayes’ Theorem for Classification</h3>
<p>Consider the scenario where we want to classify an observation into one of <span class="math inline">\(K\)</span> classes, where <span class="math inline">\(K &gt;= 2\)</span>.</p>
<ul>
<li>Let <span class="math inline">\(\pi_k\)</span> represent the overall or <em>prior</em> probability that a randomly chosen observation comes from the <span class="math inline">\(k\)</span>th class</li>
<li>Let <span class="math inline">\(f_k(x) = Pr(X = x|Y = k)\)</span> denote the <em>density function</em> of <span class="math inline">\(X\)</span> for an observation that comes from the <span class="math inline">\(k\)</span>th class.</li>
</ul>
<p>In other words, <span class="math inline">\(f_k(x)\)</span> being large means that there is a high probability that an observation in the <span class="math inline">\(k\)</span>th class has <span class="math inline">\(X \approx x\)</span>.</p>
<p>We can use Bayes’ theorem</p>
<p><span class="math display">\[
\operatorname{Pr}(Y=k | X=x)=\frac{\pi_{k} f_{k}(x)}{\sum_{l=1}^{K} \pi_{l} f_{l}(x)}
\]</span></p>
<p>And call the left-hand side <span class="math inline">\(p_k(X)\)</span>. We can plug in estimates of <span class="math inline">\(\pi_k\)</span> and <span class="math inline">\(f_k(X)\)</span> into Bayes’ theorem above to get the probability of a certain class, given an observation.</p>
<ul>
<li>Solving for <span class="math inline">\(\pi_k\)</span> is easy if we have a random sample of <span class="math inline">\(Y\)</span>s from the population. We simply calculate the fraction of observations that fall into a <span class="math inline">\(k\)</span> class.</li>
<li>Estimating <span class="math inline">\(f_k(X)\)</span> is more challenging unless we assume simple forms for these densities</li>
</ul>
<p>We refer to <span class="math inline">\(p_k(x)\)</span> as the posterior probability that an observation <span class="math inline">\(X = x\)</span> belongs to the <span class="math inline">\(k\)</span>th class. This is the probability that the observation belongs to the <span class="math inline">\(k\)</span>th class, <em>given</em> the predictor value for that observation.</p>
<p>The Bayes’ classifier classifies an observation to the class for which <span class="math inline">\(p_k(X)\)</span> is largest. If we can find a way to estimate <span class="math inline">\(f_k(X)\)</span>, we can develop a classifier that approximates the Bayes classifier.</p>
</div>
<div id="linear-discriminant-analysis-for-p-1" class="section level3">
<h3><span class="header-section-number">4.5.2</span> Linear Discriminant Analysis for p = 1</h3>
<p>Let’s assume we have one predictor. We need to obtain an estimate for <span class="math inline">\(f_k(x)\)</span> (the density function for <span class="math inline">\(X\)</span> given a class <span class="math inline">\(k\)</span>). This will obtain a value for <span class="math inline">\(p_k(x)\)</span>. We will then classify this observation for which <span class="math inline">\(p_k(x)\)</span> is greatest.</p>
<p>To estimate <span class="math inline">\(f_k(x)\)</span>, we need to make some assumptions about its form.</p>
<p>Let’s assume <span class="math inline">\(f_k(x)\)</span> is <em>normal</em> or <em>Gaussian</em>. The normal density takes the form</p>
<p><span class="math display">\[
f_{k}(x)=\frac{1}{\sqrt{2 \pi} \sigma_{k}} \exp \left(-\frac{1}{2 \sigma_{k}^{2}}\left(x-\mu_{k}\right)^{2}\right)
\]</span></p>
<p>Plugging this back in to <span class="math inline">\(p_k(x)\)</span>, we obtain</p>
<p><span class="math display">\[
p_{k}(x)=\frac{\pi_{k} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2 \sigma^{2}}\left(x-\mu_{k}\right)^{2}\right)}{\sum_{l=1}^{K} \pi_{l} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2 \sigma^{2}}\left(x-\mu_{l}\right)^{2}\right)}
\]</span></p>
<p>Taking the log and rearranging results in</p>
<p><span class="math display">\[
\delta_{k}(x)=x \cdot \frac{\mu_{k}}{\sigma^{2}}-\frac{\mu_{k}^{2}}{2 \sigma^{2}}+\log \left(\pi_{k}\right)
\]</span></p>
<p>In this case, the Bayes decision boundary corresponds to</p>
<p><span class="math display">\[
x=\frac{\mu_{1}^{2}-\mu_{2}^{2}}{2\left(\mu_{1}-\mu_{2}\right)}=\frac{\mu_{1}+\mu_{2}}{2}
\]</span></p>
<p>We can simulate some data to show a simple example.</p>
<p>In this data we have two classes:</p>
<ul>
<li><span class="math inline">\(\mu_1 = -1.25, \mu_2 = 1.25, \sigma_1^2 = \sigma_2^2 = 1\)</span></li>
</ul>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb126-1" data-line-number="1">var_<span class="dv">1</span> =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb126-2" data-line-number="2">var_<span class="dv">2</span> =<span class="st"> </span>var_<span class="dv">1</span></a>
<a class="sourceLine" id="cb126-3" data-line-number="3">f_<span class="dv">1</span> =<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">fun =</span> <span class="st">&quot;f_1&quot;</span>, <span class="dt">x =</span> <span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">10000</span>, <span class="dt">mean =</span> <span class="fl">-1.25</span>, <span class="dt">sd =</span> var_<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb126-4" data-line-number="4">f_<span class="dv">2</span> =<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">fun =</span> <span class="st">&quot;f_2&quot;</span>, <span class="dt">x =</span> <span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">10000</span>, <span class="dt">mean =</span> <span class="fl">1.25</span>, <span class="dt">sd =</span> var_<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb126-5" data-line-number="5">f_x =<span class="st"> </span><span class="kw">bind_rows</span>(f_<span class="dv">1</span>, f_<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb126-6" data-line-number="6"></a>
<a class="sourceLine" id="cb126-7" data-line-number="7"><span class="co"># add summary statistics</span></a>
<a class="sourceLine" id="cb126-8" data-line-number="8"></a>
<a class="sourceLine" id="cb126-9" data-line-number="9">f_x &lt;-<span class="st"> </span>f_x <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb126-10" data-line-number="10"><span class="st">  </span><span class="kw">group_by</span>(fun) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb126-11" data-line-number="11"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pi =</span> <span class="kw">n</span>(),</a>
<a class="sourceLine" id="cb126-12" data-line-number="12">         <span class="dt">var =</span> <span class="kw">var</span>(x),</a>
<a class="sourceLine" id="cb126-13" data-line-number="13">         <span class="dt">mu =</span> <span class="kw">mean</span>(x)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb126-14" data-line-number="14"><span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb126-15" data-line-number="15"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pi =</span> pi <span class="op">/</span><span class="st"> </span><span class="kw">n</span>())</a>
<a class="sourceLine" id="cb126-16" data-line-number="16"></a>
<a class="sourceLine" id="cb126-17" data-line-number="17">decision_boundary &lt;-<span class="st"> </span>f_x <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb126-18" data-line-number="18"><span class="st">  </span><span class="kw">group_by</span>(fun) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb126-19" data-line-number="19"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">mu =</span> <span class="kw">mean</span>(x)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb126-20" data-line-number="20"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">decision_boundary =</span> <span class="kw">sum</span>(mu) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb126-21" data-line-number="21"><span class="st">  </span><span class="kw">pull</span>()</a>
<a class="sourceLine" id="cb126-22" data-line-number="22">f_x <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb126-23" data-line-number="23"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">colour =</span> fun)) <span class="op">+</span></a>
<a class="sourceLine" id="cb126-24" data-line-number="24"><span class="st">  </span><span class="kw">geom_density</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb126-25" data-line-number="25"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> decision_boundary, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>)</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-84-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>These two densities overlap, and so given <span class="math inline">\(X = x\)</span>, we still have uncertaintly about which class the observation belongs to. If both classes are equally likely for a random observation <span class="math inline">\(\pi_1 = \pi_2\)</span>, then we see the Bayes classifier assigns the observation to class 1 if <span class="math inline">\(x &lt; 0\)</span> and class 2 otherwise.</p>
<p>Even if we are sure that <span class="math inline">\(X\)</span> is drawn from a Gaussian distribution within each class, we still need to estimate <span class="math inline">\(\mu_1,...,\mu_k\)</span>, <span class="math inline">\(\pi_1,...,\pi_k\)</span>, and <span class="math inline">\(\sigma^2\)</span>. The <em>linear discriminant analysis</em> method approximates these by plugging in estimates as follows</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat{\mu}_k = \frac{1}{n_k}\sum_{i:y_i=k}{x_i}\)</span>
</p>
<p class="vocab" style="text-align:right">
*
</p>
</div>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat{\sigma}^2 = \frac{1}{n-K}\sum_{k=1}^{K}\sum_{i:y_i=k}{(x_i-\hat{\mu}_k)^2}\)</span>
</p>
<p class="vocab" style="text-align:right">
*
</p>
</div>
<p>The estimate for <span class="math inline">\(\hat{\mu}_k\)</span> is the average of all training observations from the <span class="math inline">\(k\)</span>th class. The estimate for <span class="math inline">\(\hat{\sigma}^2\)</span> is the weighted average of the sample variances for each of the K classes.</p>
<p>To estimate <span class="math inline">\(\hat{\pi}_k\)</span>, we simply take the proportion of training observations that belong to the <span class="math inline">\(k\)</span>th class</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat{\pi}_k = n_k/n\)</span>
</p>
<p class="vocab" style="text-align:right">
*
</p>
</div>
<p>From these estimates, we can achieve a decision boundary</p>
<p><span class="math display">\[
\hat{\delta}_{k}(x)=x \cdot \frac{\hat{\mu}_{k}}{\hat{\sigma}^{2}}-\frac{\hat{\mu}_{k}^{2}}{2 \hat{\sigma}^{2}}+\log \left(\hat{\pi}_{k}\right)
\]</span></p>
<p>This classifier has <em>linear</em> in the name due to the fact that the <em>discriminant function</em> above are linear functions of <span class="math inline">\(x\)</span>.</p>
<p>Let’s take a sample from our earlier distribution and see how it performs.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb127-1" data-line-number="1"><span class="kw">library</span>(discrim)</a>
<a class="sourceLine" id="cb127-2" data-line-number="2">f_sample =<span class="st"> </span>f_x <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_frac</span>(<span class="dt">size =</span> <span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb127-3" data-line-number="3"></a>
<a class="sourceLine" id="cb127-4" data-line-number="4">lda_f &lt;-<span class="st"> </span>discrim<span class="op">::</span><span class="kw">discrim_linear</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb127-5" data-line-number="5"><span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> f_sample, <span class="kw">as.factor</span>(fun) <span class="op">~</span><span class="st"> </span>x)</a>
<a class="sourceLine" id="cb127-6" data-line-number="6"></a>
<a class="sourceLine" id="cb127-7" data-line-number="7">preds &lt;-<span class="st"> </span><span class="kw">predict</span>(lda_f, f_sample, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb127-8" data-line-number="8"></a>
<a class="sourceLine" id="cb127-9" data-line-number="9">f_sample &lt;-<span class="st"> </span>f_sample <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_cols</span>(preds)</a>
<a class="sourceLine" id="cb127-10" data-line-number="10"><span class="co"># </span><span class="al">TODO</span><span class="co"> figure out how to truly extract decision boundary from MASS::lda</span></a>
<a class="sourceLine" id="cb127-11" data-line-number="11">est_decision &lt;-<span class="st"> </span>f_sample <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(x) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(.pred_class <span class="op">==</span><span class="st"> &#39;f_2&#39;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb127-12" data-line-number="12"><span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(x)</a>
<a class="sourceLine" id="cb127-13" data-line-number="13"></a>
<a class="sourceLine" id="cb127-14" data-line-number="14"><span class="kw">ggplot</span>(f_sample, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">fill =</span> fun)) <span class="op">+</span></a>
<a class="sourceLine" id="cb127-15" data-line-number="15"><span class="st">  </span><span class="kw">geom_histogram</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb127-16" data-line-number="16"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> est_decision, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb127-17" data-line-number="17"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">0</span>)</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-85-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Notice the estimated decision boundary (dashed line) being very close to the Bayes decision boundary.</p>
<div id="measuring-performance" class="section level4">
<h4><span class="header-section-number">4.5.2.1</span> Measuring Performance</h4>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" data-line-number="1"><span class="co"># </span><span class="al">TODO</span><span class="co"> show lda performance compared to true value</span></a></code></pre></div>
</div>
</div>
<div id="linear-discriminant-analysis-for-p-1-1" class="section level3">
<h3><span class="header-section-number">4.5.3</span> Linear Discriminant Analysis for p &gt; 1</h3>
<p>We can extend LDA classifier to multiple predictors.</p>
<p>The multivariate Gaussian distribution assumes that each predictor follows a one-dimensional normal distribution, with some correlation between each pair of predictors.</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=JjB58InuTqM">Andrew Ng on Multivariate Gaussian Distribution</a></li>
</ul>
<p>To indicate that a <span class="math inline">\(p\)</span>-dimensional random variable <span class="math inline">\(X\)</span> has a multi-variate Gaussian distribution, we write $ X N(, )$</p>
<ul>
<li><span class="math inline">\(E(X) = \mu\)</span> is the mean of <span class="math inline">\(X\)</span> (a vector with <span class="math inline">\(p\)</span> components)</li>
<li><span class="math inline">\(Cov(X) = \Sigma\)</span> is the <span class="math inline">\(p*p\)</span> covariance matrix of <span class="math inline">\(X\)</span>.</li>
</ul>
<p>The multivariate Gaussian density is defined as</p>
<p><span class="math display">\[
f(x)=\frac{1}{(2 \pi)^{p / 2}|\mathbf{\Sigma}|^{1 / 2}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \mathbf{\Sigma}^{-1}(x-\mu)\right)
\]</span></p>
<p>In the case of <span class="math inline">\(p&gt;1\)</span> predictors, the LDA classifier assumes that the observations in the <span class="math inline">\(k\)</span>th class are drawn from a multivariate Gaussian distribution <span class="math inline">\(N(\mu_k, \Sigma)\)</span>, where <span class="math inline">\(\mu_k\)</span> is a class-specific mean vector, and <span class="math inline">\(\Sigma\)</span> is the covariance matrix that is common to all <span class="math inline">\(K\)</span> classes.</p>
<p>Plugging the density function for the <span class="math inline">\(k\)</span>th class, <span class="math inline">\(f_k(X = x)\)</span>, into</p>
<p><span class="math display">\[
\operatorname{Pr}(Y=k | X=x)=\frac{\pi_{k} f_{k}(x)}{\sum_{l=1}^{K} \pi_{l} f_{l}(x)}
\]</span></p>
<p>and performing some algebra reveals that the Bayes classifier will assign observation <span class="math inline">\(X = x\)</span> by identifying the class for which</p>
<p><span class="math display">\[
\delta_{k}(x)=x^{T} \boldsymbol{\Sigma}^{-1} \mu_{k}-\frac{1}{2} \mu_{k}^{T} \boldsymbol{\Sigma}^{-1} \mu_{k}+\log \pi_{k}
\]</span></p>
<p>is largest.</p>
<div id="performing-lda-on-default-data" class="section level4">
<h4><span class="header-section-number">4.5.3.1</span> Performing LDA on Default data</h4>
<p>If we run an LDA model on our <code>default</code> dataset, predicting the probability of <code>default</code> based off of <code>student</code> and <code>balance</code>, we achieve a respectable <code>3.0%</code> error rate.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb129-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb129-2" data-line-number="2">default_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(default, <span class="dt">prop =</span> <span class="dv">3</span><span class="op">/</span><span class="dv">4</span>)</a>
<a class="sourceLine" id="cb129-3" data-line-number="3">train_default &lt;-<span class="st"> </span><span class="kw">training</span>(default_split)</a>
<a class="sourceLine" id="cb129-4" data-line-number="4">test_default &lt;-<span class="st"> </span><span class="kw">testing</span>(default_split)</a>
<a class="sourceLine" id="cb129-5" data-line-number="5"></a>
<a class="sourceLine" id="cb129-6" data-line-number="6">lda_default &lt;-<span class="st"> </span>discrim<span class="op">::</span><span class="kw">discrim_linear</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb129-7" data-line-number="7"><span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> train_default, default <span class="op">~</span><span class="st"> </span>student <span class="op">+</span><span class="st"> </span>balance)</a>
<a class="sourceLine" id="cb129-8" data-line-number="8"></a>
<a class="sourceLine" id="cb129-9" data-line-number="9">preds &lt;-<span class="st"> </span><span class="kw">predict</span>(lda_default, test_default, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb129-10" data-line-number="10"></a>
<a class="sourceLine" id="cb129-11" data-line-number="11"><span class="co"># error rate</span></a>
<a class="sourceLine" id="cb129-12" data-line-number="12">test_default <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb129-13" data-line-number="13"><span class="st">  </span><span class="kw">bind_cols</span>(preds) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb129-14" data-line-number="14"><span class="st">  </span><span class="kw">metrics</span>(<span class="dt">truth =</span> default, <span class="dt">estimate =</span> .pred_class)</a></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.97 
## 2 kap      binary         0.369</code></pre>
<p>While this may seem impressive, let’s remember that only <code>3.6%</code> of observations in the dataset end up in default. This means that if we assigned a <em>null</em> classifier, which simply predicted every observation to not end in default, our error rate would be <code>3.6%</code>. This is worse, but not by much, compared to our LDA error rate.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb131-1" data-line-number="1"><span class="co"># null error rate</span></a>
<a class="sourceLine" id="cb131-2" data-line-number="2">test_default <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb131-3" data-line-number="3"><span class="st">  </span><span class="kw">group_by</span>(default) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb131-4" data-line-number="4"><span class="st">  </span><span class="kw">count</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb131-5" data-line-number="5"><span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb131-6" data-line-number="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prop =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n))</a></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   default     n  prop
##   &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt;
## 1 No       2410 0.964
## 2 Yes        90 0.036</code></pre>
<p>Binary decision makers can make to types of errors:</p>
<ul>
<li>Incorrectly assigning an individual who defaults to the “no default” category</li>
<li>Incorrectly assigning an individual who doesn’t default to the “default” category.</li>
</ul>
<p>We can identify the breakdown by using a <em>confusion matrix</em></p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb133-1" data-line-number="1">cm &lt;-<span class="st"> </span>test_default <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb133-2" data-line-number="2"><span class="st">  </span><span class="kw">bind_cols</span>(preds) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb133-3" data-line-number="3"><span class="st">  </span><span class="kw">conf_mat</span>(<span class="dt">truth =</span> default, <span class="dt">estimate =</span> .pred_class)</a>
<a class="sourceLine" id="cb133-4" data-line-number="4"></a>
<a class="sourceLine" id="cb133-5" data-line-number="5">cm</a></code></pre></div>
<pre><code>##           Truth
## Prediction   No  Yes
##        No  2402   67
##        Yes    8   23</code></pre>
<p>We see that our LDA only predicted <code>31</code> people to default. Of these, <code>23</code> actually defaulted. So, only <code>8</code> of out of the <code>~7500</code> people who did not default were incorrectly labeled.</p>
<p>However, of the <code>90</code> people in our test set who defaulted, we only predicted this correctly for <code>23</code> of them. That means <code>~75%</code> of individuals who default were incorrectly classified. Having an error rate this high for the problematic class (those who default) is unacceptable.</p>
<p>Class-specific performance is an important concept. <em>Sensitivity</em> and <em>specificity</em> characterize the performance of a classifier or screening test. In this case, the sensitivity is the percentage of true defaults who are identified (a low <code>~25%</code>). The specificity is the percentage of non-defaulters who are correctly identified (<code>7492/7500 ~ 99.9%</code>).</p>
<p>Remember that LDA is trying to approximate the Bayes classifier, which has the lowest <em>total</em> error rate out of all classifiers (assuming Gaussian assumption is correct). The classifier will yield the smallest total number of misclassifications, regardless of which class the errors came from. In this credit card scenario, the credit card company might wish to avoid incorrectly misclassifying a user who defaults. In this case, they value sensitivity. For them, the cost of misclassifying a defaulter is higher than the cost of misclassifying a non-defaulter (which they still desire to avoid).</p>
<p>It’s possible to modify LDA for such circumstances. Given the Bayes classifier works by assigning an observation to a class in which the posterior probability <span class="math inline">\(p_k(X)\)</span> is greatest (in the two-class scenario, this decision boundary is at <code>0.5</code>), we can modify the probability threshold to suit our needs. If we wish to increase our sensitivity, we can lower this threshold.</p>
<p>Imagine we lowered the threshold to <code>0.2</code>. Sure, we would classify more people as defaulters than before (decreasing our specificity) but we would also catch more defaulters we previously missed (increasing our sensitivity).</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb135-1" data-line-number="1">preds &lt;-<span class="st"> </span><span class="kw">predict</span>(lda_default, test_default, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)</a>
<a class="sourceLine" id="cb135-2" data-line-number="2"></a>
<a class="sourceLine" id="cb135-3" data-line-number="3"><span class="co"># error rate</span></a>
<a class="sourceLine" id="cb135-4" data-line-number="4">test_default <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb135-5" data-line-number="5"><span class="st">  </span><span class="kw">bind_cols</span>(preds) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb135-6" data-line-number="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.pred_class =</span> <span class="kw">as.factor</span>(<span class="kw">if_else</span>(.pred_Yes <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.2</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>))) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb135-7" data-line-number="7"><span class="st">  </span><span class="kw">conf_mat</span>(<span class="dt">truth =</span> default, <span class="dt">estimate =</span> .pred_class)</a></code></pre></div>
<pre><code>##           Truth
## Prediction   No  Yes
##        No  2357   37
##        Yes   53   53</code></pre>
<p>Now our sensitivy has increased. Of the <code>90</code> people who defaulted, we correctly identified <code>53, or ~58.8%</code> of them (up from <code>~25%</code> previously).</p>
<p>This came at a cost, as our specificity decreased. This time, we predicted <code>106</code> people to default. Of those, <code>53</code> actually defaulted. This means that <code>53</code> of the <code>7500</code> people who didn’t default were incorrectly labelled. This gives us a specificity of (<code>7447/7500 ~ 99.2%</code>)</p>
<p>Despite the overall increase in error rate, the lower threshold may be chosen, depending on the context of the problem. To make a decision, an extensive amount of <em>domain knowledge</em> is required.</p>
<p>The <em>ROC curve</em> is a popular graphic for displaying the two types of errors for all possible thresholds. “ROC” stands for <em>receiver operating characteristics</em>.</p>
<p>The overall performance of a classifier, summarized over all possible thresholds, is given by the <em>area under the (ROC) curve</em> (AUC). An ideal ROC curve will hug the top left corner. Think of it this way: ideal ROC curves are able to increase sensitivity at a much higher rate than reduction in specificity.</p>
<p>We can use <code>yardstick::</code> (part of <code>tidymodels::</code>) to plot an ROC curve.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb137-1" data-line-number="1">test_default <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb137-2" data-line-number="2"><span class="st">  </span><span class="kw">bind_cols</span>(preds) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb137-3" data-line-number="3"><span class="st">  </span><span class="kw">roc_curve</span>(default, .pred_Yes) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb137-4" data-line-number="4"><span class="st">  </span><span class="kw">autoplot</span>()</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-91-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>We can think of the <em>sensitivity</em> as the true positive, and <em>1 - specificity</em> as the false positive.</p>
</div>
</div>
<div id="quadratic-discriminant-analysis" class="section level3">
<h3><span class="header-section-number">4.5.4</span> Quadratic Discriminant Analysis</h3>
<p>LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution, with a class-specific mean vector and a covariance matrix that is common to all <span class="math inline">\(K\)</span> classes. <em>Quadratic discriminant analysis</em> (QDA) assumes that class has its own covariance matrix.</p>
<p>It assumes that each observation from the <span class="math inline">\(k\)</span>th class has the form <span class="math inline">\(X \sim N(\mu_k, \Sigma_k)\)</span>, where <span class="math inline">\(\Sigma_k\)</span> is a covariance matrix for the <span class="math inline">\(k\)</span>th class. Under this assumption, the Bayes classifier assigns an observation <span class="math inline">\(X=x\)</span> to the class for which</p>
<p><span class="math display">\[
\begin{aligned} \delta_{k}(x) &amp;=-\frac{1}{2}\left(x-\mu_{k}\right)^{T} \boldsymbol{\Sigma}_{k}^{-1}\left(x-\mu_{k}\right)-\frac{1}{2} \log \left|\boldsymbol{\Sigma}_{k}\right|+\log \pi_{k} \\ &amp;=-\frac{1}{2} x^{T} \boldsymbol{\Sigma}_{k}^{-1} x+x^{T} \boldsymbol{\Sigma}_{k}^{-1} \mu_{k}-\frac{1}{2} \mu_{k}^{T} \boldsymbol{\Sigma}_{k}^{-1} \mu_{k}-\frac{1}{2} \log \left|\boldsymbol{\Sigma}_{k}\right|+\log \pi_{k} \end{aligned}
\]</span></p>
<p>is largest. In this case, we plug in estimates for <span class="math inline">\(\Sigma_k\)</span>, <span class="math inline">\(\mu_k\)</span>, and <span class="math inline">\(\pi_k\)</span>. Notice the quantity <span class="math inline">\(x\)</span> appears as a quadratic function, hence the name.</p>
<p>So why would one prefer LDA to QDA, or vice-versa? We again approach the bias-variance trade-off. With <span class="math inline">\(p\)</span> predictors, estimating a class-independent covariance matrix requires estimating <span class="math inline">\(p(p+1)/2\)</span> parameters. For example, a covariance matrix with <code>4</code> predictors would require estimating <code>4(4+1)/2 = 10</code> parameters. To estimate a covariance matrix for each class, the number of parameters is <span class="math inline">\(Kp(p+1)/2\)</span> paramters. With <code>50</code> predictors, this becomes some multiple of <code>1,275</code>, depending on <span class="math inline">\(K\)</span>. The assumption of the common covariance matrix in LDA causes the model to become linear in <span class="math inline">\(x\)</span>, which means there are <span class="math inline">\(Kp\)</span> linear coefficients to estimate. As a result, LDA is much less flexible clasifier than QDA, and has lower variance.</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb138-1" data-line-number="1"><span class="co"># </span><span class="al">TODO</span><span class="co"> understand the parameter calculation</span></a></code></pre></div>
<p>The consequence of this is that if LDA’s assumption of a common covariance matrix is significantly off, the LDA can suffer from high bias. In general, LDA tends to be a better bet than QDA when there are relatively few training observations and so reduction of variance is crucial. In contrast, with large data sets, QDA can be recommended as the variance of the classifier is not a major concern, or the assumption of a common covariance matrix for the <span class="math inline">\(K\)</span> classes is clearly not correct.</p>
<p>Breaking the assumption of a common covariance matrix can “curve” the decision boundary, and so the use of a more flexible model (QDA) could yield better results.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb139-1" data-line-number="1"><span class="co"># </span><span class="al">TODO</span><span class="co"> add QDA example</span></a></code></pre></div>
</div>
</div>
<div id="a-comparison-of-classification-methods" class="section level2">
<h2><span class="header-section-number">4.6</span> A Comparison of Classification Methods</h2>
<p>Let’s discuss the classification methods we have considered and the scenarios for which one might be superior.</p>
<ul>
<li>Logistic regression</li>
<li>LDA</li>
<li>QDA</li>
<li>K-nearest neighbors</li>
</ul>
<p>There is a connection between LDA and logistic regression, particularyly in the two-class setting with <span class="math inline">\(p=1\)</span> predictor. The difference being that logistic regression estimates coefficients via maximum likelihood, and LDA uses the estimated mean and variance from a normal distribution.</p>
<p>The similarity in fitting procedure means that LDA and logistic regression often give similar results. When the assumption that observations are drawn from a Gaussian distribution with a common covariance matrix in each class are in fact true, the LDA can perform better than logistic regression. If the assumptions are in fact false, logistic regression can outperform LDA.</p>
<p>KNN, on the other hand, is completely non-parametric. KNN looks at observations “closest” to <span class="math inline">\(x\)</span>, and assigns it to the class to which the plurality of these observations belong. No assumptions are made about the shape of the decision boundary. We can expect KNN to outperform both LDA and logistic regression when the decision boundary is highly non-linear. A downside of KNN, even when it does outperform, is its lack of interpretability. KNN does not tell us which predictors are important.</p>
<p>QDA serves as a compromise between the non-parametric KNN method and the linear LDA and logistic regression approaches. The assumption of quadratic decision boundary allows it to accurately model a wider range of problems. It’s reduced flexibility compared to KNN allows it to produce a lower variance with a limited number of training observations due to it making some assumptions about the form of the decision boundary.</p>
</div>
<div id="lab-logistic-regression-lda-qda-and-knn" class="section level2">
<h2><span class="header-section-number">4.7</span> Lab: Logistic Regression, LDA, QDA, and KNN</h2>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb140-1" data-line-number="1"><span class="co"># </span><span class="al">TODO</span><span class="co"> add QDA example</span></a></code></pre></div>
<div id="churn-dataset" class="section level3">
<h3><span class="header-section-number">4.7.1</span> Churn Dataset</h3>
<p>We will be using the <code>modeldata::wa_churn</code> dataset to test our classification techniques.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb141-1" data-line-number="1"><span class="kw">library</span>(modeldata)</a>
<a class="sourceLine" id="cb141-2" data-line-number="2"><span class="kw">data</span>(wa_churn)</a></code></pre></div>
<pre><code>These data were downloaded from the IBM Watson site (see below) in September 2018. The data contain a factor for whether a customer churned or not. Alternatively, the tenure column presumably contains information on how long the customer has had an account. A survival analysis can be done on this column using the churn outcome as the censoring information. A data dictionary can be found on the source website.</code></pre>
<p>Our interest for this dataset is predicting whether a customer will churn or not. If a customer is likely to churn, we can offer them an incentive to stay. These incentives cost us money, so we want to minimize the incentives given out to customers that won’t churn. We will identify a balance between <em>sensitivity</em> and <em>specificity</em> that maximizes the ROI of our incentive.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb143-1" data-line-number="1">wa_churn <span class="op">%&gt;%</span><span class="st"> </span>skimr<span class="op">::</span><span class="kw">skim</span>()</a></code></pre></div>
<table style='width: auto;'
        class='table table-condensed'>
<caption>
<span id="tab:unnamed-chunk-96">Table 4.2: </span>Data summary
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Name
</td>
<td style="text-align:left;">
Piped data
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of rows
</td>
<td style="text-align:left;">
7043
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of columns
</td>
<td style="text-align:left;">
20
</td>
</tr>
<tr>
<td style="text-align:left;">
_______________________
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Column type frequency:
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
factor
</td>
<td style="text-align:left;">
11
</td>
</tr>
<tr>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:left;">
9
</td>
</tr>
<tr>
<td style="text-align:left;">
________________________
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Group variables
</td>
<td style="text-align:left;">
None
</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left;">
skim_variable
</th>
<th style="text-align:right;">
n_missing
</th>
<th style="text-align:right;">
complete_rate
</th>
<th style="text-align:left;">
ordered
</th>
<th style="text-align:right;">
n_unique
</th>
<th style="text-align:left;">
top_counts
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
churn
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
No: 5174, Yes: 1869
</td>
</tr>
<tr>
<td style="text-align:left;">
multiple_lines
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
No: 3390, Yes: 2971, No : 682
</td>
</tr>
<tr>
<td style="text-align:left;">
internet_service
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
Fib: 3096, DSL: 2421, No: 1526
</td>
</tr>
<tr>
<td style="text-align:left;">
online_security
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
No: 3498, Yes: 2019, No : 1526
</td>
</tr>
<tr>
<td style="text-align:left;">
online_backup
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
No: 3088, Yes: 2429, No : 1526
</td>
</tr>
<tr>
<td style="text-align:left;">
device_protection
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
No: 3095, Yes: 2422, No : 1526
</td>
</tr>
<tr>
<td style="text-align:left;">
tech_support
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
No: 3473, Yes: 2044, No : 1526
</td>
</tr>
<tr>
<td style="text-align:left;">
streaming_tv
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
No: 2810, Yes: 2707, No : 1526
</td>
</tr>
<tr>
<td style="text-align:left;">
streaming_movies
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
No: 2785, Yes: 2732, No : 1526
</td>
</tr>
<tr>
<td style="text-align:left;">
contract
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
Mon: 3875, Two: 1695, One: 1473
</td>
</tr>
<tr>
<td style="text-align:left;">
payment_method
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
Ele: 2365, Mai: 1612, Ban: 1544, Cre: 1522
</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left;">
skim_variable
</th>
<th style="text-align:right;">
n_missing
</th>
<th style="text-align:right;">
complete_rate
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
<th style="text-align:right;">
p0
</th>
<th style="text-align:right;">
p25
</th>
<th style="text-align:right;">
p50
</th>
<th style="text-align:right;">
p75
</th>
<th style="text-align:right;">
p100
</th>
<th style="text-align:left;">
hist
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
female
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:left;">
▇▁▁▁▇
</td>
</tr>
<tr>
<td style="text-align:left;">
senior_citizen
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.16
</td>
<td style="text-align:right;">
0.37
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:left;">
▇▁▁▁▂
</td>
</tr>
<tr>
<td style="text-align:left;">
partner
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.48
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:left;">
▇▁▁▁▇
</td>
</tr>
<tr>
<td style="text-align:left;">
dependents
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.30
</td>
<td style="text-align:right;">
0.46
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:left;">
▇▁▁▁▃
</td>
</tr>
<tr>
<td style="text-align:left;">
tenure
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
32.37
</td>
<td style="text-align:right;">
24.56
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
9.00
</td>
<td style="text-align:right;">
29.00
</td>
<td style="text-align:right;">
55.00
</td>
<td style="text-align:right;">
72.00
</td>
<td style="text-align:left;">
▇▃▃▃▆
</td>
</tr>
<tr>
<td style="text-align:left;">
phone_service
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.90
</td>
<td style="text-align:right;">
0.30
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:left;">
▁▁▁▁▇
</td>
</tr>
<tr>
<td style="text-align:left;">
paperless_billing
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.59
</td>
<td style="text-align:right;">
0.49
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:left;">
▆▁▁▁▇
</td>
</tr>
<tr>
<td style="text-align:left;">
monthly_charges
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
64.76
</td>
<td style="text-align:right;">
30.09
</td>
<td style="text-align:right;">
18.25
</td>
<td style="text-align:right;">
35.50
</td>
<td style="text-align:right;">
70.35
</td>
<td style="text-align:right;">
89.85
</td>
<td style="text-align:right;">
118.75
</td>
<td style="text-align:left;">
▇▅▆▇▅
</td>
</tr>
<tr>
<td style="text-align:left;">
total_charges
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2283.30
</td>
<td style="text-align:right;">
2266.77
</td>
<td style="text-align:right;">
18.80
</td>
<td style="text-align:right;">
401.45
</td>
<td style="text-align:right;">
1397.47
</td>
<td style="text-align:right;">
3794.74
</td>
<td style="text-align:right;">
8684.80
</td>
<td style="text-align:left;">
▇▂▂▂▁
</td>
</tr>
</tbody>
</table>
<p>The dataset contains <code>7043</code> observations and <code>20</code> variables. The dataset consists mostly of factor variables, all with a small amount of unique levels. We are going to try out logistic regression, LDA, and K-nearest neighbors on this dataset. For now, we will only remove the <code>11</code> observations which have missing values for the <code>total_charges</code> column. We will also reorder the <code>churn</code> column levels so that a “success” corresponds with a customer churning.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb144-1" data-line-number="1">wa_churn &lt;-<span class="st"> </span>wa_churn <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(<span class="op">!</span><span class="kw">is.na</span>(total_charges)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb144-2" data-line-number="2"><span class="kw">mutate</span>(<span class="dt">churn =</span> <span class="kw">fct_relevel</span>(churn, <span class="st">&quot;No&quot;</span>, <span class="st">&quot;Yes&quot;</span>))</a></code></pre></div>
<p>Now let’s take a look at the response variable.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb145-1" data-line-number="1">wa_churn <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(churn) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tally</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">prop =</span> n <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(n))</a></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   churn     n  prop
##   &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;
## 1 No     5163 0.734
## 2 Yes    1869 0.266</code></pre>
<p>In this dataset, <code>73.5%</code> of the observations do not churn. There is some skew, but nothing drastic. We will now prepare the data and apply binary classification techniques to see which model performs best.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb147-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">40</span>)</a>
<a class="sourceLine" id="cb147-2" data-line-number="2">split_churn &lt;-<span class="st"> </span><span class="kw">initial_split</span>(wa_churn, <span class="dt">prop =</span> <span class="dv">3</span><span class="op">/</span><span class="dv">4</span>)</a>
<a class="sourceLine" id="cb147-3" data-line-number="3">train_churn &lt;-<span class="st"> </span><span class="kw">training</span>(split_churn)</a>
<a class="sourceLine" id="cb147-4" data-line-number="4">test_churn &lt;-<span class="st"> </span><span class="kw">testing</span>(split_churn)</a></code></pre></div>
</div>
<div id="logistic-regression-1" class="section level3">
<h3><span class="header-section-number">4.7.2</span> Logistic Regression</h3>
<p>Let’s run a logistic regression to predict <code>churn</code> using the available variables.</p>
<p>Unlike ISLR, we will use the <code>parsnip::logistic_reg</code> function over <code>glm</code> due to its API design and machine learning workflow provided by its parent package, <code>tidymodels</code>. Models in the <code>{parsnip}</code> package also allow for choice of different computational engines. This reduces cognitive overhead by standardizing the high-level arguments for training a model without rembembering the specifications of different engine. In our case, we will be using the <code>glm</code> engine.</p>
<pre><code>logistic_reg() is a way to generate a specification of a model before fitting and allows the model to be created using different packages in R, Stan, keras, or via Spark.</code></pre>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb149-1" data-line-number="1">logi_churn &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>(<span class="dt">mode =</span> <span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb149-2" data-line-number="2"><span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> train_churn, churn <span class="op">~</span><span class="st"> </span>.)</a>
<a class="sourceLine" id="cb149-3" data-line-number="3">broom<span class="op">::</span><span class="kw">tidy</span>(logi_churn) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(p.value)</a></code></pre></div>
<pre><code>## # A tibble: 24 x 5
##   term                estimate std.error statistic  p.value
##   &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 tenure             -0.0558   0.00702       -7.95 1.81e-15
## 2 contractTwo year   -1.42     0.206         -6.87 6.48e-12
## 3 contractOne year   -0.655    0.126         -5.21 1.88e- 7
## 4 paperless_billing   0.397    0.0859         4.62 3.87e- 6
## 5 total_charges       0.000288 0.0000800      3.59 3.26e- 4
## 6 online_securityYes -0.533    0.208         -2.56 1.05e- 2
## # … with 18 more rows</code></pre>
<p>The model shows that the smallest p-value is associated with <code>tenure</code> and <code>contract</code>. This makes sense, as users locked into contracts will have a tougher time cancelling their service. We can interpret the negative coefficients as reducing the chance a customer churns. Notice how customers that utilize <code>paperless_billing</code> and electronic <code>payment_method</code> exhibit higher likelihood to churn. They are probably more tech-savvy, younger, and more likely to research other options.</p>
<div id="measuring-model-performance" class="section level4">
<h4><span class="header-section-number">4.7.2.1</span> Measuring model performance</h4>
<p>We can use the <code>predict()</code> function with our <code>{tidymodels}</code> workflow. The <code>type</code> parameter specifies whether we want probabilities or classifications returned. The object returned is a tibble with columns of the predicted probability of the observation being in each class.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb151-1" data-line-number="1">logi_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> logi_churn, <span class="dt">new_data =</span> test_churn,</a>
<a class="sourceLine" id="cb151-2" data-line-number="2">                      <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</a></code></pre></div>
<p>Specifying <code>type = &quot;class&quot;</code> will generate classification predictions based off of a <code>0.5</code> threshold (this might not be optimal for the problem).</p>
<p>Let’s add them to our test set.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb152-1" data-line-number="1">test_metrics &lt;-<span class="st"> </span>test_churn <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb152-2" data-line-number="2"><span class="st">  </span><span class="kw">bind_cols</span>(logi_preds)</a></code></pre></div>
<p>This adds a column, <code>.pred_class</code> at each observation.</p>
<p>We can produce a confusion matrix using <code>conf_mat()</code> function of the <code>{yardstick}</code> package (used for measuring model performance, also part of <code>{tidymodels}</code>).</p>
<p>We tell <code>conf_mat()</code> that the <code>direction</code> column is our source of truth, and our classifications are contained in the <code>.pred_class</code> column.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb153-1" data-line-number="1">cm_churn &lt;-<span class="st"> </span>test_metrics <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb153-2" data-line-number="2"><span class="st">  </span><span class="kw">conf_mat</span>(<span class="dt">truth =</span> churn, <span class="dt">estimate =</span> .pred_class)</a>
<a class="sourceLine" id="cb153-3" data-line-number="3"></a>
<a class="sourceLine" id="cb153-4" data-line-number="4">cm_churn</a></code></pre></div>
<pre><code>##           Truth
## Prediction   No  Yes
##        No  1156  208
##        Yes  142  252</code></pre>
<p><code>conf_mat</code> objects also have a <code>summary()</code> method that computes various classification metrics.</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb155-1" data-line-number="1"><span class="kw">summary</span>(cm_churn) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb155-2" data-line-number="2"><span class="st">  </span><span class="kw">filter</span>(.metric <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;accuracy&#39;</span>, <span class="st">&#39;sens&#39;</span>, <span class="st">&#39;spec&#39;</span>))</a></code></pre></div>
<pre><code>## # A tibble: 3 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.801
## 2 sens     binary         0.891
## 3 spec     binary         0.548</code></pre>
<p>Our overall accuracy is around <code>~80.1%</code>. This may seem high, but if we look at the original dataset, the data is skewed. A naive classifier would produce a <code>73.5%</code> accuracy (the proportion of yes/no churn values).Still, it suggests that we are better off than randomly guessing.</p>
<p>Can we account for the skew of data when assessing our model? If we utilize the <code>yardstick::metrics()</code> function on our test data fitted with class predictions, we can get a dataframe containing overall model performance.</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb157-1" data-line-number="1">test_metrics <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb157-2" data-line-number="2"><span class="st">  </span><span class="kw">metrics</span>(<span class="dt">truth =</span> churn, <span class="dt">estimate =</span> .pred_class)</a></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.801
## 2 kap      binary         0.460</code></pre>
<p>Notice the metric of <code>kap</code>. The <a href="https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english">Kappa statistic</a> compares an observed accuracy with expected accuracy (random chance). This controls for the skewness of our model by comparing our model’s accuracy with the naive classifier. In this case, our <code>kap</code> of <code>0.460</code> indicates that our model is <code>46%</code> better than the naive classifier.</p>
<p>We will be using <code>kap</code> to evaluate how our logistic regression fits to other techniques.</p>
</div>
</div>
<div id="linear-discriminant-analysis-1" class="section level3">
<h3><span class="header-section-number">4.7.3</span> Linear discriminant analysis</h3>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb159-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">40</span>)</a>
<a class="sourceLine" id="cb159-2" data-line-number="2">lda_churn &lt;-<span class="st"> </span><span class="kw">discrim_linear</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb159-3" data-line-number="3"><span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> train_churn, churn <span class="op">~</span><span class="st"> </span>.)</a></code></pre></div>
<p>Calling the <code>parsnip::</code> model object gives us information about the model fit. We can see the prior probabilities as well as the <em>coefficients of linear discriminants</em>. These provide the linear combination of the variables used to create the decision rule. If the combination is large, the model will predict an increase.</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb160-1" data-line-number="1">lda_churn<span class="op">$</span>fit<span class="op">$</span>scaling <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb160-2" data-line-number="2"><span class="st">  </span><span class="kw">as_tibble</span>(<span class="dt">rownames =</span> <span class="st">&quot;term&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb160-3" data-line-number="3"><span class="st">  </span><span class="kw">arrange</span>(LD1)</a></code></pre></div>
<pre><code>## # A tibble: 30 x 2
##   term                  LD1
##   &lt;chr&gt;               &lt;dbl&gt;
## 1 contractOne year   -0.519
## 2 online_securityYes -0.422
## 3 contractTwo year   -0.362
## 4 tech_supportYes    -0.285
## 5 phone_service      -0.220
## 6 online_backupYes   -0.209
## # … with 24 more rows</code></pre>
<p>We see similar important terms as the logistic regression model. Customers on long contracts are less likely to churn.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb162-1" data-line-number="1">lda_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(lda_churn, test_churn, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>) </a>
<a class="sourceLine" id="cb162-2" data-line-number="2"></a>
<a class="sourceLine" id="cb162-3" data-line-number="3">test_churn <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb162-4" data-line-number="4"><span class="st">  </span><span class="kw">bind_cols</span>(lda_preds) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb162-5" data-line-number="5"><span class="st">  </span><span class="kw">metrics</span>(<span class="dt">truth =</span> churn, <span class="dt">estimate =</span> .pred_class)</a></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.787
## 2 kap      binary         0.429</code></pre>
<p>Here we achieve a <code>kap</code> of <code>42.9%</code>.</p>
</div>
<div id="k-nearest-neighbors-1" class="section level3">
<h3><span class="header-section-number">4.7.4</span> K-Nearest Neighbors</h3>
<p>Next, we will utilize the <code>parsnip::nearest_neighbor()</code> function.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb164-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">40</span>)</a>
<a class="sourceLine" id="cb164-2" data-line-number="2">knn_churn &lt;-<span class="st"> </span><span class="kw">nearest_neighbor</span>(<span class="dt">mode =</span> <span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb164-3" data-line-number="3"><span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> train_churn, churn <span class="op">~</span><span class="st"> </span>.)</a>
<a class="sourceLine" id="cb164-4" data-line-number="4">knn_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> knn_churn, <span class="dt">new_data =</span> test_churn)</a>
<a class="sourceLine" id="cb164-5" data-line-number="5"></a>
<a class="sourceLine" id="cb164-6" data-line-number="6">test_churn <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb164-7" data-line-number="7"><span class="st">  </span><span class="kw">bind_cols</span>(knn_preds) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb164-8" data-line-number="8"><span class="st">  </span><span class="kw">metrics</span>(<span class="dt">truth =</span> churn, <span class="dt">estimate =</span> .pred_class)</a></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.745
## 2 kap      binary         0.341</code></pre>
<p>K-nearest neighbors performs much worse, with a <code>kap</code> of <code>34.1%</code>.</p>
<p>KNN’s poor performance is most likely due to the predictor variables and true decision boundary not being highly non-linear. In this case, KNN’s flexibility doesn’t match the shape of the data.</p>
</div>
<div id="choosing-the-model" class="section level3">
<h3><span class="header-section-number">4.7.5</span> Choosing the model</h3>
<p>Given logistic regression and LDA exhibiting similar performance metrics, we could use either model. However, logistic regression makes less assumptions about the underlying data.</p>
<p>Let’s take a look at the ROC curve using <code>yardstick::roc_curve()</code> function.</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb166-1" data-line-number="1"><span class="co"># https://stats.stackexchange.com/questions/188416/discriminant-analysis-vs-logistic-regression</span></a>
<a class="sourceLine" id="cb166-2" data-line-number="2">logi_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(logi_churn, test_churn, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)</a>
<a class="sourceLine" id="cb166-3" data-line-number="3">test_churn <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb166-4" data-line-number="4"><span class="st">  </span><span class="kw">bind_cols</span>(logi_preds) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb166-5" data-line-number="5"><span class="st">  </span><span class="kw">roc_curve</span>(churn, .pred_Yes) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb166-6" data-line-number="6"><span class="st">  </span><span class="kw">autoplot</span>()</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-110-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Our sensitivity (ability to correctly identify those who will churn) has two visible inflection points where the slope changes, one around <code>~55%</code> and the other around <code>~80%</code>. What should we pick as our classifier threshold?</p>
</div>
<div id="evaluating-the-threshold" class="section level3">
<h3><span class="header-section-number">4.7.6</span> Evaluating the threshold</h3>
<p>Choosing a classification threshold greatly depends on the problem context.</p>
<p>Imagine we want to hand out an incentive to customers we believe will churn. This incentive costs us money, but has a <code>100%</code> success rate of retaining a customer (turning them from a <code>Yes</code> to <code>No</code> churn).</p>
<p>Let’s assume the cost of this incentive for us is <code>$75</code>, but the value of retaining the user is <code>$150</code>. We want to maximize the return we get from this incentive.</p>
<p>In this case, every true positive will net us <code>$75</code> of profit, and every false positive will cost us <code>$75</code> (the customer doesn’t churn, so giving the incentive was a waste). We want to find the optimal decision boundary where we maximize our return from this incentive.</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb167-1" data-line-number="1">ltv =<span class="st"> </span><span class="dv">150</span></a>
<a class="sourceLine" id="cb167-2" data-line-number="2">incentive =<span class="st"> </span><span class="dv">75</span></a>
<a class="sourceLine" id="cb167-3" data-line-number="3"></a>
<a class="sourceLine" id="cb167-4" data-line-number="4">logi_class &lt;-<span class="st"> </span><span class="kw">predict</span>(logi_churn, test_churn, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb167-5" data-line-number="5">costs &lt;-<span class="st"> </span>test_churn <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb167-6" data-line-number="6"><span class="st">  </span><span class="kw">bind_cols</span>(logi_preds) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb167-7" data-line-number="7"><span class="st">  </span><span class="kw">bind_cols</span>(logi_class) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb167-8" data-line-number="8"><span class="st">  </span><span class="kw">roc_curve</span>(churn, .pred_Yes) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb167-9" data-line-number="9"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">profit  =</span> sensitivity <span class="op">*</span><span class="st"> </span>(ltv <span class="op">-</span><span class="st"> </span>incentive),</a>
<a class="sourceLine" id="cb167-10" data-line-number="10">         <span class="dt">cost =</span> (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>specificity) <span class="op">*</span><span class="st"> </span>incentive,</a>
<a class="sourceLine" id="cb167-11" data-line-number="11">         <span class="dt">net =</span> profit <span class="op">-</span><span class="st"> </span>cost)</a>
<a class="sourceLine" id="cb167-12" data-line-number="12"></a>
<a class="sourceLine" id="cb167-13" data-line-number="13">costs <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb167-14" data-line-number="14"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> sensitivity, <span class="dt">y =</span> net)) <span class="op">+</span></a>
<a class="sourceLine" id="cb167-15" data-line-number="15"><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb167-16" data-line-number="16"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">max</span>(net)))</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-111-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>We can grab the optimal sensitivity and find the corresponding decision threshold.</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb168-1" data-line-number="1">optimal_sens &lt;-<span class="st"> </span>costs <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb168-2" data-line-number="2"><span class="st">  </span><span class="kw">filter</span>(net <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(net))</a>
<a class="sourceLine" id="cb168-3" data-line-number="3">optimal_sens</a></code></pre></div>
<pre><code>## # A tibble: 1 x 6
##   .threshold specificity sensitivity profit  cost   net
##        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1      0.308       0.757       0.764   57.3  18.3  39.1</code></pre>
<p>We find that our ideal sensitivity of <code>76.4%</code> corresponds with a specificity of <code>75.7%</code>. For whatever reason, <code>roc_curve()</code> gives the <code>.threshold</code> column the predicted <code>No</code> churn class (despite specifying an ROC curve with <code>.pred_Yes</code> column). Thus, if we eastimate a customer to have a churn percentage greater or equal to <code>1 - 0.308 = 0.692</code>, we can offer them this incentive.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb170-1" data-line-number="1">yes_threshold &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(optimal_sens<span class="op">$</span>.threshold)</a>
<a class="sourceLine" id="cb170-2" data-line-number="2">test_churn <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb170-3" data-line-number="3"><span class="st">  </span><span class="kw">bind_cols</span>(logi_preds) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb170-4" data-line-number="4"><span class="st">  </span><span class="kw">filter</span>(.pred_Yes <span class="op">&gt;=</span><span class="st"> </span>yes_threshold) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb170-5" data-line-number="5"><span class="st">  </span><span class="kw">group_by</span>(churn) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tally</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb170-6" data-line-number="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">revenue =</span> <span class="kw">if_else</span>(churn <span class="op">==</span><span class="st"> &quot;No&quot;</span>, n <span class="op">*</span><span class="st"> </span><span class="op">-</span>incentive, n <span class="op">*</span><span class="st"> </span>(ltv <span class="op">-</span><span class="st"> </span>incentive)))</a></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   churn     n revenue
##   &lt;fct&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 No       39   -2925
## 2 Yes     103    7725</code></pre>
<p>This incentive program will cost us <code>~$23k</code> through false positives (handing out incentives to customers that won’t churn), but will net us <code>~$26k</code> in revenue from true positives. It’s a gross oversimplification, but we expect this program to yield us <code>~11%</code> profit margin.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2><span class="header-section-number">4.8</span> Conclusion</h2>
<p>We demonstrated how a classification model could be used to solve a real-world problem in which we want to maximize the value of sending incentives to customers that are likely to churn.</p>
</div>
<div id="exercises-2" class="section level2">
<h2><span class="header-section-number">4.9</span> Exercises</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="resampling-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["tidy_islr.pdf", "tidy_islr.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
