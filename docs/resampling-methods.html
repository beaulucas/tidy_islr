<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Resampling Methods | A Tidy Introduction To Statistical Learning</title>
  <meta name="description" content="Chapter 5 Resampling Methods | A Tidy Introduction To Statistical Learning" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Resampling Methods | A Tidy Introduction To Statistical Learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Resampling Methods | A Tidy Introduction To Statistical Learning" />
  
  
  

<meta name="author" content="Beau Lucas" />


<meta name="date" content="2020-02-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Tidy Introduction To Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#an-overview-of-statistical-learning"><i class="fa fa-check"></i><b>1.1</b> An Overview of Statistical Learning</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#data-sets-used-in-labs-and-exercises"><i class="fa fa-check"></i><b>1.2</b> Data Sets Used in Labs and Exercises</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#book-resources"><i class="fa fa-check"></i><b>1.3</b> Book Resources:</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-learning.html"><a href="statistical-learning.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#packages-used-in-this-chapter"><i class="fa fa-check"></i><b>2.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#what-is-statistical-learning"><i class="fa fa-check"></i><b>2.2</b> What is Statistical Learning?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#why-estimate-f"><i class="fa fa-check"></i><b>2.2.1</b> Why Estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#how-do-we-estimate-f"><i class="fa fa-check"></i><b>2.2.2</b> How do we estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-trade-off-between-prediction-accuracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.2.3</b> The Trade-Off Between Prediction Accuracy and Model Interpretability</a></li>
<li class="chapter" data-level="2.2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#supervised-versus-unsupervised-learning"><i class="fa fa-check"></i><b>2.2.4</b> Supervised Versus Unsupervised Learning</a></li>
<li class="chapter" data-level="2.2.5" data-path="statistical-learning.html"><a href="statistical-learning.html#regression-versus-classification-problems"><i class="fa fa-check"></i><b>2.2.5</b> Regression Versus Classification Problems</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.3</b> Assessing Model Accuracy</a><ul>
<li class="chapter" data-level="2.3.1" data-path="statistical-learning.html"><a href="statistical-learning.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.3.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.3.2" data-path="statistical-learning.html"><a href="statistical-learning.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>2.3.2</b> The Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="2.3.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-classification-setting"><i class="fa fa-check"></i><b>2.3.3</b> The Classification Setting</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#lab-introduction-to-r"><i class="fa fa-check"></i><b>2.4</b> Lab: Introduction to R</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-learning.html"><a href="statistical-learning.html#exercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#packages-used-in-this-chapter-1"><i class="fa fa-check"></i><b>3.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>3.2.1</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimate"><i class="fa fa-check"></i><b>3.2.2</b> Assessing the Accuracy of the Coefficient Estimate</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-model"><i class="fa fa-check"></i><b>3.2.3</b> Assessing the Accuracy of the Model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>3.3.1</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-regression.html"><a href="linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>3.3.2</b> Some Important Questions</a></li>
<li class="chapter" data-level="3.3.3" data-path="linear-regression.html"><a href="linear-regression.html#other-considerations-in-the-regression-model"><i class="fa fa-check"></i><b>3.3.3</b> Other Considerations in the Regression Model</a></li>
<li class="chapter" data-level="3.3.4" data-path="linear-regression.html"><a href="linear-regression.html#qualitative-predictors-with-more-than-two-levels"><i class="fa fa-check"></i><b>3.3.4</b> Qualitative Predictors with More than Two Levels</a></li>
<li class="chapter" data-level="3.3.5" data-path="linear-regression.html"><a href="linear-regression.html#extensions-of-the-linear-model"><i class="fa fa-check"></i><b>3.3.5</b> Extensions of the Linear Model</a></li>
<li class="chapter" data-level="3.3.6" data-path="linear-regression.html"><a href="linear-regression.html#potential-problems"><i class="fa fa-check"></i><b>3.3.6</b> Potential Problems</a></li>
<li class="chapter" data-level="3.3.7" data-path="linear-regression.html"><a href="linear-regression.html#the-marketing-plan"><i class="fa fa-check"></i><b>3.3.7</b> The Marketing Plan</a></li>
<li class="chapter" data-level="3.3.8" data-path="linear-regression.html"><a href="linear-regression.html#comparison-of-linear-regression-with-k-nearest-neighbors"><i class="fa fa-check"></i><b>3.3.8</b> Comparison of Linear Regression with <em>K</em>-Nearest Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-regression.html"><a href="linear-regression.html#lab-linear-regression"><i class="fa fa-check"></i><b>3.4</b> Lab: Linear Regression</a><ul>
<li class="chapter" data-level="3.4.1" data-path="linear-regression.html"><a href="linear-regression.html#fitting-a-linear-regression"><i class="fa fa-check"></i><b>3.4.1</b> Fitting a linear regression</a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression-1"><i class="fa fa-check"></i><b>3.4.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="3.4.3" data-path="linear-regression.html"><a href="linear-regression.html#interaction-terms"><i class="fa fa-check"></i><b>3.4.3</b> Interaction Terms</a></li>
<li class="chapter" data-level="3.4.4" data-path="linear-regression.html"><a href="linear-regression.html#non-linear-transformations-of-the-predictors"><i class="fa fa-check"></i><b>3.4.4</b> Non-linear Transformations of the Predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="linear-regression.html"><a href="linear-regression.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>4</b> Classification</a><ul>
<li class="chapter" data-level="4.1" data-path="classification.html"><a href="classification.html#packages-used-in-this-chapter-2"><i class="fa fa-check"></i><b>4.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="4.2" data-path="classification.html"><a href="classification.html#an-overview-of-classification"><i class="fa fa-check"></i><b>4.2</b> An Overview of Classification</a></li>
<li class="chapter" data-level="4.3" data-path="classification.html"><a href="classification.html#why-not-linear-regression"><i class="fa fa-check"></i><b>4.3</b> Why Not Linear Regression?</a></li>
<li class="chapter" data-level="4.4" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>4.4</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.4.1" data-path="classification.html"><a href="classification.html#the-logistic-model"><i class="fa fa-check"></i><b>4.4.1</b> The Logistic Model</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification.html"><a href="classification.html#estimating-the-regression-coefficients-1"><i class="fa fa-check"></i><b>4.4.2</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="4.4.3" data-path="classification.html"><a href="classification.html#making-predictions"><i class="fa fa-check"></i><b>4.4.3</b> Making Predictions</a></li>
<li class="chapter" data-level="4.4.4" data-path="classification.html"><a href="classification.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>4.4.4</b> Multiple Logistic Regression</a></li>
<li class="chapter" data-level="4.4.5" data-path="classification.html"><a href="classification.html#logistic-regression-for-2-response-classes"><i class="fa fa-check"></i><b>4.4.5</b> Logistic Regression for &gt;2 Response Classes</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>4.5</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="4.5.1" data-path="classification.html"><a href="classification.html#using-bayes-theorem-for-classification"><i class="fa fa-check"></i><b>4.5.1</b> Using Bayes’ Theorem for Classification</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1"><i class="fa fa-check"></i><b>4.5.2</b> Linear Discriminant Analysis for p = 1</a></li>
<li class="chapter" data-level="4.5.3" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1-1"><i class="fa fa-check"></i><b>4.5.3</b> Linear Discriminant Analysis for p &gt; 1</a></li>
<li class="chapter" data-level="4.5.4" data-path="classification.html"><a href="classification.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>4.5.4</b> Quadratic Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="classification.html"><a href="classification.html#a-comparison-of-classification-methods"><i class="fa fa-check"></i><b>4.6</b> A Comparison of Classification Methods</a></li>
<li class="chapter" data-level="4.7" data-path="classification.html"><a href="classification.html#lab-logistic-regression-lda-qda-and-knn"><i class="fa fa-check"></i><b>4.7</b> Lab: Logistic Regression, LDA, QDA, and KNN</a><ul>
<li class="chapter" data-level="4.7.1" data-path="classification.html"><a href="classification.html#churn-dataset"><i class="fa fa-check"></i><b>4.7.1</b> Churn Dataset</a></li>
<li class="chapter" data-level="4.7.2" data-path="classification.html"><a href="classification.html#logistic-regression-1"><i class="fa fa-check"></i><b>4.7.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.7.3" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-1"><i class="fa fa-check"></i><b>4.7.3</b> Linear discriminant analysis</a></li>
<li class="chapter" data-level="4.7.4" data-path="classification.html"><a href="classification.html#k-nearest-neighbors-1"><i class="fa fa-check"></i><b>4.7.4</b> K-Nearest Neighbors</a></li>
<li class="chapter" data-level="4.7.5" data-path="classification.html"><a href="classification.html#choosing-the-model"><i class="fa fa-check"></i><b>4.7.5</b> Choosing the model</a></li>
<li class="chapter" data-level="4.7.6" data-path="classification.html"><a href="classification.html#evaluating-the-threshold"><i class="fa fa-check"></i><b>4.7.6</b> Evaluating the threshold</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="classification.html"><a href="classification.html#conclusion"><i class="fa fa-check"></i><b>4.8</b> Conclusion</a></li>
<li class="chapter" data-level="4.9" data-path="classification.html"><a href="classification.html#exercises-2"><i class="fa fa-check"></i><b>4.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="resampling-methods.html"><a href="resampling-methods.html"><i class="fa fa-check"></i><b>5</b> Resampling Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation"><i class="fa fa-check"></i><b>5.1</b> Cross-Validation</a><ul>
<li class="chapter" data-level="5.1.1" data-path="resampling-methods.html"><a href="resampling-methods.html#the-validation-set-approach"><i class="fa fa-check"></i><b>5.1.1</b> The Validation Set Approach</a></li>
<li class="chapter" data-level="5.1.2" data-path="resampling-methods.html"><a href="resampling-methods.html#leave-one-out-cross-validation."><i class="fa fa-check"></i><b>5.1.2</b> Leave-One-Out Cross-Validation.</a></li>
<li class="chapter" data-level="5.1.3" data-path="resampling-methods.html"><a href="resampling-methods.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.1.3</b> k-Fold Cross-Validation</a></li>
<li class="chapter" data-level="5.1.4" data-path="resampling-methods.html"><a href="resampling-methods.html#bias-variance-trade-off-for-k-fold-cross-validation"><i class="fa fa-check"></i><b>5.1.4</b> Bias-Variance Trade-Off for <em>k</em>-fold Cross-Validation</a></li>
<li class="chapter" data-level="5.1.5" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation-on-classification-problems"><i class="fa fa-check"></i><b>5.1.5</b> Cross-Validation on Classification Problems</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="resampling-methods.html"><a href="resampling-methods.html#the-bootstrap"><i class="fa fa-check"></i><b>5.2</b> The Bootstrap</a></li>
<li class="chapter" data-level="5.3" data-path="resampling-methods.html"><a href="resampling-methods.html#lab"><i class="fa fa-check"></i><b>5.3</b> Lab</a><ul>
<li class="chapter" data-level="5.3.1" data-path="resampling-methods.html"><a href="resampling-methods.html#the-validation-set-approach-1"><i class="fa fa-check"></i><b>5.3.1</b> The Validation Set Approach</a></li>
<li class="chapter" data-level="5.3.2" data-path="resampling-methods.html"><a href="resampling-methods.html#loocv"><i class="fa fa-check"></i><b>5.3.2</b> LOOCV</a></li>
<li class="chapter" data-level="5.3.3" data-path="resampling-methods.html"><a href="resampling-methods.html#k-fold-cross-validation-1"><i class="fa fa-check"></i><b>5.3.3</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation</a></li>
<li class="chapter" data-level="5.3.4" data-path="resampling-methods.html"><a href="resampling-methods.html#the-bootstrap-1"><i class="fa fa-check"></i><b>5.3.4</b> The Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="resampling-methods.html"><a href="resampling-methods.html#exercises-3"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/beaulucas/tidy_islr" target="blank">GitHub Repository</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tidy Introduction To Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="resampling-methods" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Resampling Methods</h1>
<hr />
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb172-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb172-2" data-line-number="2"><span class="kw">library</span>(knitr)</a>
<a class="sourceLine" id="cb172-3" data-line-number="3"><span class="kw">library</span>(skimr)</a>
<a class="sourceLine" id="cb172-4" data-line-number="4"><span class="kw">library</span>(ISLR)</a>
<a class="sourceLine" id="cb172-5" data-line-number="5"><span class="kw">library</span>(tidymodels)</a></code></pre></div>
<p>Resampling methods involve repeatedly drawing samples from a training set and refitting a model of interest on each sample. This provides additional information about the fitted model.</p>
<p>If we wanted to estimate the variability of a linear regression fit, we could repeatedly draw different samples from the training data, fit a linear regression model to each sample, and then measure how our fits differ. This can yield us novel information that would otherwise not be available from fitting a single model using the entirety of the training data.</p>
<p>It is computationally expensive to repeatedly fit statistical methods across different subsets of data. In most cases, this cost is not prohibitive.</p>
<p>In this chapter, we will look at two of the most common resampling methods, <em>cross-validation</em> and the <em>bootstrap</em>. Cross-validation can be used to estimate the test error associated with a given statistical learning method or to select the appropriate level of flexibility. Evaluating a model’s performance is known as <em>model assessment</em>. Selecting the proper level of flexibility for a model is <em>model selection</em>. The bootstrap is most commonly used to provide a measure of accuracy of a parameter estimate or of a given statistical learning method.</p>
<div id="cross-validation" class="section level2">
<h2><span class="header-section-number">5.1</span> Cross-Validation</h2>
<p>Let’s remember the distinction between the <em>test error rate</em> and the <em>training error rate</em>.</p>
<p>The test error is the average error that results from using the statistical learning method to predict the response on a new observation (test dataset). In general, the method with the lowest test error is warranted to use. If a designated test set is available, then calculating the test error is trivial. Unfortunately, there are times when a a reasonably-sized test dataset is difficult or impossible to achieve.</p>
<p>The training error can be calculated by applying the statistical learning method to the observations used in its training. But this training error is often quite different from the test error rate, and in particular can significantly underestimate the test error rate.</p>
<p>Without a large, designated test set, a number of techniques can be used to estimate this quantity using the training data. Some methods make a mathematical adjustment to the training error rate (discussed in Chapter 6 TODO add link). In this chapter, we instead consider a class of methods that estimate the test error rate by <em>holding out</em> a subset of the training observations from the fitting process.</p>
<div id="the-validation-set-approach" class="section level3">
<h3><span class="header-section-number">5.1.1</span> The Validation Set Approach</h3>
<p>The <em>validation set approach</em> is a strategy to estimate the test error associated with fitting a particular statistical learning method on a set of observations. It involves randomly diving the available set of observations into two parts, a <em>training set</em> and a <em>validation set</em> or <em>hold-out set</em>. The model is fit on the training set, and the model is then used to predict responses for observations in the validation set. The resulting validation set error rate provides an estimate of the test error rate.</p>
<p>Let’s try this out on the <code>ISLR::Auto</code> dataset. We can utilize <a href="https://tidymodels.github.io/rsample/index.html"><code>{rsample}</code></a> (part of the <code>{tidymodels}</code> ecosystem) to handle this. We used this package in the lab of the previous chapter to split our data into training and test sets.</p>
<pre><code>The scope of rsample is to provide the basic building blocks for creating and analyzing resamples of a data set but does not include code for modeling or calculating statistics.</code></pre>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb174-1" data-line-number="1">auto &lt;-<span class="st"> </span>ISLR<span class="op">::</span>Auto <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>()</a>
<a class="sourceLine" id="cb174-2" data-line-number="2">split_auto &lt;-<span class="st"> </span><span class="kw">initial_split</span>(auto, <span class="dt">prop =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb174-3" data-line-number="3">train_auto &lt;-<span class="st"> </span><span class="kw">training</span>(split_auto)</a>
<a class="sourceLine" id="cb174-4" data-line-number="4">test_auto &lt;-<span class="st"> </span><span class="kw">testing</span>(split_auto)</a></code></pre></div>
<p>We split the data using <code>initial_split()</code>, and then assigned splits to training and validation (test) sets.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb175-1" data-line-number="1"><span class="kw">nrow</span>(train_auto) <span class="op">==</span><span class="st"> </span><span class="kw">nrow</span>(test_auto)</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Now let’s fit various regression models on the <code>train_auto</code> dataset. We will repeat the sampling process and fit linear regression models for polynomials from 1st to 10th degree. All in all, we create one-hundred fits, each fit and tested on a random, one-half sample of the data.</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb177-1" data-line-number="1"><span class="co"># </span><span class="al">TODO</span><span class="co"> fix this with purrr and clean up</span></a>
<a class="sourceLine" id="cb177-2" data-line-number="2"><span class="co"># copied from https://uc-r.github.io/resampling_methods</span></a>
<a class="sourceLine" id="cb177-3" data-line-number="3"></a>
<a class="sourceLine" id="cb177-4" data-line-number="4">df_mse &lt;-<span class="st"> </span><span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb177-5" data-line-number="5">  <span class="dt">sample =</span> <span class="kw">vector</span>(<span class="st">&quot;integer&quot;</span>, <span class="dv">100</span>),</a>
<a class="sourceLine" id="cb177-6" data-line-number="6">  <span class="dt">degree =</span> <span class="kw">vector</span>(<span class="st">&quot;integer&quot;</span>, <span class="dv">100</span>),</a>
<a class="sourceLine" id="cb177-7" data-line-number="7">  <span class="dt">mse =</span> <span class="kw">vector</span>(<span class="st">&quot;double&quot;</span>, <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb177-8" data-line-number="8">)</a>
<a class="sourceLine" id="cb177-9" data-line-number="9">counter &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb177-10" data-line-number="10"></a>
<a class="sourceLine" id="cb177-11" data-line-number="11"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) {</a>
<a class="sourceLine" id="cb177-12" data-line-number="12">  <span class="co"># random sample</span></a>
<a class="sourceLine" id="cb177-13" data-line-number="13">  <span class="kw">set.seed</span>(i)</a>
<a class="sourceLine" id="cb177-14" data-line-number="14">  split_auto &lt;-<span class="st"> </span><span class="kw">initial_split</span>(auto, <span class="dt">prop =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb177-15" data-line-number="15">  train_auto &lt;-<span class="st"> </span><span class="kw">training</span>(split_auto)</a>
<a class="sourceLine" id="cb177-16" data-line-number="16">  test_auto &lt;-<span class="st"> </span><span class="kw">testing</span>(split_auto)</a>
<a class="sourceLine" id="cb177-17" data-line-number="17"></a>
<a class="sourceLine" id="cb177-18" data-line-number="18">  <span class="co"># modeling</span></a>
<a class="sourceLine" id="cb177-19" data-line-number="19">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) {</a>
<a class="sourceLine" id="cb177-20" data-line-number="20">    lm.fit &lt;-<span class="st"> </span><span class="kw">linear_reg</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb177-21" data-line-number="21"><span class="st">      </span><span class="kw">fit</span>(</a>
<a class="sourceLine" id="cb177-22" data-line-number="22">        <span class="dt">data =</span> train_auto,</a>
<a class="sourceLine" id="cb177-23" data-line-number="23">        <span class="dt">formula =</span> mpg <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(horsepower, j)</a>
<a class="sourceLine" id="cb177-24" data-line-number="24">      )</a>
<a class="sourceLine" id="cb177-25" data-line-number="25">    lm_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(lm.fit, <span class="dt">new_data =</span> test_auto)</a>
<a class="sourceLine" id="cb177-26" data-line-number="26"></a>
<a class="sourceLine" id="cb177-27" data-line-number="27">    <span class="co"># calculate mse</span></a>
<a class="sourceLine" id="cb177-28" data-line-number="28">    mse &lt;-<span class="st"> </span>test_auto <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb177-29" data-line-number="29"><span class="st">      </span><span class="kw">bind_cols</span>(lm_preds) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb177-30" data-line-number="30"><span class="st">      </span><span class="kw">summarise</span>(<span class="dt">mse =</span> <span class="kw">mean</span>((.pred <span class="op">-</span><span class="st"> </span>mpg)<span class="op">^</span><span class="dv">2</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb177-31" data-line-number="31"><span class="st">      </span><span class="kw">pull</span>()</a>
<a class="sourceLine" id="cb177-32" data-line-number="32"></a>
<a class="sourceLine" id="cb177-33" data-line-number="33">    <span class="co"># add degree &amp; mse values to tibble</span></a>
<a class="sourceLine" id="cb177-34" data-line-number="34">    df_mse[counter, <span class="dv">2</span>] &lt;-<span class="st"> </span>j</a>
<a class="sourceLine" id="cb177-35" data-line-number="35">    df_mse[counter, <span class="dv">3</span>] &lt;-<span class="st"> </span>mse</a>
<a class="sourceLine" id="cb177-36" data-line-number="36"></a>
<a class="sourceLine" id="cb177-37" data-line-number="37">    <span class="co"># add sample identifier</span></a>
<a class="sourceLine" id="cb177-38" data-line-number="38">    df_mse[counter, <span class="dv">1</span>] &lt;-<span class="st"> </span>i</a>
<a class="sourceLine" id="cb177-39" data-line-number="39">    counter &lt;-<span class="st"> </span>counter <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb177-40" data-line-number="40">  }</a>
<a class="sourceLine" id="cb177-41" data-line-number="41">  <span class="cf">next</span></a>
<a class="sourceLine" id="cb177-42" data-line-number="42">}</a></code></pre></div>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb178-1" data-line-number="1"><span class="kw">ggplot</span>(df_mse, <span class="kw">aes</span>(<span class="dt">x =</span> degree, <span class="dt">y =</span> mse, <span class="dt">color =</span> <span class="kw">factor</span>(sample))) <span class="op">+</span></a>
<a class="sourceLine" id="cb178-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb178-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb178-4" data-line-number="4"><span class="st">  </span><span class="co"># scale_x_continuous(breaks = scales::pretty_breaks())</span></a>
<a class="sourceLine" id="cb178-5" data-line-number="5"><span class="st">  </span><span class="kw">ylim</span>(<span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">30</span>))</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-118-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>While the curves vary in MSE values, they all exhibit a similar shape. All ten curves indicarte that the model with a quadratic term has a significantly smaller MSE than the linear term. All curves also include there isn’t much additionao benefit in extended beyond cubic or higher-order polynomials in the model. However, the variability of the curves and different test MSE estimates generated by each fit can only lead us to conclude that a linear fit is not adequate for this data.</p>
<p>There are two drawbacks of the validation set approach:</p>
<ol style="list-style-type: decimal">
<li><p>The validation estimate of the test error rate can be highly variable depending on which observations are included in the training set and which are included in the validation set.</p></li>
<li><p>Only a subset of observations are used to fit the model. Statisticsl methods perform worse when trained on fewer observations. In some cases, this sample size problem can lead the validation set error rate to be an <em>overestimate</em> of the test error rate.</p></li>
</ol>
</div>
<div id="leave-one-out-cross-validation." class="section level3">
<h3><span class="header-section-number">5.1.2</span> Leave-One-Out Cross-Validation.</h3>
<p><em>Leave-one-out cross-validation (LOOCV)</em> is closely related to the validation set approach.</p>
<p>LOOCV also involves splitting the set of observations into two parts. However, a single observations <span class="math inline">\((x_1, y_1)\)</span> is used for the validation set. The remaining observations make up the training set. We fit the model on the <span class="math inline">\(n - 1\)</span> training observations, and make a prediction for the excluded observation. Since the held-out observation was not used in the training set, it provides an approximately unbiased estimate for the test error. While it may be unbiased, it is highly variable, causing it to be a poor estimate.</p>
<p>We can repeat this procedure by holding out differnet observations, again fitting the model on the remaining <span class="math inline">\(n - 1\)</span> observations, and computing the MSE of the held-out observation. If we repeat this approach <span class="math inline">\(n\)</span> times, we generate <span class="math inline">\(n\)</span> squared errors. The LOOCV estimate for the test MSE is the average of these <span class="math inline">\(n\)</span> test error estimates.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(CV_(n) = \frac{1}{n} \sum_{i=1}^{n}MSE_i\)</span>
</p>
</div>
<p>LOOCV has a couple of major advantages over the validation set approach. First, it has far less bias. We are repeatedly fitting the statistical learning method to almost all the observations in the entire dataset. As a result, it tends to not overestimate the test error rate as much as the validation set approach does. Secondly, performing LOOCV multiple times will always yield the same results. There is no randomness in the training/validation set splits.</p>
<p>Since LOOCV has to fit <span class="math inline">\(n\)</span> times, it can be expensive to implement on large datasets or high-computational statistical learning methods. For least squares linear or polynomial regression, the cost of LOOCV is actually the same as a single model fit if we take advantage of an observation’s leverage.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(CV_(n) = \frac{1}{n} \sum_{i=1}^{n}(\frac{y_i - \hat{y}_i}{1-h_i})^2\)</span>
</p>
</div>
<p>In this case, we divide residuals of a fitted observation by how much that observation influences its own fit.</p>
<p>LOOCV can be used with any kind of predictive modeling.</p>
</div>
<div id="k-fold-cross-validation" class="section level3">
<h3><span class="header-section-number">5.1.3</span> k-Fold Cross-Validation</h3>
<p>An alternative to LOOCV is <em>k-fold CV</em>. This approach involves randomly dividing the set of observations into <em>k</em> groups, or <em>folds</em>, of approximatelty equal size. The first fold is treated as a valdation set, and the method is fit on the remaining <span class="math inline">\(k - 1\)</span> folds. The mean squared error, <span class="math inline">\(MSE_1\)</span>, is then computed on the observations in the held-out fold. This procedure is repeated <span class="math inline">\(k\)</span> times. Each time, a different group of observations is treated as a validation set. This process results in <span class="math inline">\(k\)</span> estimates of the test error, <span class="math inline">\(MSE_1, MSE_2, ..., MSE_k\)</span>. The <span class="math inline">\(k\)</span>-fold CV estimate computed by averaging these values,</p>
<div>
<p style="text-align:center">
<span class="math inline">\(CV_(k) = \frac{1}{k} \sum_{i=1}^{k}MSE_i\)</span>
</p>
</div>
<p>We can also think of LOOCV as a special case of <span class="math inline">\(k\)</span>-fold CV in which <span class="math inline">\(k\)</span> is set equal to <span class="math inline">\(n\)</span>. It is common to find <span class="math inline">\(k\)</span>-fold CV operations using <span class="math inline">\(k = 5\)</span> or <span class="math inline">\(k = 10\)</span>. Part of using small values of <span class="math inline">\(k\)</span> is computational costs, especially with large datasets and/or expensive learning methods. Later on, we will also discus non-computational advantages of low <span class="math inline">\(k\)</span>-fold CV, which involve the bias-variance tradeoff.</p>
<p>With real data, we do not know the <em>true</em> test MSE. With simulated data, we can compute the true test MSE and actually compare how accurate cross-validation is.</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb179-1" data-line-number="1"><span class="co"># </span><span class="al">TODO</span><span class="co"> v-fold CV</span></a></code></pre></div>
<p>When we perform cross-validation, our goal might be to determine how well a statistical learning method can be expected to perform on independent date. In other cases, we are interested in the location of the <em>minimum point in the estimated test MSE curve</em>. This is because we might be performing cross-validation on a number of different statistical learning methods, or the same method with varying levels of flexibility. We can use the minimum point in the estimated test MSE curve to choose an appropriate model.</p>
<p>While CV curves can over or underestimate the true test MSE, they generally come close to identifying the correct level of model flexibility.</p>
</div>
<div id="bias-variance-trade-off-for-k-fold-cross-validation" class="section level3">
<h3><span class="header-section-number">5.1.4</span> Bias-Variance Trade-Off for <em>k</em>-fold Cross-Validation</h3>
<p><span class="math inline">\(k\)</span>-fold validation with <span class="math inline">\(k &lt; n\)</span> has computational advantages to LOOCV. It also often gives more accurate estimates of the test error rate than does LOOCV due to the bias-variance trade-off.</p>
<p>We know that the validation set approach tends to overestimate the test error due to training on only half of the available data. It is not hard to conclude that LOOCV, which uses training sets of <span class="math inline">\(n - 1\)</span> observations (virtually the entire data set), will give approximately unbiased estimates of the test error. <span class="math inline">\(k\)</span>-fold CV, which is trained on more observations than the validation set approach but less than LOOCV, will produce an intermediate level of bias. From the perspective of bias reduction, LOOCV is the winner.</p>
<p>However, bias is not the only source of test error. The procedure’s variance is equally as important. LOOCV has higher variance than <span class="math inline">\(k\)</span>-fold with <span class="math inline">\(k &lt; n\)</span>. LOOCV always comparing models that are trained on an almost identical set of observations. These outputs are highly correlated, with each other. In contrast, <span class="math inline">\(k\)</span>-fold with values <span class="math inline">\(k &lt; n\)</span> averages <span class="math inline">\(k\)</span> models that share less overlap in observations. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than the test error estimate resulting from <span class="math inline">\(k\)</span>-fold CV.</p>
<p>There is a bias-variance trade-off associated with the choice of <span class="math inline">\(k\)</span> in <span class="math inline">\(k\)</span>-fold cross-validation. Empirical evidence shows that <span class="math inline">\(k\)</span> values between 5 and 10 tend to yield test error estimates with optimal bias-variance trade-off.</p>
</div>
<div id="cross-validation-on-classification-problems" class="section level3">
<h3><span class="header-section-number">5.1.5</span> Cross-Validation on Classification Problems</h3>
<p>Thus far, we have shown cross-validation as it relates to regression, using MSE as an evaluation metric. But cross-validation is equally as useful in the classification setting. Instead of MSE, we can use the number of misclassified observations.</p>
<p>In the classification setting, the LOOCV error rate takes the form</p>
<div>
<p style="text-align:center">
<span class="math inline">\(CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n}Err_i\)</span>
</p>
</div>
<p>Where <span class="math inline">\(Err_i\)</span> are the number of misclassified observations. The <span class="math inline">\(k\)</span>-fold CV error rate and validation set error rates are defined analogously.</p>
<p>As an example, let’s fit various logistic regression models on some two-dimensional classification data.</p>
<p>First, we generate two-dimensional data with classes exhibiting visual, but not perfect, separation.</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb180-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">10</span>)</a>
<a class="sourceLine" id="cb180-2" data-line-number="2">sim_sep &lt;-<span class="st"> </span><span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb180-3" data-line-number="3">  <span class="dt">x =</span> <span class="kw">rnorm</span>(<span class="dv">5000</span>, <span class="dt">mean =</span> <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb180-4" data-line-number="4">  <span class="dt">y =</span> <span class="kw">rnorm</span>(<span class="dv">5000</span>, <span class="dt">mean =</span> <span class="dv">20</span>),</a>
<a class="sourceLine" id="cb180-5" data-line-number="5">  <span class="dt">class =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">0</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">2500</span>, <span class="dv">2500</span>))</a>
<a class="sourceLine" id="cb180-6" data-line-number="6">)</a>
<a class="sourceLine" id="cb180-7" data-line-number="7"><span class="co"># add some separation</span></a>
<a class="sourceLine" id="cb180-8" data-line-number="8">sim_sep &lt;-<span class="st"> </span>sim_sep <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb180-9" data-line-number="9"><span class="st">  </span><span class="kw">mutate</span>(</a>
<a class="sourceLine" id="cb180-10" data-line-number="10">    <span class="dt">y =</span> <span class="kw">if_else</span>(class <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, y <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>), y),</a>
<a class="sourceLine" id="cb180-11" data-line-number="11">    <span class="dt">class =</span> <span class="kw">as.factor</span>(class)</a>
<a class="sourceLine" id="cb180-12" data-line-number="12">  )</a>
<a class="sourceLine" id="cb180-13" data-line-number="13">sim_sep <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb180-14" data-line-number="14"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">colour =</span> class)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-120-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Now let’s fit logistic regression models of varying flexibilities.</p>
<p>We can extend our original logistic regression to obtain a non-linear decision boundary by using polynomial functions of the predictors. Below is an example of <em>quadratic logistic regression</em> with two predictors.</p>
<p><span class="math display">\[
\log \left(\frac{p}{1-p}\right)=\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{1}^{2}+\beta_{3} X_{2}+\beta_{4} X_{2}^{2}
\]</span></p>
<p>We will use this extension to fit logistic regression models from degrees <code>1</code> through <code>4</code>.</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb181-1" data-line-number="1">split_sim &lt;-<span class="st"> </span><span class="kw">initial_split</span>(sim_sep, <span class="dt">prop =</span> <span class="dv">3</span> <span class="op">/</span><span class="st"> </span><span class="dv">4</span>)</a>
<a class="sourceLine" id="cb181-2" data-line-number="2">train_sim &lt;-<span class="st"> </span><span class="kw">training</span>(split_sim)</a>
<a class="sourceLine" id="cb181-3" data-line-number="3">test_sim &lt;-<span class="st"> </span><span class="kw">testing</span>(split_sim)</a>
<a class="sourceLine" id="cb181-4" data-line-number="4"></a>
<a class="sourceLine" id="cb181-5" data-line-number="5"><span class="co"># </span><span class="al">TODO</span><span class="co"> loop</span></a>
<a class="sourceLine" id="cb181-6" data-line-number="6">logi_sim &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>(<span class="dt">mode =</span> <span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb181-7" data-line-number="7"><span class="st">  </span><span class="kw">fit</span>(train_sim, <span class="dt">formula =</span> class <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>y)</a>
<a class="sourceLine" id="cb181-8" data-line-number="8">logi_sim_exp2 &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>(<span class="dt">mode =</span> <span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb181-9" data-line-number="9"><span class="st">  </span><span class="kw">fit</span>(train_sim, <span class="dt">formula =</span> class <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">poly</span>(y, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb181-10" data-line-number="10">logi_sim_exp3 &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>(<span class="dt">mode =</span> <span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb181-11" data-line-number="11"><span class="st">  </span><span class="kw">fit</span>(train_sim, <span class="dt">formula =</span> class <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">poly</span>(y, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb181-12" data-line-number="12">logi_sim_exp4 &lt;-<span class="st"> </span><span class="kw">logistic_reg</span>(<span class="dt">mode =</span> <span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb181-13" data-line-number="13"><span class="st">  </span><span class="kw">fit</span>(train_sim, <span class="dt">formula =</span> class <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dv">4</span>) <span class="op">+</span><span class="st"> </span><span class="kw">poly</span>(y, <span class="dv">4</span>))</a></code></pre></div>
<p>Let’s get the test error rate for each model.</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb182-1" data-line-number="1"><span class="co"># </span><span class="al">TODO</span><span class="co"> loop</span></a>
<a class="sourceLine" id="cb182-2" data-line-number="2">fit &lt;-<span class="st"> </span>test_sim <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb182-3" data-line-number="3"><span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">predict</span>(logi_sim, <span class="dt">new_data =</span> .)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb182-4" data-line-number="4"><span class="st">  </span><span class="kw">metrics</span>(class, .pred_class)</a>
<a class="sourceLine" id="cb182-5" data-line-number="5">fit_exp2 &lt;-<span class="st"> </span>test_sim <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb182-6" data-line-number="6"><span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">predict</span>(logi_sim_exp2, <span class="dt">new_data =</span> .)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb182-7" data-line-number="7"><span class="st">  </span><span class="kw">metrics</span>(class, .pred_class)</a>
<a class="sourceLine" id="cb182-8" data-line-number="8">fit_exp3 &lt;-<span class="st"> </span>test_sim <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb182-9" data-line-number="9"><span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">predict</span>(logi_sim_exp3, <span class="dt">new_data =</span> .)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb182-10" data-line-number="10"><span class="st">  </span><span class="kw">metrics</span>(class, .pred_class)</a>
<a class="sourceLine" id="cb182-11" data-line-number="11">fit_exp4 &lt;-<span class="st"> </span>test_sim <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb182-12" data-line-number="12"><span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">predict</span>(logi_sim_exp4, <span class="dt">new_data =</span> .)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb182-13" data-line-number="13"><span class="st">  </span><span class="kw">metrics</span>(class, .pred_class)</a>
<a class="sourceLine" id="cb182-14" data-line-number="14"></a>
<a class="sourceLine" id="cb182-15" data-line-number="15"><span class="kw">bind_rows</span>(<span class="kw">list</span>(<span class="dt">lin =</span> fit, <span class="dt">quad =</span> fit_exp2, <span class="dt">cubic =</span> fit_exp3, <span class="dt">quadric =</span> fit_exp4), <span class="dt">.id =</span> <span class="st">&quot;id&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb182-16" data-line-number="16"><span class="st">  </span><span class="kw">filter</span>(.metric <span class="op">==</span><span class="st"> &quot;accuracy&quot;</span>)</a></code></pre></div>
<p>The <span class="math inline">\(k\)</span>-fold CV follows the true test error rate closer than the training error does. While it doesn’t perfectly estimate it, it gives a good idea of ideal model flexibility as well as a less biased estimate of the test error.</p>
</div>
</div>
<div id="the-bootstrap" class="section level2">
<h2><span class="header-section-number">5.2</span> The Bootstrap</h2>
<p>The bootstrap is used to quantify the uncertainty associated with a given estimator or statistical learning method. An example of this would be estimating the standard errors of the coefficient of a linear regression fit. This isn’t the best example, since with inear regression, standard error is easy to achieve without the bootstrap. The value of the bootstrap lies in its ability to be applied to a wide range of statistical learning methods, particularly those methods in which a measure of variability is difficult to obtain.</p>
<p>Suppose we wish to determine the best investment allocation under a simple model. Let’s say we have to financial assets, that yield returns of random quantities <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We will invest a fraction of our money <span class="math inline">\(\alpha\)</span> in <span class="math inline">\(X\)</span>, and the remaining <span class="math inline">\(1 - \alpha\)</span> in <span class="math inline">\(Y\)</span>. We wish to choose <span class="math inline">\(\alpha\)</span> to minimize the total risk, or variance, of our investment. In other words, we want to minimize <span class="math inline">\(Var(\alpha X + (1 - \alpha)Y)\)</span>.</p>
<p>One can show that the value that minimizes risk is</p>
<p><span class="math display">\[
\alpha=\frac{\sigma_{Y}^{2}-\sigma_{X Y}}{\sigma_{X}^{2}+\sigma_{Y}^{2}-2 \sigma_{X Y}}
\]</span></p>
<p>In reality, these quantities are unknown. Let’s imagine we have access to a dataset containing pairs of previous returns of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We can use these previous returns to estimate these variance quantities.</p>
<p>Let’s simulate the data and see how our estimation performs.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb183-1" data-line-number="1">samples &lt;-<span class="st"> </span><span class="dv">400</span></a>
<a class="sourceLine" id="cb183-2" data-line-number="2">var_x &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb183-3" data-line-number="3">var_y &lt;-<span class="st"> </span><span class="fl">1.25</span></a>
<a class="sourceLine" id="cb183-4" data-line-number="4">cov_xy &lt;-<span class="st"> </span><span class="fl">0.5</span></a>
<a class="sourceLine" id="cb183-5" data-line-number="5">alpha &lt;-<span class="st"> </span>(var_y <span class="op">-</span><span class="st"> </span>cov_xy) <span class="op">/</span></a>
<a class="sourceLine" id="cb183-6" data-line-number="6"><span class="st">              </span>(var_x <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>var_y <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>cov_xy)</a>
<a class="sourceLine" id="cb183-7" data-line-number="7">data &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(</a>
<a class="sourceLine" id="cb183-8" data-line-number="8">  <span class="dt">n =</span> samples,</a>
<a class="sourceLine" id="cb183-9" data-line-number="9">  <span class="dt">mu =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>),</a>
<a class="sourceLine" id="cb183-10" data-line-number="10">  <span class="dt">Sigma =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(var_x, cov_xy, cov_xy, var_y), <span class="dt">nrow =</span> <span class="dv">2</span>),</a>
<a class="sourceLine" id="cb183-11" data-line-number="11">  <span class="dt">empirical =</span> <span class="ot">TRUE</span></a>
<a class="sourceLine" id="cb183-12" data-line-number="12">)</a>
<a class="sourceLine" id="cb183-13" data-line-number="13"></a>
<a class="sourceLine" id="cb183-14" data-line-number="14">x_return &lt;-<span class="st"> </span>data[, <span class="dv">1</span>]</a>
<a class="sourceLine" id="cb183-15" data-line-number="15">y_return &lt;-<span class="st"> </span>data[, <span class="dv">2</span>] </a>
<a class="sourceLine" id="cb183-16" data-line-number="16">returns &lt;-<span class="st"> </span><span class="kw">tibble</span>(x_return, y_return)</a>
<a class="sourceLine" id="cb183-17" data-line-number="17">returns &lt;-<span class="st"> </span>returns <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb183-18" data-line-number="18"><span class="st">  </span><span class="co"># random four bins</span></a>
<a class="sourceLine" id="cb183-19" data-line-number="19"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">sample =</span> <span class="kw">runif</span>(<span class="dt">n =</span> <span class="kw">nrow</span>(returns)),</a>
<a class="sourceLine" id="cb183-20" data-line-number="20">         <span class="dt">bin =</span> <span class="kw">ntile</span>(sample, <span class="dv">4</span>)) </a></code></pre></div>
<p>We created four simulated sets of <code>100</code> pairs of returns for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Let’s estimate <span class="math inline">\(\alpha\)</span> for each set and see how it compares to the true value.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb184-1" data-line-number="1"><span class="kw">ggplot</span>(returns, <span class="kw">aes</span>(<span class="dt">x =</span> x_return, <span class="dt">y =</span> y_return)) <span class="op">+</span></a>
<a class="sourceLine" id="cb184-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb184-3" data-line-number="3"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>bin)</a></code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-124-1.png" width="576" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb185-1" data-line-number="1"><span class="co"># get alpha estimate for each set</span></a>
<a class="sourceLine" id="cb185-2" data-line-number="2"><span class="co"># calculate var(x), var(y), and cov(x, y) for each bin</span></a>
<a class="sourceLine" id="cb185-3" data-line-number="3">returns <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb185-4" data-line-number="4"><span class="st">  </span><span class="kw">group_by</span>(bin) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb185-5" data-line-number="5"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">var_x =</span> <span class="kw">var</span>(x_return),</a>
<a class="sourceLine" id="cb185-6" data-line-number="6">            <span class="dt">var_y =</span> <span class="kw">var</span>(y_return),</a>
<a class="sourceLine" id="cb185-7" data-line-number="7">            <span class="dt">cov_xy =</span> <span class="kw">cov</span>(x_return, y_return),</a>
<a class="sourceLine" id="cb185-8" data-line-number="8">            <span class="dt">alpha =</span> (var_y <span class="op">-</span><span class="st"> </span>cov_xy) <span class="op">/</span></a>
<a class="sourceLine" id="cb185-9" data-line-number="9"><span class="st">              </span>(var_x <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>var_y <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>cov_xy))</a></code></pre></div>
<pre><code>## # A tibble: 4 x 5
##     bin var_x var_y cov_xy alpha
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1     1 0.878  1.60  0.416 0.474
## 2     2 0.863  1.13  0.469 0.611
## 3     3 1.07   1.20  0.589 0.434
## 4     4 1.19   1.09  0.523 0.362</code></pre>
<p>Our <span class="math inline">\(\alpha\)</span> estimates vary arround the true value of 0.48.</p>
<p>Let’s simulate <code>100</code> paired observations <code>1000</code> times and see how our estimate for <span class="math inline">\(\alpha\)</span> converges.</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb187-1" data-line-number="1"><span class="co"># </span><span class="al">TODO</span><span class="co"> increase to 1000, simulate, compare</span></a></code></pre></div>
<p>This gives us a very good idea of the accuracy of <span class="math inline">\(\hat\alpha\)</span>. So, for a random sample of the population, we would expect <span class="math inline">\(\hat\alpha\)</span> to differ from <span class="math inline">\(alpha\)</span> by approximately the standard error.</p>
<p>In reality, we often cannot generate new samples from the population. The boostrap approach allows us to use a computer to emulate the process of obtaining new samples so that we can estimate the variability of <span class="math inline">\(\hat\alpha\)</span> without generating new samples. Rather than generating new independent samples, we obtain distinct data sets through repeatedly sampling observations from the original data set.</p>
<p>The bootstrap approach is as follows. Imagine we have a data set <span class="math inline">\(Z\)</span> which contains <span class="math inline">\(n = 3\)</span> observations. We randomly select <span class="math inline">\(n\)</span> observations from the data set, with replacement, to produce a bootstrap data set, <span class="math inline">\(Z^{*1}\)</span>. We can use <span class="math inline">\(Z^{*1}\)</span> to produce a new boostrap estimate for <span class="math inline">\(\alpha\)</span>, which we call <span class="math inline">\(\hat\alpha^{*1}\)</span>. This procedure is repeated <span class="math inline">\(B\)</span> times for some large value of <span class="math inline">\(B\)</span>, in order to produce <span class="math inline">\(B\)</span> different bootstrap data sets, <span class="math inline">\(Z^{*1}, Z^{*2},..., Z^{*B}\)</span>, and <span class="math inline">\(B\)</span> corresponding <span class="math inline">\(\alpha\)</span> estimates, <span class="math inline">\(\hat\alpha^{*1}, \hat\alpha^{*2}, ..., \hat\alpha^{*B}\)</span>.</p>
<p>We can compute the standard error of these bootstrap estimates using the formula</p>
<p><span class="math display">\[
\operatorname{SE}_{B}(\hat{\alpha})=\sqrt{\frac{1}{B-1} \sum_{r=1}^{B}\left(\hat{\alpha}^{* r}-\frac{1}{B} \sum_{r^{\prime}=1}^{B} \hat{\alpha}^{* r^{\prime}}\right)^{2}}
\]</span></p>
<p>This serves as an estimate of the standard error of <span class="math inline">\(\alpha\)</span> estimated from the original dataset.</p>
<p>Let’s use the boostrap approach to generate <code>1,000</code> distinct samples from a single data set, and compare it to our simulated approach.</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb188-1" data-line-number="1"><span class="co"># </span><span class="al">TODO</span><span class="co"> tidymodels boostrap, simulate, histogram</span></a></code></pre></div>
<p>Note how close the distribution of the bootstrap estimates are to the simulated data sets.</p>
<p>The bootstrap approach can be used to effectively estimate the variability associated with <span class="math inline">\(\hat\alpha\)</span>.</p>
</div>
<div id="lab" class="section level2">
<h2><span class="header-section-number">5.3</span> Lab</h2>
<div id="the-validation-set-approach-1" class="section level3">
<h3><span class="header-section-number">5.3.1</span> The Validation Set Approach</h3>
</div>
<div id="loocv" class="section level3">
<h3><span class="header-section-number">5.3.2</span> LOOCV</h3>
</div>
<div id="k-fold-cross-validation-1" class="section level3">
<h3><span class="header-section-number">5.3.3</span> <span class="math inline">\(k\)</span>-Fold Cross-Validation</h3>
</div>
<div id="the-bootstrap-1" class="section level3">
<h3><span class="header-section-number">5.3.4</span> The Bootstrap</h3>
</div>
</div>
<div id="exercises-3" class="section level2">
<h2><span class="header-section-number">5.4</span> Exercises</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["tidy_islr.pdf", "tidy_islr.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
