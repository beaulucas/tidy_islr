\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={A Tidy Introduction To Statistical Learning},
            pdfauthor={Beau Lucas},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{A Tidy Introduction To Statistical Learning}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Beau Lucas}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{2017-11-17}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

This book will serve as a source of notes and exercise solutions for
\emph{An Introduction to Statistical Learning}. My approach will be
centered around the \texttt{tidyverse}. This is not a replacement for
the book, which should be read front to back by all machine learning
enthusiasts.

Chapter names will line up, and certain subheadings will also match.
Sometimes my notes will contain text lifted straight from the book
without modification. This is not an attempt to plagiarize or claim
their writing as my own. My goal is for this bookdown project to be a
quick stop for machine learning enthusiasts to reference high-level
ideas from ISLR in a modern media format.

\chapter{Introduction}\label{intro}

\section{An Overview of Statistical
Learning}\label{an-overview-of-statistical-learning}

Statistical learning is focused on supervised and unsupervised modeling
and prediction.

\section{Data Sets Used in Labs and
Exercises}\label{data-sets-used-in-labs-and-exercises}

All data sets used in this book can be found in \texttt{ISLR} and
\texttt{MASS} packages, with some also being found in the base
\texttt{R} distribution.

We will utilize the \texttt{tidyverse} ecosystem to tackle the exercises
and labs, as the \texttt{R} code found in the original textbook is
outdated.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(ISLR)}
\KeywordTok{require}\NormalTok{(MASS)}
\KeywordTok{require}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\section{Book Website}\label{book-website}

The website and free PDF for the book can be found here:

\href{http://www-bcf.usc.edu/~gareth/ISL/}{www.statlearning.com}
\href{https://www.youtube.com/channel/UC4OWDcPB1peiBXDfCSZ3h-w}{Youtube
- Statistical Learning}

\chapter{Statistical Learning}\label{statistical-learning}

\section{What is Statistical
Learning?}\label{what-is-statistical-learning}

Methods to estimate functions that connect inputs to outputs.

If there exists a quantitative response variable \(Y\) and \(p\)
different predictors (\(X_1\), \(X_2\), \ldots{}, \(X_p\)), we can write
this relationship as:

\(Y = f(X) + ε\)

\subsection{\texorpdfstring{Why Estimate
\emph{\(f\)}?}{Why Estimate f?}}\label{why-estimate-f}

\subsubsection{Prediction}\label{prediction}

We can predict Y using:

\(\hat{Y} = \hat{f}(X)\)

Accuracy of \(Y\) is dependant on:

\begin{itemize}
\tightlist
\item
  \emph{reducible error}

  \begin{itemize}
  \tightlist
  \item
    \(\hat{f}\) will never be perfect estimate of \(f\), and model can
    always be potentially improved
  \item
    Even if \(\hat{f} = f\), prediction would still have some error
  \end{itemize}
\item
  \emph{irreducible error}

  \begin{itemize}
  \tightlist
  \item
    Because \(Y\) is also a function of random \(ε\), there will alwayas
    be variability
  \item
    We cannot reduce the error introduced by \(ε\)
  \end{itemize}
\end{itemize}

\subsubsection{Inference}\label{inference}

How does \(Y\) respond to changes in \(X_1, X_2, ..., X_p\)?

\subsection{\texorpdfstring{How do we estimate
\emph{\(f\)}?}{How do we estimate f?}}\label{how-do-we-estimate-f}

\begin{itemize}
\tightlist
\item
  Use \emph{training data} to train method
\item
  \(x_ij\) is value of \(j\)th predictor for observation \(i\), \(y_i\)
  is value of response variable

  \begin{itemize}
  \tightlist
  \item
    \(i = 1, 2, ..., n\), \(j = 1, 2, ..., p\)
  \end{itemize}
\item
  Using training data, apply statistical learning method estimate
  unknown function \(f\)
\item
  Most statistical learning methods can be characterized as either
  \emph{parametric} or \emph{non-parametric}
\end{itemize}

\subsubsection{Parametric Methods}\label{parametric-methods}

Two-step model-based approach:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Make an assumption about functional form of \(f\), such as ``\(f\) is
  linear in \(X\)''
\item
  Perform procedure that uses training data to train the model

  \begin{itemize}
  \tightlist
  \item
    In case of linear model, this procedure estimates parameters
    \(β_0, β_1, ..., β_p\)
  \item
    Most common approach to fit linear model is \emph{(ordinary) least
    squares}
  \end{itemize}
\end{enumerate}

This is \emph{parametric}, as it reduces the problem of estimating \(f\)
down to one of estimating a set of parameters. Problems that can arise:
- Model will not match the true unknown form of \(f\) - If model is made
more \emph{flexible}, which generally requires estimating a greater
number of parameters, \emph{overfitting} can occur

\subsubsection{Non-parametric Methods}\label{non-parametric-methods}

Non-parametric methods do not make assumptions about the form of \(f\).
An advantage of this is that they have the potential to fit a wider
range of possible shapes for \(f\). A disadvantage is that, because
there are no assumptions about the form of \(f\), the problem of
estimating \(f\) is not reduced to a set number of parameters. This
means more observations are needed compared to a parametric approach to
estimate \(f\) accurately.

\subsection{The Trade-Off Between Prediction Accuracy and Model
Interpretability}\label{the-trade-off-between-prediction-accuracy-and-model-interpretability}

Restrictive models are much more intepretable than flexible ones.
Flexible approaches can be so complicated that it is hard to understand
how predictors affect the response.

If inference is the goal, simple and inflexible methods are easier to
interpret. For prediction, accuracy is the biggest concern. However,
flexible models are more prone to overfitting.

\subsection{Supervised Versus Unsupervised
Learning}\label{supervised-versus-unsupervised-learning}


\end{document}
