<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Linear Regression | A Tidy Introduction To Statistical Learning</title>
  <meta name="description" content="Chapter 3 Linear Regression | A Tidy Introduction To Statistical Learning" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Linear Regression | A Tidy Introduction To Statistical Learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Linear Regression | A Tidy Introduction To Statistical Learning" />
  
  
  

<meta name="author" content="Beau Lucas" />


<meta name="date" content="2019-12-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="statistical-learning.html">
<link rel="next" href="classification.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Tidy Introduction To Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#an-overview-of-statistical-learning"><i class="fa fa-check"></i><b>1.1</b> An Overview of Statistical Learning</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#data-sets-used-in-labs-and-exercises"><i class="fa fa-check"></i><b>1.2</b> Data Sets Used in Labs and Exercises</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#book-resources"><i class="fa fa-check"></i><b>1.3</b> Book Resources:</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-learning.html"><a href="statistical-learning.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#packages-used-in-this-chapter"><i class="fa fa-check"></i><b>2.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#what-is-statistical-learning"><i class="fa fa-check"></i><b>2.2</b> What is Statistical Learning?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#why-estimate-f"><i class="fa fa-check"></i><b>2.2.1</b> Why Estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#how-do-we-estimate-f"><i class="fa fa-check"></i><b>2.2.2</b> How do we estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-trade-off-between-prediction-accuracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.2.3</b> The Trade-Off Between Prediction Accuracy and Model Interpretability</a></li>
<li class="chapter" data-level="2.2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#supervised-versus-unsupervised-learning"><i class="fa fa-check"></i><b>2.2.4</b> Supervised Versus Unsupervised Learning</a></li>
<li class="chapter" data-level="2.2.5" data-path="statistical-learning.html"><a href="statistical-learning.html#regression-versus-classification-problems"><i class="fa fa-check"></i><b>2.2.5</b> Regression Versus Classification Problems</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.3</b> Assessing Model Accuracy</a><ul>
<li class="chapter" data-level="2.3.1" data-path="statistical-learning.html"><a href="statistical-learning.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.3.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.3.2" data-path="statistical-learning.html"><a href="statistical-learning.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>2.3.2</b> The Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="2.3.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-classification-setting"><i class="fa fa-check"></i><b>2.3.3</b> The Classification Setting</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#lab-introduction-to-r"><i class="fa fa-check"></i><b>2.4</b> Lab: Introduction to R</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-learning.html"><a href="statistical-learning.html#exercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#packages-used-in-this-chapter-1"><i class="fa fa-check"></i><b>3.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>3.2.1</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimate"><i class="fa fa-check"></i><b>3.2.2</b> Assessing the Accuracy of the Coefficient Estimate</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-model"><i class="fa fa-check"></i><b>3.2.3</b> Assessing the Accuracy of the Model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>3.3.1</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-regression.html"><a href="linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>3.3.2</b> Some Important Questions</a></li>
<li class="chapter" data-level="3.3.3" data-path="linear-regression.html"><a href="linear-regression.html#other-considerations-in-the-regression-model"><i class="fa fa-check"></i><b>3.3.3</b> Other Considerations in the Regression Model</a></li>
<li class="chapter" data-level="3.3.4" data-path="linear-regression.html"><a href="linear-regression.html#qualitative-predictors-with-more-than-two-levels"><i class="fa fa-check"></i><b>3.3.4</b> Qualitative Predictors with More than Two Levels</a></li>
<li class="chapter" data-level="3.3.5" data-path="linear-regression.html"><a href="linear-regression.html#extensions-of-the-linear-model"><i class="fa fa-check"></i><b>3.3.5</b> Extensions of the Linear Model</a></li>
<li class="chapter" data-level="3.3.6" data-path="linear-regression.html"><a href="linear-regression.html#potential-problems"><i class="fa fa-check"></i><b>3.3.6</b> Potential Problems</a></li>
<li class="chapter" data-level="3.3.7" data-path="linear-regression.html"><a href="linear-regression.html#the-marketing-plan"><i class="fa fa-check"></i><b>3.3.7</b> The Marketing Plan</a></li>
<li class="chapter" data-level="3.3.8" data-path="linear-regression.html"><a href="linear-regression.html#comparison-of-linear-regression-with-k-nearest-neighbors"><i class="fa fa-check"></i><b>3.3.8</b> Comparison of Linear Regression with <em>K</em>-Nearest Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-regression.html"><a href="linear-regression.html#lab-linear-regression"><i class="fa fa-check"></i><b>3.4</b> Lab: Linear Regression</a><ul>
<li class="chapter" data-level="3.4.1" data-path="linear-regression.html"><a href="linear-regression.html#fitting-a-linear-regression"><i class="fa fa-check"></i><b>3.4.1</b> Fitting a linear regression</a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression-1"><i class="fa fa-check"></i><b>3.4.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="3.4.3" data-path="linear-regression.html"><a href="linear-regression.html#interaction-terms"><i class="fa fa-check"></i><b>3.4.3</b> Interaction Terms</a></li>
<li class="chapter" data-level="3.4.4" data-path="linear-regression.html"><a href="linear-regression.html#non-linear-transformations-of-the-predictors"><i class="fa fa-check"></i><b>3.4.4</b> Non-linear Transformations of the Predictors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="linear-regression.html"><a href="linear-regression.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>4</b> Classification</a><ul>
<li class="chapter" data-level="4.1" data-path="classification.html"><a href="classification.html#packages-used-in-this-chapter-2"><i class="fa fa-check"></i><b>4.1</b> Packages used in this chapter</a></li>
<li class="chapter" data-level="4.2" data-path="classification.html"><a href="classification.html#an-overview-of-classification"><i class="fa fa-check"></i><b>4.2</b> An Overview of Classification</a></li>
<li class="chapter" data-level="4.3" data-path="classification.html"><a href="classification.html#why-not-linear-regression"><i class="fa fa-check"></i><b>4.3</b> Why Not Linear Regression?</a></li>
<li class="chapter" data-level="4.4" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>4.4</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.4.1" data-path="classification.html"><a href="classification.html#the-logistic-model"><i class="fa fa-check"></i><b>4.4.1</b> The Logistic Model</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification.html"><a href="classification.html#estimating-the-regression-coefficients-1"><i class="fa fa-check"></i><b>4.4.2</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="4.4.3" data-path="classification.html"><a href="classification.html#making-predictions"><i class="fa fa-check"></i><b>4.4.3</b> Making Predictions</a></li>
<li class="chapter" data-level="4.4.4" data-path="classification.html"><a href="classification.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>4.4.4</b> Multiple Logistic Regression</a></li>
<li class="chapter" data-level="4.4.5" data-path="classification.html"><a href="classification.html#logistic-regression-for-2-response-classes"><i class="fa fa-check"></i><b>4.4.5</b> Logistic Regression for &gt;2 Response Classes</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>4.5</b> Linear Discriminant Analysis</a><ul>
<li class="chapter" data-level="4.5.1" data-path="classification.html"><a href="classification.html#using-bayes-theorem-for-classification"><i class="fa fa-check"></i><b>4.5.1</b> Using Bayes’ Theorem for Classification</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1"><i class="fa fa-check"></i><b>4.5.2</b> Linear Discriminant Analysis for p = 1</a></li>
<li class="chapter" data-level="4.5.3" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-for-p-1-1"><i class="fa fa-check"></i><b>4.5.3</b> Linear Discriminant Analysis for p &gt; 1</a></li>
<li class="chapter" data-level="4.5.4" data-path="classification.html"><a href="classification.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>4.5.4</b> Quadratic Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="classification.html"><a href="classification.html#a-comparison-of-classification-methods"><i class="fa fa-check"></i><b>4.6</b> A Comparison of Classification Methods</a></li>
<li class="chapter" data-level="4.7" data-path="classification.html"><a href="classification.html#lab-logistic-regression-lda-qda-and-knn"><i class="fa fa-check"></i><b>4.7</b> Lab: Logistic Regression, LDA, QDA, and KNN</a><ul>
<li class="chapter" data-level="4.7.1" data-path="classification.html"><a href="classification.html#the-stock-market-data"><i class="fa fa-check"></i><b>4.7.1</b> The Stock Market Data</a></li>
<li class="chapter" data-level="4.7.2" data-path="classification.html"><a href="classification.html#logistic-regression-1"><i class="fa fa-check"></i><b>4.7.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.7.3" data-path="classification.html"><a href="classification.html#linear-discriminant-analysis-1"><i class="fa fa-check"></i><b>4.7.3</b> Linear Discriminant Analysis</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/beaulucas/tidy_islr" target="blank">GitHub Repository</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tidy Introduction To Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Linear Regression</h1>
<hr />
<p>Linear regression is a simple yet very powerful approach in statistical learning. It is important to have a strong understanding of it before moving on to more complex learning methods.</p>
<div id="packages-used-in-this-chapter-1" class="section level2">
<h2><span class="header-section-number">3.1</span> Packages used in this chapter</h2>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(modelr)
<span class="kw">library</span>(knitr)
<span class="kw">library</span>(kableExtra)</code></pre>
</div>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">3.2</span> Simple Linear Regression</h2>
<p>Simple linear regression is predicting a quantitative response <span class="math inline">\(Y\)</span> based off a single predcitor <span class="math inline">\(X\)</span>.</p>
<p>It can be written as below:</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Y \approx \beta_0 + \beta_1X\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>simple linear regression</em>
</p>
</div>
<p><span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> represent the <em>intercept</em> and <em>slope</em> terms and are together known as the <em>coefficients</em>.
<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> represent the unknown <em>intercept</em> and <em>slope</em> terms and are together known as the <em>coefficients</em>. We will use our training data to estimate these parameters and thus estimate the response <span class="math inline">\(Y\)</span> based on the value of <span class="math inline">\(X = x\)</span>:</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat y = \hat\beta_0 + \hat\beta_1x\)</span>
</p>
</div>
<div id="estimating-the-coefficients" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Estimating the Coefficients</h3>
<p>We need to use data to estimate these coefficients.</p>
<div>
<p style="text-align:center">
<span class="math inline">\((x_1,y_1), (x_2,y_2),..., (x_n,y_n)\)</span>
</p>
</div>
<p>These represent the training observations, in this case pairs of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> measurements. The goal is to use these measurements to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that the linear model fits our data as close as possible. Measuring <em>closeness</em> can be tackled a number of ways, but <a href="https://en.wikipedia.org/wiki/Least_squares">least squares</a> is the most popular.</p>
<p>If we let <span class="math inline">\(\hat y_i = \hat\beta_0 + \hat\beta_1x_i\)</span> be the prediction of <span class="math inline">\(Y\)</span> at observation <span class="math inline">\(X_i\)</span>, then <span class="math inline">\(e_i = y_i - \hat y_i\)</span> represents the <span class="math inline">\(i\)</span>th <em>residual</em>, the difference between the observed value <span class="math inline">\(y_i\)</span> and the predicted value <span class="math inline">\(\hat y_i\)</span>. Now we can define the <em>residual sum of squares (RSS)</em> as</p>
<div>
<p style="text-align:center">
<span class="math inline">\(RSS = e_1^2 + e_2^2 + ... + e_n^2\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>residual sum of squares</em>
</p>
</div>
<p>or more explicitly as</p>
<div>
<p style="text-align:center">
<span class="math inline">\(RSS = (y_1 - \hat\beta_0 - \hat\beta_1x_2)^2 + (y_2 - \hat\beta_0 - \hat\beta_1x_2)^2 + ... + (y_n - \hat\beta_0 - \hat\beta_1x_n)^2\)</span>
</p>
</div>
<p>Minimizing the RSS (proof can be found <a href="https://en.m.wikipedia.org/wiki/Simple_linear_regression#Derivation_of_simple_regression_estimators">here</a>) using <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> produces:</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\frac{\displaystyle \sum_{i=1}^{n}(x_i-\bar x)(y_i - \bar x)}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2}\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>least squares coefficient estimates (simple linear regression)</em>
</p>
</div>
</div>
<div id="assessing-the-accuracy-of-the-coefficient-estimate" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Assessing the Accuracy of the Coefficient Estimate</h3>
<p>Remember that the true function for <span class="math inline">\(f\)</span> contains a random error term <span class="math inline">\(\epsilon\)</span>. This means the linear relationship can be written as</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Y = \beta_0 + \beta_1X + \epsilon\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>population regression line</em>
</p>
</div>
<p><span class="math inline">\(\beta_0\)</span> is the intercept term (value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X = 0\)</span>). <span class="math inline">\(\beta_1\)</span> is the slope (how much does <span class="math inline">\(Y\)</span> change with one-unit change of <span class="math inline">\(X\)</span>). <span class="math inline">\(\epsilon\)</span> is the error term that captures everything our model doesn’t (unknown variables, measurement error, unknown true relationship).</p>
<p>The population regression line captures the best linear approximation to the true relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In real data, we often don’t know the true relationship and have to rely on a set of observations. Using the observations to estimate the coefficients via least squares produces the <em>least squares line</em>. Let’s simulate and visualize this relationship:</p>
<ul>
<li>simulate <code>n = 200</code> observations</li>
<li>compare the population regression line (<code>sim_y</code>) to a number of possible least squares lines (generated from 10 different training sets of the data)</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># f(x), or Y = 2 + 2x + error</span>

sim_linear &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">b0 =</span> <span class="dv">2</span>,
  <span class="dt">b1 =</span> <span class="dv">2</span>,
  <span class="dt">x =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">200</span>, <span class="dt">mean =</span> <span class="dv">100</span>, <span class="dt">sd =</span> <span class="dv">15</span>),
  <span class="dt">err =</span> <span class="kw">rnorm</span>(<span class="dv">200</span>, <span class="dt">sd =</span> <span class="dv">50</span>),
  <span class="dt">sim_y =</span> b0 <span class="op">+</span><span class="st"> </span>b1 <span class="op">*</span><span class="st"> </span>x,
  <span class="dt">true_y =</span> b0 <span class="op">+</span><span class="st"> </span>b1 <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>err
)

<span class="co"># generate 10 training sets</span>
y &lt;-<span class="st"> </span><span class="kw">tibble</span>()
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) {
  x &lt;-<span class="st"> </span><span class="kw">sample_frac</span>(sim_linear, <span class="fl">0.1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">iter_set =</span> i)
  y &lt;-<span class="st"> </span>y <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_rows</span>(x)
}

<span class="co"># apply linear model to each sample</span>
by_iter &lt;-<span class="st"> </span>y <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(iter_set) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">nest</span>()
lm_model &lt;-<span class="st"> </span><span class="cf">function</span>(df) {
  <span class="kw">lm</span>(true_y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> df)
}
by_iter &lt;-<span class="st"> </span>by_iter <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">model =</span> <span class="kw">map</span>(data, lm_model),
    <span class="dt">preds =</span> <span class="kw">map2</span>(data, model, add_predictions)
  )

<span class="co"># extract predictions</span>
preds &lt;-<span class="st"> </span><span class="kw">unnest</span>(by_iter, preds)

<span class="kw">ggplot</span>(<span class="dt">data =</span> sim_linear, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> true_y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> preds, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> pred, <span class="dt">colour =</span> iter_set, <span class="dt">group =</span> iter_set), <span class="dt">linetype =</span> <span class="st">&quot;F1&quot;</span>, <span class="dt">size =</span> <span class="fl">.75</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> sim_y), <span class="dt">colour =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">size =</span> <span class="fl">1.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(
    <span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>, <span class="dt">panel.grid.minor =</span> <span class="kw">element_blank</span>(),
    <span class="dt">panel.grid.major =</span> <span class="kw">element_blank</span>(), <span class="dt">axis.line =</span> <span class="kw">element_line</span>(<span class="dt">colour =</span> <span class="st">&quot;grey92&quot;</span>)
  ) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">title =</span> <span class="st">&quot;Each least squares line provides a reasonable estimate&quot;</span>,
    <span class="dt">y =</span> <span class="st">&quot;y&quot;</span>
  )</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-11-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The chart above demonstrates the population regression line (red) surrounded by ten different estimates of the least squares line. Notice how every least squares line (shades of blue) is different. This is because each one is generated from a random sample pulled from the simulated data. For a real-world comparison, the simulated data would be the entire population data which is often impossible to obtain. The observations used to generate the least squares line would be the sample data we have access to. In the same way a sample mean can provide a reasonable estimate of the population mean, fitting a least squares line can provide a reasonable estimate of the population regression line.</p>
<p>This comparison of linear regression to estimating population means touches on the topic of bias. An estimate of <span class="math inline">\(\mu\)</span> using the the sample mean <span class="math inline">\(\hat\mu\)</span> is unbiased. On average, the sample mean will not systemically over or underestimate <span class="math inline">\(\mu\)</span>. If we were to take a large enough estimates of <span class="math inline">\(\mu\)</span>, each produced by a particular set of observations, then this average would exactly equal <span class="math inline">\(\mu\)</span>. This concept applies to our estimates of <span class="math inline">\(\beta_0, \beta_1\)</span> as well.</p>
<p>A question that can be asked is how close on average the sample mean <span class="math inline">\(\hat\mu\)</span> is to <span class="math inline">\(\mu\)</span>. We can compute the <em>standard error</em> of <span class="math inline">\(\hat\mu\)</span> to answer this.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Var(\hat\mu) = SE(\hat\mu)^2 = \sigma^2/n\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>standard error</em>
</p>
</div>
<p>This formula measures the average amount that <span class="math inline">\(\hat\mu\)</span> differs from <span class="math inline">\(\mu\)</span>. As the number of observations <span class="math inline">\(n\)</span> increases, the standard error decreases.</p>
<p>We can also use this to calculate how close <span class="math inline">\(\hat\beta_0, \hat\beta_1\)</span> are to <span class="math inline">\(\beta_0, \beta_1\)</span>.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(SE(\hat\beta_0)^2= \sigma^2 \left[1/n + \frac{\displaystyle \bar x^2}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2} \right]\)</span>
</p>
</div>
<div>
<p style="text-align:center">
<span class="math inline">\(SE(\hat\beta_1)^2=\frac{\displaystyle \sigma^2}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2}\)</span>
</p>
</div>
<p>where <span class="math inline">\(\sigma^2 = Var(\epsilon)\)</span>. For this to work, the assumption has to be made that the error terms <span class="math inline">\(\epsilon_i\)</span> are uncorrelated and all share a common variance. This is often not the case, but it doesn’t mean the formula can’t be used for a decent approximation. <span class="math inline">\(\sigma^2\)</span> is not known, but can be estimated from training observations. This estimate is the <em>residual standard error</em> and is given by formula <span class="math inline">\(RSE = \sqrt{RSS/(n-2}\)</span>.</p>
<p>What can we use these standard error formulas for? A useful technique is to calculate <em>confidence intervals</em> from the standard error. If we wanted to compute a 95% confidence interval for <span class="math inline">\(\beta_0,\beta_1\)</span>, it would take the form below.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat\beta_1 \pm 2 * SE(\hat\beta_1)\)</span>
</p>
</div>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat\beta_0 \pm 2 * SE(\hat\beta_0)\)</span>
</p>
</div>
<p>Standard errors can also be used to perform hypotheses tests.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(H_0\)</span>: There is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, or <span class="math inline">\(\beta_1 = 0\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>null hypothesis</em>
</p>
</div>
<div>
<p style="text-align:center">
<span class="math inline">\(H_0\)</span>: There exists a relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, or <span class="math inline">\(\beta_1 \neq 0\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>alternative hypothesis</em>
</p>
</div>
<p>To test the null hypothesis, we need to test whether <span class="math inline">\(\hat\beta_1\)</span> is far enough away from zero to conclude that is it non-zero. How far enough from zero is determined by the value of <span class="math inline">\(\hat\beta_1\)</span> as well as <span class="math inline">\(SE(\hat\beta_1)\)</span>. We compute a <em>t-statistic</em></p>
<div>
<p style="text-align:center">
<span class="math inline">\(t = (\beta_1 - 0)/SE(\hat\beta_1)\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>t-statistic</em>
</p>
</div>
<p>This measures how many standard deviations <span class="math inline">\(\hat\beta_1\)</span> is from 0. If there is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, then <span class="math inline">\(t\)</span> will follow a t-distribution. The t-distribution is similar to the normal distribution, but has slightly heavier tails. Like the normal distribution, we can use this to compute the probability of observing any number equal to or larger than <span class="math inline">\(|t|\)</span>. This probability is the <em>p-value</em>. We can interpret a p-value as the probability we would observe the sample data that produced the <span class="math inline">\(t\)</span>-statistic, given that there is no actual relationship between the predictor <span class="math inline">\(X\)</span> and the response <span class="math inline">\(Y\)</span>. This means that a small p-value supports the inference that there exists a relationship between the predictor and the response. In this case, based on whichever threshold <span class="math inline">\(\alpha\)</span> (common value is 0.05) we set, a small enough p-value would lead us to reject the null hypothesis.</p>
</div>
<div id="assessing-the-accuracy-of-the-model" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Assessing the Accuracy of the Model</h3>
<p>Now that we determined the existence of a relationship, how can we measure how well the model fits the data?</p>
<p>Measuring the quality of a linear regression fit is often handled by two quantities: the <em>residual standard error</em> and the <em>R^2</em> statistic.</p>
<div id="residual-standard-error" class="section level4">
<h4><span class="header-section-number">3.2.3.1</span> Residual Standard Error</h4>
<p>Since every observation has an associated error term <span class="math inline">\(\epsilon\)</span>, having the knowledge of true <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> will still not allow one to perfectly predict <span class="math inline">\(Y\)</span>. The residual standard error estimates the standard deviation of the error term.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(RSE = \sqrt{1/(n-2)*RSS} = \sqrt{1/(n-2)\sum_{i=1}^{n}(y_i - \hat y)^2}\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>residual standard error</em>
</p>
</div>
<p>We can interpret the residual standard error as how much, on average, our predictions deviate from the true value. Whether the value is acceptable in terms of being a successful model depends on the context of the problem. Predicting hardware failure on an airplane would obviously carry much more stringent requirements than predicting the added sales from a change in a company’s advertising budget.</p>
</div>
<div id="r2-statistic" class="section level4">
<h4><span class="header-section-number">3.2.3.2</span> R^2 statistic</h4>
<p>The RSE provides an absolute number. Given that it depends on the scale of <span class="math inline">\(Y\)</span>, comparing RSE values across different domains and datasets isn’t useful. The R^2 statistic solves this problem by measuring in terms of proportion – it measures the variance explained and so always takes a value between 0 and 1.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(R^2 = (TSS - RSS)/TSS = 1 - RSS/TSS\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>R^2 statistic</em>
</p>
</div>
<p>where <span class="math inline">\(TSS = \sum_{i=1}^{n}(y_i-\bar y)^2\)</span> is the <em>total sum of squares</em>. TSS can be thought of the amount of total variability in the response variable before any model is fitted to it. RSS is measured after fitting a model, and measures the amount of unexplained variance remaining in the data. Therefore, R^2 can be thought of as the proportion of variance in the data that is explained by fitting a model with <span class="math inline">\(X\)</span>. While R^2 is more intrepetable, determing what constitutes a R^2 is subjective to the problem. Relationships that are known to be linear with little variance would expect an R^2 very close to 1. In reality, a lot of real-world data is not truly linear and could be heavily influenced by unknown, immeasurable predictors. In such cases a linear approximation would be a rough fit, and a smaller R^2 would not be unordinary.</p>
<p>There is a relation between R^2 and the correlation.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(r = Cor(X,Y) = \sum_{i=1}^{n}((x_i-\bar x)(y_i - \bar y))/(\sqrt{\sum_{i=1}^{n}(x_i - \bar x)^2}\sqrt{\sum_{i=1}^{n}(y_i - \bar y)^2})\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>correlation</em>
</p>
</div>
<p>Both measure the linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and within the simple linear regression domain, <span class="math inline">\(r^2 = R^2\)</span>. Once we move into multiple linear regression, in which we are using multiple predictors to predict a response, correlation loses effectiveness at measuring a model in whole as it can only measure the relationship between a single pair of variables.</p>
</div>
</div>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2><span class="header-section-number">3.3</span> Multiple Linear Regression</h2>
<p>Simple linear regression works well when the data involves a single predictor variable. In reality, there are often multiple predictor variables. We will need to extend the simple linear regression model and provide each predictor variable <span class="math inline">\(p\)</span> with a slope coefficient.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>multiple linear regression</em>
</p>
</div>
<div id="estimating-the-regression-coefficients" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Estimating the Regression Coefficients</h3>
<p>Again, we need to estimate the regression coefficients.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat y = \hat\beta_0 + \hat\beta_1X_1 + \hat\beta_2X_2 + ... + \hat\beta_pX_p\)</span>
</p>
</div>
<p>We will utilize the same approach of minimizing the sum of squared residuals (RSS).</p>
<div>
<p style="text-align:center">
<span class="math inline">\(RSS = \sum_{i=1}^{n}(y_i - \hat y_i)^2 = \sum_{i=1}^{n}(y_i - \hat\beta_0 - \hat\beta_1x_{i1} - \hat\beta_2x_{i2} - ... - \hat\beta_px_{ip})^2\)</span>
</p>
</div>
<p>Minimizing these coefficients is more complicated than the simple linear regression setting, and is best represented using linear algebra. See <a href="https://en.wikipedia.org/wiki/Residual_sum_of_squares#Matrix_expression_for_the_OLS_residual_sum_of_squares">this Wikipedia section</a> for more information on the formula.</p>
<p>Interpreting a particular coefficient, (say <span class="math inline">\(\beta_1\)</span>) in a multiple regression model can be thought of as follows: if constant value for all other <span class="math inline">\(\beta_p\)</span> are maintained, what effect would an increase in <span class="math inline">\(beta_1\)</span> have on <span class="math inline">\(Y\)</span>?</p>
<p>A side effect of this is that certain predictors which were deemed significant when contained in a simple linear regression can become insignificant when multiple predictors are involved. For an advertising example, <code>newspaper</code> could be a significant predictor of <code>revenue</code> in the simple linear regression context. However, when combined with <code>tv</code> and <code>radio</code> in a multiple linear regression setting, the effects of increasing <code>newspaper</code> spend while maintaining <code>tv</code> and <code>radio</code> becomes insignificant. This could be due to a correlation of <code>newspaper</code> spend in markets where <code>radio</code> spend is high. Multiple linear regression exposes predictors that act as “surrogates” for others due to correlation.</p>
</div>
<div id="some-important-questions" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Some Important Questions</h3>
<div id="is-there-a-relationship-between-the-response-and-predictors" class="section level4">
<h4><span class="header-section-number">3.3.2.1</span> Is There a Relationship Between the Response and Predictors?</h4>
<p>To check this, we need to check whethere all <span class="math inline">\(p\)</span> coefficients are zero, i.e. <span class="math inline">\(\beta_1 = \beta_2 = ... = \beta_p = 0\)</span>. We test the null hypothesis,</p>
<div>
<p style="text-align:center">
<span class="math inline">\(H_o:\beta_1 = \beta_2 = ... = \beta_p = 0\)</span>
</p>
</div>
<p>against the alternative</p>
<div>
<p style="text-align:center">
<span class="math inline">\(H_a:\)</span> at least one <span class="math inline">\(\beta_j\)</span> is non-zero
</p>
</div>
<p>The hypothesis test is performed by computing the <span class="math inline">\(F-statistic\)</span>,</p>
<div>
<p style="text-align:center">
<span class="math inline">\(F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>correlation</em>
</p>
</div>
<p>If linear model assumptions are correct, one can show that</p>
<div>
<p style="text-align:center">
<span class="math inline">\(E\{RSS/(n-p-1)\} = \sigma^2\)</span>
</p>
</div>
<p>and that, provided <span class="math inline">\(H_o\)</span> is true,</p>
<div>
<p style="text-align:center">
<span class="math inline">\(E\{(TSS-RSS)/p\} = \sigma^2\)</span>
</p>
</div>
<p>In simple terms, if <span class="math inline">\(H_o\)</span> were true and all of the predictors have regression coefficients of 0, we would expect the unexplained variance of the model to be approximately equal to that of the total variance, and both the numerator and the denominator of the F-statistic formula to be equal. When there is no relationship between the response and predictors, the F-statistic will take on a value close to 1. However, as RSS shrinks (the model begins to account for more of the variance), the numerator grows and the denominator shrinks, both causing the F-statistic to increase. We can think of the F-statistic as a ratio between the explained variance and unexplained variance. As the explained variance grows larger than the unexplained portion, the likelihood that we reject the null hypothesis grows.</p>
<p>How large does the F-statistic need to be to reject the null hypothesis? This depends on <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>. As <span class="math inline">\(n\)</span> grows, F-statistics closer to 1 may provide sufficient evidence to reject <span class="math inline">\(H_o\)</span>. If <span class="math inline">\(H_o\)</span> is true and <span class="math inline">\(\epsilon_i\)</span> have a normal distribution, the F-statistic follows an F-distribution. We can compute the p-value for any value of <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> associated with an F-statistic.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> remove this section?</span></code></pre>
<p>Sometimes we want to test whether a particular subset of <span class="math inline">\(q\)</span> of the coefficients are zero.</p>
<p>The null hypothesis could be</p>
<div>
<p style="text-align:center">
<span class="math inline">\(H_o : \beta_{p-q+1} = \beta_{p-q+2} = \beta_p = 0\)</span>
</p>
</div>
<p>In this case we fit a second model that uses all the variables except the last <span class="math inline">\(q\)</span>. We will call the residual sum of squares for the second model <span class="math inline">\(RSS_0\)</span>.</p>
<p>Then, the F-statistic is,</p>
<div>
<p style="text-align:center">
<span class="math inline">\(F = \frac{(RSS_0 - RSS)/q}{RSS(n-p-1)}\)</span>
</p>
</div>
<p>We are testing a model without the <span class="math inline">\(q\)</span> predictors and seeing how it compares to the original model containing all the predictors.</p>
<p>Why do we need to look at overall F-statistics if we have individual p-values of the predictors? There are scenarios where individual predictors, by chance, will have <em>small</em> p-values, even in the absence of any true association. This could lead us to incorrectly diagnose a relationship.</p>
<p>The overall F-statistic does not suffer this problem because it adjusts for the number of predictors.</p>
<p>The F-statistic approach works when the number of predictors <span class="math inline">\(p\)</span> is small compared to <span class="math inline">\(n\)</span>. Sometimes, we have situations where <span class="math inline">\(p &gt; n\)</span>. In this situation, ther eare more coefficients <span class="math inline">\(\beta_j\)</span> to estimate than observations from which to estimate them. Such situations requires different approaches that we haven’t discussed yet (see chapter 6)</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> add chapter 6 link</span></code></pre>
</div>
<div id="deciding-on-important-variables" class="section level4">
<h4><span class="header-section-number">3.3.2.2</span> Deciding on Important Variables</h4>
<p>The first thing we do in a multiple regression is to compute the F-statistic and determine that at least one of the predictors is related to the response.</p>
<p>The task of determining which predictors are associated with the response is referred to as <em>variable selection</em>. We could try out a lot of different models with combinations of predictors, <span class="math inline">\(2^p\)</span>, but this is not practical as <span class="math inline">\(p\)</span> grows.</p>
<p>There are three ways to approach this task:</p>
<ul>
<li><p><em>Forward selection</em>: we begin with the <em>null model</em>, which contains an intercept but no predictors. We then fit <span class="math inline">\(p\)</span> simple linear regressions and add to the null model the variable that results in the lowest RSS. We then repeat the process to determine the lowest RSS of the now two-variable model, continuing until some stopping rule is satisfied.</p></li>
<li><p><em>Backward selection</em>: Start with all the variables in the model, remove the variable with the largest p-value. Then, for the new <span class="math inline">\((p - 1)\)</span>-variable model, do the same. Continue until stopping rule is reached (for example, some p-value threshold)</p></li>
<li><p><em>Mixed selection</em>: Start with no variables, and proceed with forward selection. If any p-value of added variables pass a threshold once new predictors are added, we remove them. We continue the forward and backward until all variables in model have a sufficiently low p-value.</p></li>
</ul>
</div>
<div id="model-fit" class="section level4">
<h4><span class="header-section-number">3.3.2.3</span> Model Fit</h4>
<p>Two common methods of model fit are the <span class="math inline">\(RSE\)</span> and <span class="math inline">\(R^2\)</span>, the fraction of variance explained.</p>
<p>More on <span class="math inline">\(R^2\)</span>:</p>
<ul>
<li>Values closer to <code>1</code> indicate a better fit</li>
<li>Adding more variables can only increase it
<ul>
<li>Adding variables that barely increase it can lead to overfitting</li>
</ul></li>
</ul>
<p>Plotting the model can also be useful.</p>
</div>
<div id="predictions" class="section level4">
<h4><span class="header-section-number">3.3.2.4</span> Predictions</h4>
<p>Three sorts of uncertainty within a given model:</p>
<ol style="list-style-type: decimal">
<li><p>The coefficient estimates <span class="math inline">\(\hat\beta_0 + \hat\beta_1...,\hat\beta_p\)</span> are estimates for <span class="math inline">\(\beta_0 + \beta_1...,\beta_p\)</span>. This inaccuracy is part of the <em>reducible error</em>. We can compute a confidence interval to determine how close <span class="math inline">\(\hat Y\)</span> is to <span class="math inline">\(f(X)\)</span>.</p></li>
<li><p><em>Model bias</em> can result from the fact that we are fitting a linear approximation to the true surface of <span class="math inline">\(f(X)\)</span>.</p></li>
<li><p>Even if we knew <span class="math inline">\(f(X)\)</span>, we still have random error <span class="math inline">\(\epsilon\)</span>, which is the <em>irreducible error</em>. We can use prediction intervals to estimate how far <span class="math inline">\(Y\)</span> will differ from <span class="math inline">\(\hat Y\)</span>. These will always be larger than confidence intervals, because they incorporate both the reducible + irreducible error.</p></li>
</ol>
</div>
</div>
<div id="other-considerations-in-the-regression-model" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Other Considerations in the Regression Model</h3>
<p>So far, all predictors have been <em>quantitative</em>. However, it is common to have <em>qualitative</em> variables as well.</p>
<p>Take a look at the <code>ISLR::Credit</code> dataset, which has a mix of both types.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_credit &lt;-<span class="st"> </span>ISLR<span class="op">::</span>Credit <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span>
<span class="st">  </span>janitor<span class="op">::</span><span class="kw">clean_names</span>()
tidy_credit</code></pre>
<pre><code>## # A tibble: 400 x 12
##      id income limit rating cards   age education gender student married
##   &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;fct&gt;  
## 1     1   14.9  3606    283     2    34        11 &quot; Mal… No      Yes    
## 2     2  106.   6645    483     3    82        15 Female Yes     Yes    
## 3     3  105.   7075    514     4    71        11 &quot; Mal… No      No     
## 4     4  149.   9504    681     3    36        11 Female No      No     
## 5     5   55.9  4897    357     2    68        16 &quot; Mal… No      Yes    
## 6     6   80.2  8047    569     4    77        10 &quot; Mal… No      No     
## # … with 394 more rows, and 2 more variables: ethnicity &lt;fct&gt;,
## #   balance &lt;int&gt;</code></pre>
<div id="predictors-with-only-two-levels" class="section level4">
<h4><span class="header-section-number">3.3.3.1</span> Predictors with only Two Levels</h4>
<p>Suppose we wish to investigate difference in credit card balance between males and females, ignoring all other variables. If a <em>qualitative</em> variable (also known as a <em>factor</em>) only has two possible values, then incorporating it into a model is easy. We can create a binomial dummy variable that takes on two values. For <code>gender</code>, this could be a variable that is <code>0</code> if observation has value <code>male</code>, and <code>1</code> if observation has value <code>female</code>. This variable can then be used in the regression equation.</p>
<p>Take note that <code>lm()</code> automatically creates dummy variables when given qualitative predictors.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> insert regression equation</span></code></pre>
<pre class="sourceCode r"><code class="sourceCode r">credit_model &lt;-<span class="st"> </span><span class="kw">lm</span>(balance <span class="op">~</span><span class="st"> </span>gender, <span class="dt">data =</span> tidy_credit)
tidy_credit_model &lt;-<span class="st"> </span>broom<span class="op">::</span><span class="kw">tidy</span>(credit_model)
tidy_credit_model</code></pre>
<pre><code>## # A tibble: 2 x 5
##   term         estimate std.error statistic  p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)     510.       33.1    15.4   2.91e-42
## 2 genderFemale     19.7      46.1     0.429 6.69e- 1</code></pre>
<p>How to interpret this: males are estimated to carry a balance of <code>$510</code>. Meanwhile, females are expected to carry an additional <span class="math inline">\(19.70\)</span> in debt. Notice the p-value is very high, indicating there is no significant difference between genders.</p>
</div>
</div>
<div id="qualitative-predictors-with-more-than-two-levels" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Qualitative Predictors with More than Two Levels</h3>
<p>A single dummy variable can not represent all the possible values. We can create additional dummy variables for this.</p>
<p>Let’s make a dummy variable from <code>ethnicity</code> column, which takes three distinct values. This will yield two dummy variables.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_credit <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">distinct</span>(ethnicity)</code></pre>
<pre><code>## # A tibble: 3 x 1
##   ethnicity       
##   &lt;fct&gt;           
## 1 Caucasian       
## 2 Asian           
## 3 African American</code></pre>
<p><code>fastDummies</code> package will be used to generate these. In this case, <code>African American</code> serves as the baseline, and dummy variables are created for <code>Caucasian</code> and <code>Asian</code>. <strong>There will always be one fewer dummy variable than the number of levels.</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_credit_dummy &lt;-<span class="st"> </span>tidy_credit <span class="op">%&gt;%</span>
<span class="st">  </span>fastDummies<span class="op">::</span><span class="kw">dummy_cols</span>(<span class="dt">select_columns =</span> <span class="st">&quot;ethnicity&quot;</span>, <span class="dt">remove_first_dummy =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span>janitor<span class="op">::</span><span class="kw">clean_names</span>()

tidy_credit_dummy <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="kw">starts_with</span>(<span class="st">&quot;ethnicity&quot;</span>))</code></pre>
<pre><code>## # A tibble: 400 x 3
##   ethnicity ethnicity_asian ethnicity_caucasian
##   &lt;fct&gt;               &lt;int&gt;               &lt;int&gt;
## 1 Caucasian               0                   1
## 2 Asian                   1                   0
## 3 Asian                   1                   0
## 4 Asian                   1                   0
## 5 Caucasian               0                   1
## 6 Caucasian               0                   1
## # … with 394 more rows</code></pre>
<p>We can again run the model with newly created dummy variables. Keep in mind, prior creation is not necessary, as <code>lm</code> will generate them automatically.</p>
<pre class="sourceCode r"><code class="sourceCode r">ethnicity_model &lt;-<span class="st"> </span><span class="kw">lm</span>(balance <span class="op">~</span><span class="st"> </span>ethnicity_asian <span class="op">+</span><span class="st"> </span>ethnicity_caucasian, <span class="dt">data =</span> tidy_credit_dummy)
broom<span class="op">::</span><span class="kw">tidy</span>(ethnicity_model)</code></pre>
<pre><code>## # A tibble: 3 x 5
##   term                estimate std.error statistic  p.value
##   &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)            531.       46.3    11.5   1.77e-26
## 2 ethnicity_asian        -18.7      65.0    -0.287 7.74e- 1
## 3 ethnicity_caucasian    -12.5      56.7    -0.221 8.26e- 1</code></pre>
</div>
<div id="extensions-of-the-linear-model" class="section level3">
<h3><span class="header-section-number">3.3.5</span> Extensions of the Linear Model</h3>
<p>The linear regression model makes highly restrictive assumptions. Two of the most important are that the relationship between predictors and response are <em>additive</em> and <em>linear</em>.</p>
<p>Additive means that the effect of changes in a predictor <span class="math inline">\(X_j\)</span> on the response <span class="math inline">\(Y\)</span> is independet of the values of the other predictors. Linear means that the the change in response <span class="math inline">\(Y\)</span> to a one-unit change in <span class="math inline">\(X_j\)</span> is constant, regardless of the value of <span class="math inline">\(X_j\)</span>.</p>
<p>Here are some common approaches of extending the linear model.</p>
<div id="removing-the-additive-assumption" class="section level4">
<h4><span class="header-section-number">3.3.5.1</span> Removing the Additive Assumption</h4>
<p>The additive property assumes that predictors slope terms are independent of the values of other predictors. However, this is not always the case. Imagine an advertising scenario where the effectiveness of TV spend is affected by the radio spend. This is known as an <em>interaction</em> effect. Imagine we have a model with two predictors, but they are not strictly additive. We could extend this model by adding an <em>interaction term</em> to it.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_1X_2 + \epsilon\)</span>
</p>
</div>
<p>Now, the effect of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span> is no longer constant; adjusting <span class="math inline">\(X_2\)</span> will change the impact of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span>.</p>
<p>An easy scenario is the productivity of a factory. Adding lines and workers both would increase productivity. However, the effect is not purely additive. Adding lines without having workers to operate them would not increase productivity. There is an interaction between workers and lines that needs to be accounted for.</p>
<p>The <em>hierarchical principle</em> states that <em>if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant</em>.</p>
<p>It’s also possible for qualitative and quantitative variables to interact with each other. We will again use the <code>Credit</code> data set. Suppose we wish to predict <code>balance</code> using the <code>income</code> (quantitative) and <code>student</code> (qualitative) variables.</p>
<p>First, let’s take a look at what it looks like to fit this model without an interaction term. Both <code>income</code> and <code>student</code> are significant.</p>
<pre class="sourceCode r"><code class="sourceCode r">lm_credit &lt;-<span class="st"> </span><span class="kw">lm</span>(balance <span class="op">~</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span>student, <span class="dt">data =</span> tidy_credit)
lm_credit <span class="op">%&gt;%</span><span class="st"> </span>broom<span class="op">::</span><span class="kw">tidy</span>()</code></pre>
<pre><code>## # A tibble: 3 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   211.      32.5        6.51 2.34e-10
## 2 income          5.98     0.557     10.8  7.82e-24
## 3 studentYes    383.      65.3        5.86 9.78e- 9</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">tidy_credit <span class="op">%&gt;%</span>
<span class="st">  </span>modelr<span class="op">::</span><span class="kw">add_predictions</span>(lm_credit) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> income, <span class="dt">y =</span> pred, <span class="dt">colour =</span> student)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="fl">1.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> balance, <span class="dt">colour =</span> student), <span class="dt">fill =</span> <span class="st">&quot;grey&quot;</span>, <span class="dt">pch =</span> <span class="dv">21</span>, <span class="dt">alpha =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>()</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-19-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>It’s a pretty good fit, and because there is no interaction terms, the lines are parallel. Notice how many more observations there are to fit on for the non-students.</p>
<p>Now, let’s add an interaction term.</p>
<pre class="sourceCode r"><code class="sourceCode r">lm_credit_int &lt;-<span class="st"> </span><span class="kw">lm</span>(balance <span class="op">~</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span>student <span class="op">+</span><span class="st"> </span>income <span class="op">*</span><span class="st"> </span>student, <span class="dt">data =</span> tidy_credit)
lm_credit_int <span class="op">%&gt;%</span><span class="st"> </span>broom<span class="op">::</span><span class="kw">tidy</span>()</code></pre>
<pre><code>## # A tibble: 4 x 5
##   term              estimate std.error statistic  p.value
##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)         201.      33.7        5.95 5.79e- 9
## 2 income                6.22     0.592     10.5  6.34e-23
## 3 studentYes          477.     104.         4.57 6.59e- 6
## 4 income:studentYes    -2.00     1.73      -1.15 2.49e- 1</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">tidy_credit <span class="op">%&gt;%</span>
<span class="st">  </span>modelr<span class="op">::</span><span class="kw">add_predictions</span>(lm_credit_int) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> income, <span class="dt">y =</span> pred, <span class="dt">colour =</span> student)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="fl">1.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> balance, <span class="dt">colour =</span> student), <span class="dt">fill =</span> <span class="st">&quot;grey&quot;</span>, <span class="dt">pch =</span> <span class="dv">21</span>, <span class="dt">alpha =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>()</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-21-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The model now takes into account how <code>income</code> and <code>student</code> interact with each other. Interpreting the chart suggests that increases in <code>income</code> among students has a smaller effect on balance than it does to non-students.</p>
<p>Does it fit better?</p>
<pre class="sourceCode r"><code class="sourceCode r">models &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">without_interaction =</span> lm_credit, <span class="dt">with_interaction =</span> lm_credit_int)
purrr<span class="op">::</span><span class="kw">map_df</span>(models, broom<span class="op">::</span>glance, <span class="dt">.id =</span> <span class="st">&quot;model&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(model, r.squared, statistic, p.value, df)</code></pre>
<pre><code>## # A tibble: 2 x 5
##   model               r.squared statistic  p.value    df
##   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;
## 1 without_interaction     0.277      76.2 9.64e-29     3
## 2 with_interaction        0.280      51.3 4.94e-28     4</code></pre>
<p>Not by much. The model with the interaction term has a slightly higher <span class="math inline">\(R^2\)</span>, but the added complexity of the model, combined with the small number of observations of students in the dataset, suggests overfitting.</p>
</div>
<div id="non-linear-relationships" class="section level4">
<h4><span class="header-section-number">3.3.5.2</span> Non-linear Relationships</h4>
<p>The linear model assumes a linear relationship between the response and predictors. We can extend the linear model to accomodate non-linear relationships using <em>polynomial regression</em>.</p>
<p>A way to incorporate non-linear associations into a linear model is to include transformed versions of the predictor in the model. For example, within the <code>Auto</code> dataset, predicting <code>mpg</code> with a second-order polynomial of <code>horsepower</code> would look like this:</p>
<div>
<p style="text-align:center">
<span class="math inline">\(mpg = \beta_0 + \beta_1horsepower + \beta_2horsepower^2 + \epsilon\)</span>
</p>
</div>
<p>Let’s look at the <code>Auto</code> dataset with models of different polynomial degrees overlaid. Clearly, the data is not linear, exhibiting a <em>quadratic</em> shape.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_auto &lt;-<span class="st"> </span>ISLR<span class="op">::</span>Auto <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>()

lm_auto &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>horsepower, <span class="dt">data =</span> tidy_auto)
lm_auto2 &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(horsepower, <span class="dv">2</span>), <span class="dt">data =</span> tidy_auto)
lm_auto5 &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(horsepower, <span class="dv">5</span>), <span class="dt">data =</span> tidy_auto)

tidy_auto <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather_predictions</span>(lm_auto, lm_auto2, lm_auto5) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> horsepower, <span class="dt">y =</span> mpg)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">pch =</span> <span class="dv">21</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred, <span class="dt">colour =</span> model), <span class="dt">size =</span> <span class="fl">1.5</span>)</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-23-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The second-order polynomial does a good job of fitting the data, while the fifth-order seems to be unnecessary. The model performance reflects that:</p>
<pre class="sourceCode r"><code class="sourceCode r">models &lt;-<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">linear =</span> lm_auto,
  <span class="dt">second_order =</span> lm_auto2,
  <span class="dt">fifth_order =</span> lm_auto5
)
purrr<span class="op">::</span><span class="kw">map_df</span>(models, broom<span class="op">::</span>glance, <span class="dt">.id =</span> <span class="st">&quot;model&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(model, r.squared, statistic, p.value, df)</code></pre>
<pre><code>## # A tibble: 3 x 5
##   model        r.squared statistic  p.value    df
##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;
## 1 linear           0.606      600. 7.03e-81     2
## 2 second_order     0.688      428. 5.40e-99     3
## 3 fifth_order      0.697      177. 1.16e-97     6</code></pre>
<p>The second-order model has significantly higher R^2, and only one more degree of freedom.</p>
<p>This approach of extending linear models to accomodate non-linear relationships is known as polynomial regression.</p>
</div>
</div>
<div id="potential-problems" class="section level3">
<h3><span class="header-section-number">3.3.6</span> Potential Problems</h3>
<p>Many problems can occur when fitting a linear model to a data set.</p>
<ol style="list-style-type: decimal">
<li>Non-linearity of the response-predictor relationship.</li>
<li>Correlation of error terms.</li>
<li>Non-constant variance of error terms</li>
<li>Outliers</li>
<li>High-leverage points</li>
<li>Collinearity</li>
</ol>
<div id="non-linearity-of-the-data" class="section level4">
<h4><span class="header-section-number">3.3.6.1</span> 1. Non-linearity of the data</h4>
<p>The linear model assumes a straight-line relationship between the predictors and the response. If this is not the case, the inference and prediction accuracy of the fit are suspect.</p>
<p>We can use a <em>residual plot</em> to visualize when a linear model is placed on to a non-linear relationship. For a simple linear regression model, we plot the residuals <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span> compared to the predictor. For multiple regression, we plot the residuals versus the predicted values <span class="math inline">\(\hat{y}_i\)</span>. If the relationship is linear, the residuals should exhibit a random pattern.</p>
<p>Let’s take the <code>Auto</code> dataset and plot the residuals compared to <code>horsepower</code> for each model we fit. Notice in the model containing no quadratic term is U-shaped, indicating a non-linear relationship. The model that contains <code>horsepower^2</code> exhibits little pattern in the residuals, indicating a better fit.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_auto <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather_predictions</span>(lm_auto, lm_auto2, lm_auto5) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> horsepower, <span class="dt">y =</span> mpg<span class="op">-</span>pred, <span class="dt">colour=</span>model)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="dv">3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">size =</span> <span class="fl">1.5</span>, <span class="dt">colour=</span><span class="st">&quot;grey&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>model, <span class="dt">nrow=</span><span class="dv">3</span>)</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-25-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>If the residual plot indicates that there non-linear associations in the data, a simple approach is to use non-linear transofmrations of the predictors.</p>
</div>
<div id="correlation-of-error-terms" class="section level4">
<h4><span class="header-section-number">3.3.6.2</span> 2. Correlation of Error Terms</h4>
<p>The linear regression model assumes that the error terms <span class="math inline">\(\epsilon_1,\epsilon_2,...,\epsilon_n\)</span> are uncorrelated.</p>
<p>This means that for a given error term <span class="math inline">\(e_i\)</span>, no information is provided about the value <span class="math inline">\(e_{i+1}\)</span>. The standard errors that are computed for the estimated regression coefficients are based on this assumption.</p>
<p>If there is a correlation among the error terms, than the estimated standard errors will tend to underestimate the true standard errors, producing confidence and prediction intervals narrower than they should be. Given the incorrect assumption, a 95% confidence interval may have a much lower probability than 0.95 of containing the true value of the parameter. P-values would also be lower than they should be, giving us an unwarranted sense of confidence in our model.</p>
<p>These correlations occur frequently in <em>time series</em> data, which consists of observations obtained at discrete points in time. In many cases, observations that are obtained at adjacent time periods points will have positively correlated errors.</p>
<p>We can again plot the residuals as a function of time to see if this is the case. If no correlation, there should be no pattern in the residuals. If error terms exhibit correlation, we may see that adjacent residuals exhibit similar values, known as <em>tracking</em>.</p>
<p>This can also happen outside of time series data. The assumption of uncorrelated errors is extremely important for linear regression as well as other statistical methods.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> add a residual plot for time series with correlated error terms, similar to pg. 95</span></code></pre>
<pre class="sourceCode r"><code class="sourceCode r">tidy_sunspot &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y=</span><span class="kw">as.matrix</span>(sunspot.year), <span class="dt">ds=</span><span class="kw">time</span>(sunspot.year)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>()
tidy_sunspot
sunspot_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> tidy_sunspot, y <span class="op">~</span><span class="st"> </span>ds)
tidy_sunspot <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(sunspot_lm) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>ds, <span class="dt">y=</span>pred<span class="op">-</span>y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre>
</div>
<div id="non-constant-variance-of-error-terms" class="section level4">
<h4><span class="header-section-number">3.3.6.3</span> 3. Non-constant Variance of Error Terms</h4>
<p>Another assumption of linear regression is that the error terms have a constant variance, <span class="math inline">\(Var(\epsilon_i) = \sigma^2\)</span>. Standard errors, confidence intervals, and hypothesis tests rely upon this assumption.</p>
<p>It is common for error terms to exhiti non-constant variance. Non-constant variance in the errors, also known as <em>heteroscedasticity</em>, can be identified from a <em>funnel shape</em> in the residual plot.</p>
<p>Let’s take a look at the <code>MASS::cats</code> dataset, which contains observations of various cats sex, body weight, and heart weight. We fit a linear model to it, and then plot the residuals.</p>
<p>I’ve added a linear fit to the residual plot itself. Observe how the error terms begin to funnel out as <code>bwt</code> increases, indicating non-constant variance of the error terms.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_cats &lt;-<span class="st"> </span>MASS<span class="op">::</span>cats <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span>janitor<span class="op">::</span><span class="kw">clean_names</span>()
lm_cats &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> tidy_cats, hwt <span class="op">~</span><span class="st"> </span>bwt)

tidy_cats <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(lm_cats) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bwt, <span class="dt">y =</span> hwt <span class="op">-</span><span class="st"> </span>pred)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>, <span class="dt">level =</span> <span class="fl">0.99</span>)</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-27-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> add OLS method to this</span>
<span class="co"># get weights of each response</span>

<span class="co"># fit a linear model on the residuals</span>
tidy_cats_res &lt;-<span class="st"> </span>tidy_cats <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">add_predictions</span>(lm_cats) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">res =</span> hwt <span class="op">-</span><span class="st"> </span>pred) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(bwt, hwt, res)
lm_cats_res &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> tidy_cats_res, res <span class="op">~</span><span class="st"> </span>hwt)

cat_weights &lt;-<span class="st"> </span>tidy_cats_res <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(lm_cats_res) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">res_var =</span> (res<span class="op">-</span>pred)<span class="op">^</span><span class="dv">2</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">weight =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>res_var) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>(weight)

lm_cats_weights &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> tidy_cats, hwt <span class="op">~</span><span class="st"> </span>bwt, <span class="dt">weights =</span> cat_weights)

tidy_cats <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather_predictions</span>(lm_cats, lm_cats_weights) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bwt, <span class="dt">y =</span> hwt <span class="op">-</span><span class="st"> </span>pred, <span class="dt">colour =</span> model)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> hwt<span class="op">-</span>pred, <span class="dt">colour =</span> model)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>)</code></pre>
<p>When this occurs, there a few ways to remedy it. You could transform the response <span class="math inline">\(Y\)</span> using a function such as <span class="math inline">\(logY\)</span> or <span class="math inline">\(\sqrt{Y}\)</span>. If we have a good idea of the variance of each response, we could fit our model using <em>weighted least squares</em>, which weights proportional to the inverse of the expected variance of an observation.</p>
</div>
<div id="outliers" class="section level4">
<h4><span class="header-section-number">3.3.6.4</span> 4. Outliers</h4>
<p>An <em>outlier</em> is a point for which <span class="math inline">\(y_i\)</span> is far from the value predicted by the model. These can arise for a variety of reasons, such as incorrect recording of an observation during data collection.</p>
<p>For the <code>msleep</code> dataset below, which contains data on mammal sleep durations, I’ve highlighted two observations that most would consider outliers. This is for the <code>African elephant</code> and <code>Asian elephant</code> mammals, who’s bodyweights are far and away from the rest of mammals. There are others that could be considered outliers as well. Identifying outliers is an often arbitrary process.</p>
<pre class="sourceCode r"><code class="sourceCode r">msleep <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> awake, <span class="dt">y =</span> bodywt, <span class="dt">colour =</span> name)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span>gghighlight<span class="op">::</span><span class="kw">gghighlight</span>(bodywt<span class="op">&gt;</span><span class="dv">2000</span>)</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-29-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Let’s fit a linear model to predict body weight from how long the animal is awake.</p>
<p>Notice how the data that maintains the elephant observations significantly affects the slope, drawing the regression line away from the majority of observations.</p>
<pre class="sourceCode r"><code class="sourceCode r">lm_sleep &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> msleep, bodywt <span class="op">~</span><span class="st"> </span>awake)
lm_sleep_filtered &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> msleep <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(<span class="op">!</span>name <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;African elephant&#39;</span>, <span class="st">&#39;Asian elephant&#39;</span>)), bodywt <span class="op">~</span><span class="st"> </span>awake)
msleep <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather_predictions</span>(lm_sleep, lm_sleep_filtered) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> awake, <span class="dt">y =</span> bodywt)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">pch=</span><span class="dv">21</span>, <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred, <span class="dt">colour =</span> model), <span class="dt">size =</span> <span class="fl">1.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_colour_viridis_d</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>()</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-30-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The model excluding the elephant observations has a significantly higher <span class="math inline">\(R^2\)</span>, which indicates a better fit.</p>
<pre class="sourceCode r"><code class="sourceCode r">models &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">with_outliers =</span> lm_sleep,
               <span class="dt">without_outliers =</span> lm_sleep_filtered)
purrr<span class="op">::</span><span class="kw">map_df</span>(models, broom<span class="op">::</span>glance, <span class="dt">.id =</span> <span class="st">&quot;model&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(model, r.squared, statistic, p.value, df)</code></pre>
<pre><code>## # A tibble: 2 x 5
##   model            r.squared statistic    p.value    df
##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;
## 1 with_outliers       0.0973      8.73 0.00409        2
## 2 without_outliers    0.222      22.5  0.00000900     2</code></pre>
<p>Another way of handling an outlier is transforming the response variable. Upon inspection of the scatterplot, it becomes clear that the relationship between <code>bodywt</code> and <code>awake</code> is not linear. If we take the same dataset and apply a <code>log</code> function to response variable <code>bodywt</code>, we see that the outliers no longer exists.</p>
<pre class="sourceCode r"><code class="sourceCode r">msleep <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> awake, <span class="dt">y =</span> <span class="kw">log</span>(bodywt))) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-32-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The model that uses <code>log(bodywt)</code> as the response also has better performance than both models above.</p>
<pre class="sourceCode r"><code class="sourceCode r">lm_sleep_log &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> msleep, <span class="kw">log</span>(bodywt) <span class="op">~</span><span class="st"> </span>awake)
lm_sleep_log <span class="op">%&gt;%</span><span class="st"> </span>broom<span class="op">::</span><span class="kw">glance</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(r.squared, statistic, p.value, df)</code></pre>
<pre><code>## # A tibble: 1 x 4
##   r.squared statistic      p.value    df
##       &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt;
## 1     0.324      38.7 0.0000000202     2</code></pre>
</div>
<div id="high-leverage-points" class="section level4">
<h4><span class="header-section-number">3.3.6.5</span> 5. High Leverage Points</h4>
<p>Observations with <em>high leverage</em> have an unusual value for <span class="math inline">\(x_i\)</span>. In a simple regression model, these are practically the same as outliers (I could have flipped predictors with response in my mammal sleep model above). High leverage points are observations that significantly move the regression line. However, in multiple regression, it is possible to have an observation that is well within the range of each individual predictor’s values, but unusual terms of the full set of predictors.</p>
<p>Let’s look at <code>temp</code> and <code>ozone</code> data from <code>airquality</code> dataset. Plenty of observations have ozone &gt; <code>100</code> and temp &lt; <code>80</code>, but the combination is rare. If we fit models with and without this leverage point, we can see how the regression line moves.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_airquality &lt;-<span class="st"> </span>airquality <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span>
<span class="st">  </span>janitor<span class="op">::</span><span class="kw">clean_names</span>()

tidy_airquality <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> ozone, <span class="dt">y =</span> temp)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>gghighlight<span class="op">::</span><span class="kw">gghighlight</span>(ozone <span class="op">&gt;</span><span class="st"> </span><span class="dv">100</span> <span class="op">&amp;</span><span class="st"> </span>temp <span class="op">&lt;</span><span class="st"> </span><span class="dv">80</span>)</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-34-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="sourceCode r"><code class="sourceCode r">lm_temp &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> tidy_airquality, temp <span class="op">~</span><span class="st"> </span>ozone)
lm_temp_filtered &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> tidy_airquality <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(<span class="op">!</span>(ozone <span class="op">&gt;</span><span class="st"> </span><span class="dv">100</span> <span class="op">&amp;</span><span class="st"> </span>temp <span class="op">&lt;</span><span class="st"> </span><span class="dv">80</span>)), temp <span class="op">~</span><span class="st"> </span>ozone)</code></pre>
<p>Notice the slight change in the fit. This observation isn’t that extreme, but still produces a visible difference in fit. More extreme observations would move this line even further, potentially causing an improper fit.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_airquality <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather_predictions</span>(lm_temp, lm_temp_filtered) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> ozone, <span class="dt">y =</span> pred, <span class="dt">colour =</span> model)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> temp), <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dt">colour =</span> <span class="st">&quot;grey&quot;</span>)</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-36-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The model without the strange observation performs slightly better. In reality, I would probably include it as going from an <span class="math inline">\(R^2\)</span> of <code>0.488</code> to <code>0.506</code> isn’t worth the manual effort.</p>
<pre class="sourceCode r"><code class="sourceCode r">models &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">with_leverage =</span> lm_temp,
               <span class="dt">without_leverage =</span> lm_temp_filtered)
purrr<span class="op">::</span><span class="kw">map_df</span>(models, broom<span class="op">::</span>glance, <span class="dt">.id =</span> <span class="st">&quot;model&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(model, r.squared, statistic, p.value, df)</code></pre>
<pre><code>## # A tibble: 2 x 5
##   model            r.squared statistic  p.value    df
##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;
## 1 with_leverage        0.488      109. 2.93e-18     2
## 2 without_leverage     0.506      116. 5.05e-19     2</code></pre>
<p>We can also quantify an observation’s leverage by computing the <span class="math inline">\(leverage statistic\)</span>. A large value of this statistic indicates an observation with high leverage. For a simple linear regression,</p>
<div>
<p style="text-align:center">
<span class="math inline">\(h_i = \frac{1}{n} + \frac{x_i - \bar{x}^2}{\sum_{i&#39;=1}^{n}}\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>leverage statistic</em>
</p>
</div>
<p>As <span class="math inline">\(x_i\)</span> increases from <span class="math inline">\(\bar{x}\)</span>, <span class="math inline">\(h_i\)</span> increases. Combined with a high residual (outlier), a high-leverage observation could be dangerous.</p>
</div>
<div id="colinearity" class="section level4">
<h4><span class="header-section-number">3.3.6.6</span> 6. Colinearity</h4>
<p><code>Collinearity</code> is when two or more predictor variables are closely related to one another.</p>
<p>If we look at our <code>tidy_credit</code> tibble, we see that <code>rating</code> and <code>limit</code> are very highly correlated with one another.</p>
<pre class="sourceCode r"><code class="sourceCode r">tidy_credit <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> rating, <span class="dt">y =</span> limit)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-38-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The effects of collinearity can make it difficult to separate out the individual effects of collinear variables on the response. Since <code>limit</code> and <code>rating</code> move together, it can be hard to determine how each one separately is associated with the response, <code>balance</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> add contour plot</span>
<span class="co"># clean up what this link did: https://yetanotheriteration.netlify.com/2018/01/high-collinearity-effect-in-regressions/</span></code></pre>
<p>Two multiple regression models show the effects of regressing <code>balance</code> on <code>age + limit</code> versus <code>rating + limit</code>. Notice the large p-value for <code>limit</code>, combined with the &gt;10x increase in standard error for its coefficient compared to the original model. It’s effects have been masked by the collinearity it shares with <code>rating</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">lm_age_limit &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> tidy_credit, balance <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>limit)
lm_rating_limit &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> tidy_credit, balance <span class="op">~</span><span class="st"> </span>rating <span class="op">+</span><span class="st"> </span>limit)
models &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="st">`</span><span class="dt">Balance ~ Age + Limit</span><span class="st">`</span> =<span class="st"> </span>lm_age_limit,
               <span class="st">`</span><span class="dt">Balance ~ Rating + Limit</span><span class="st">`</span> =<span class="st"> </span>lm_rating_limit)
purrr<span class="op">::</span><span class="kw">map_df</span>(models, broom<span class="op">::</span>tidy, <span class="dt">.id =</span> <span class="st">&quot;model&quot;</span>)</code></pre>
<pre><code>## # A tibble: 6 x 6
##   model                  term        estimate std.error statistic   p.value
##   &lt;chr&gt;                  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 Balance ~ Age + Limit  (Intercept) -1.73e+2  43.8        -3.96  9.01e-  5
## 2 Balance ~ Age + Limit  age         -2.29e+0   0.672      -3.41  7.23e-  4
## 3 Balance ~ Age + Limit  limit        1.73e-1   0.00503    34.5   1.63e-121
## 4 Balance ~ Rating + Li… (Intercept) -3.78e+2  45.3        -8.34  1.21e- 15
## 5 Balance ~ Rating + Li… rating       2.20e+0   0.952       2.31  2.13e-  2
## 6 Balance ~ Rating + Li… limit        2.45e-2   0.0638      0.384 7.01e-  1</code></pre>
<p>The growth in the standard error caused by collinearity caises the t-statistic (<span class="math inline">\(B_j\)</span> divided by it’s standard error) to decline. As a result, we may fail to reject <span class="math inline">\(H_0: \beta_j = 0\)</span>. This means that the <em>power</em> of the hypothesis test – the probability of correctly detecting a non-zero coefficient – is reduced by collinearity. Thus, it is desirable to identify and address collinearity problems while fitting the model.</p>
<p>A simple way is to look at the correlation matrix of the predictors.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(corrr)
tidy_credit_corr &lt;-<span class="st"> </span>tidy_credit <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select_if</span>(is.numeric) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>id) <span class="op">%&gt;%</span>
<span class="st">  </span>corrr<span class="op">::</span><span class="kw">correlate</span>() <span class="op">%&gt;%</span>
<span class="st">  </span>corrr<span class="op">:::</span><span class="kw">stretch</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span><span class="kw">is.na</span>(r))

<span class="co"># remove duplicate combinations of x, y</span>
tidy_credit_corr &lt;-<span class="st"> </span>tidy_credit_corr <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">sort_var =</span> <span class="kw">map2_chr</span>(x, y, <span class="op">~</span><span class="kw">toString</span>(<span class="kw">sort</span>(<span class="kw">c</span>(.x, .y))))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">distinct</span>(sort_var, <span class="dt">.keep_all =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>sort_var)

tidy_credit_corr <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(<span class="kw">abs</span>(r))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">abs</span>(r) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.05</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">kable</span>() <span class="op">%&gt;%</span>
<span class="st">  </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>()</code></pre>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
x
</th>
<th style="text-align:left;">
y
</th>
<th style="text-align:right;">
r
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
limit
</td>
<td style="text-align:left;">
rating
</td>
<td style="text-align:right;">
0.9968797
</td>
</tr>
<tr>
<td style="text-align:left;">
rating
</td>
<td style="text-align:left;">
balance
</td>
<td style="text-align:right;">
0.8636252
</td>
</tr>
<tr>
<td style="text-align:left;">
limit
</td>
<td style="text-align:left;">
balance
</td>
<td style="text-align:right;">
0.8616973
</td>
</tr>
<tr>
<td style="text-align:left;">
income
</td>
<td style="text-align:left;">
limit
</td>
<td style="text-align:right;">
0.7920883
</td>
</tr>
<tr>
<td style="text-align:left;">
income
</td>
<td style="text-align:left;">
rating
</td>
<td style="text-align:right;">
0.7913776
</td>
</tr>
<tr>
<td style="text-align:left;">
income
</td>
<td style="text-align:left;">
balance
</td>
<td style="text-align:right;">
0.4636565
</td>
</tr>
<tr>
<td style="text-align:left;">
income
</td>
<td style="text-align:left;">
age
</td>
<td style="text-align:right;">
0.1753384
</td>
</tr>
<tr>
<td style="text-align:left;">
rating
</td>
<td style="text-align:left;">
age
</td>
<td style="text-align:right;">
0.1031650
</td>
</tr>
<tr>
<td style="text-align:left;">
limit
</td>
<td style="text-align:left;">
age
</td>
<td style="text-align:right;">
0.1008879
</td>
</tr>
<tr>
<td style="text-align:left;">
cards
</td>
<td style="text-align:left;">
balance
</td>
<td style="text-align:right;">
0.0864563
</td>
</tr>
<tr>
<td style="text-align:left;">
rating
</td>
<td style="text-align:left;">
cards
</td>
<td style="text-align:right;">
0.0532390
</td>
</tr>
<tr>
<td style="text-align:left;">
cards
</td>
<td style="text-align:left;">
education
</td>
<td style="text-align:right;">
-0.0510842
</td>
</tr>
</tbody>
</table>
<p>Here we can see the strongest correlation is between <code>limit</code> and <code>rating</code>, <code>rating</code> and <code>balance</code>, which makes sense. Card issuers give higher <code>limit</code> to those with higher <code>rating</code>, and <code>rating</code> is directly affted by <code>balance</code>. As we move down the table, weaker correlations appear, such as <code>cards</code> and <code>education</code>.</p>
<p>Such tables allow one to identify potential collinearity problems between two predictors. However, it is possible for collinearity to exist between three or more variables even if there is no correlation between any of the pairs in that group. This is called <em>multicollinearity</em>. In this case, we compute the <em>variance inflation factor</em> (VIF). The VIF is the ratio of the variance of <span class="math inline">\(\hat\beta_j\)</span> when fitting the full model divided by the variance of <span class="math inline">\(\hat\beta_j\)</span> if fit on its own.</p>
<p>VIF can also be thought of as checking if a predictor can be explained by all the other predictors in the dataset. In this case, that predictor provides no novel information, but can add a significant amount of variance. Such predictors with high VIF are a candidate for removal from the model.</p>
<p>Here is a video (Python) explaining how to use VIF to tackle multicollinearity:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=qmt7ZZoiDwc">Variance Inflation Factor (VIF) for Detecting Multicolinearity in Python</a></li>
</ul>
<p>A rule of thumb is that a VIF value exceeding 10 indicates a problematic amount of collinearity.</p>
<p>The VIF for each variable can be computed using the formula</p>
<div>
<p style="text-align:center">
<span class="math inline">\(VIF(\hat\beta_j) = \frac{1}{1-R^2_{X_j|X_-j}}\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>leverage statistic</em>
</p>
</div>
<p>where <span class="math inline">\(R^2_{X_j|X_-j}\)</span> is the <span class="math inline">\(R^2\)</span> from a regression of <span class="math inline">\(X_j\)</span> onto all of the other predictors. If a variable <span class="math inline">\(X_j\)</span> has a high <span class="math inline">\(R^2\)</span> when regressed onto all other predictor variables (other predictor variables can explain a large amount of <span class="math inline">\(X_j\)</span>’s variance), the VIF will be high.</p>
<p>In R, we can use the <code>car</code> package to calculate <code>GVIF</code> for a <code>lm</code> object.</p>
<pre class="sourceCode r"><code class="sourceCode r">car<span class="op">::</span><span class="kw">vif</span>(<span class="kw">lm</span>(<span class="dt">data =</span> tidy_credit <span class="op">%&gt;%</span>
<span class="st">              </span><span class="kw">select</span>(<span class="op">-</span>id), balance <span class="op">~</span><span class="st"> </span>.)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as_tibble</span>(<span class="dt">rownames =</span> <span class="st">&quot;variable&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(GVIF))</code></pre>
<pre><code>## # A tibble: 10 x 4
##   variable   GVIF    Df `GVIF^(1/(2*Df))`
##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;
## 1 rating   236.       1             15.4 
## 2 limit    234.       1             15.3 
## 3 income     2.79     1              1.67
## 4 cards      1.45     1              1.20
## 5 age        1.05     1              1.03
## 6 married    1.04     1              1.02
## # … with 4 more rows</code></pre>
<p>As we can see here, the most problematic variables are <code>rating</code> and <code>limit</code>, which both exceed VIFs of 200. It seems that <code>rating</code> and <code>limit</code> can be explained by the other predictors in the dataset.</p>
<p>There are two simple solutions for this. The first is to drop problematic variables from the regression.</p>
<p>Let’s drop <code>rating</code> and see what happens.</p>
<pre class="sourceCode r"><code class="sourceCode r">car<span class="op">::</span><span class="kw">vif</span>(<span class="kw">lm</span>(<span class="dt">data =</span> tidy_credit <span class="op">%&gt;%</span>
<span class="st">              </span><span class="kw">select</span>(<span class="op">-</span>id, <span class="op">-</span>rating), balance <span class="op">~</span><span class="st"> </span>.)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as_tibble</span>(<span class="dt">rownames =</span> <span class="st">&quot;variable&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(GVIF))</code></pre>
<pre><code>## # A tibble: 9 x 4
##   variable   GVIF    Df `GVIF^(1/(2*Df))`
##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;
## 1 income     2.77     1              1.67
## 2 limit      2.71     1              1.65
## 3 age        1.05     1              1.03
## 4 married    1.03     1              1.02
## 5 ethnicity  1.03     2              1.01
## 6 student    1.02     1              1.01
## # … with 3 more rows</code></pre>
<p>Just like that, <code>limit</code> goes from VIF &gt;200 down to ~2.7. Is the fit compromised? The model without rating has an <span class="math inline">\(R^2\)</span> of <code>0.954</code>, while the model containing it has <code>0.955</code>. Such negligible change in <span class="math inline">\(R_2\)</span> shows the redundant information provided by the variable. Reducing the multicollinearity gives us an equivalent level fit while reducing the chance for a significant predictor to end up with an insignificant coefficient in our model. This is especially important when extending a pre-existing model on to a new, unknown sample.</p>
<p>Another solution would be to combine collinear variables into a single predictor. For example, taking the average of standardized versions of <code>limit</code> and <code>rating</code> and creating a new variable that measures <em>credit worthiness</em>.</p>
</div>
</div>
<div id="the-marketing-plan" class="section level3">
<h3><span class="header-section-number">3.3.7</span> The Marketing Plan</h3>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span></code></pre>
<p>We return to the seven questions we set out to answer regarding <code>Advertising</code> data.</p>
</div>
<div id="comparison-of-linear-regression-with-k-nearest-neighbors" class="section level3">
<h3><span class="header-section-number">3.3.8</span> Comparison of Linear Regression with <em>K</em>-Nearest Neighbors</h3>
<p>Linear regression is an example of a <em>parametric</em> approach because it assumes a linear functional form for <span class="math inline">\(f(X)\)</span>. The advantages are:</p>
<ul>
<li>estimating small number of coefficients is easy</li>
<li>coefficients have simple intrepretations</li>
<li>tests of statistical significance are easily performed</li>
</ul>
<p>However, the strong assumptions of the form of <span class="math inline">\(f(X)\)</span> come at a cost. If the functional form is far from the truth, then the model will perform poorly in terms of prediction accuracy.</p>
<p><em>Non-parametric</em> methods do not explicitly assume a parametric form for <span class="math inline">\(f(X)\)</span>, and thereby provide an alternative and more flexible approach for performing regression. Let’s consider one of the simplest and best-known non-parametric methods, <em>K-nearest neighbors regression</em> (KNN regression).</p>
<p>Given a value for <span class="math inline">\(K\)</span> and a prediction point <span class="math inline">\(x_0\)</span>, KNN regression first identifies the <span class="math inline">\(K\)</span> training observations that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(\mathcal{N}_0\)</span>. It then estimates <span class="math inline">\(f(x_0)\)</span> using the average of all the training response in <span class="math inline">\(\mathcal{N}_0\)</span>. In other words,</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat{f}(x_0) = \frac{1}{K}\sum_{x_i\in\mathcal{N_0}y_i\)</span>
</p>
<p class="vocab" style="text-align:right">
</p>
</div>
<p>When <span class="math inline">\(K\)</span> is small, the KNN fits close to the training observations, and provides a rough step function. As <span class="math inline">\(K\)</span> increases, the prediction is averaged over a larger number of observations, producing small regions of constant prediction and smoother fits. The optimal value for <span class="math inline">\(K\)</span> depends on the <em>bias-variance tradeoff</em>.</p>
<p>A small value of <span class="math inline">\(K\)</span> provides the most flexible fit, which will have low bias but high variance. The high variance comes from the prediction in a particular region being dependant on a small number of observations. In contrast, larger values of <span class="math inline">\(K\)</span> provide a smoother and less variable fit; incorporating the average of a larger number of points, giving a single observation less weight in the prediction. However, this smoothing can cause bias by masking some of the structure in <span class="math inline">\(f(x)\)</span>. In Chapter 5, (<em>TODO</em> add link), we introduce several approaches for estimating test error rates, which can be used identify optimal value of <span class="math inline">\(K\)</span>.</p>
<p>We can use the <code>caret</code> package to fit a number of <span class="math inline">\(k\)</span>-values with the KNN algorithm, and see which produces the best R^2. This example doesn’t take into account the bias introduced by a larger <span class="math inline">\(k\)</span>-value, and how that might perform on a test dataset.</p>
<p>A <span class="math inline">\(k\)</span> of 5 produces the best fit.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> plot 3d with decision boundaries for multiple k</span>
<span class="kw">library</span>(caret)
k_values =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">c</span>(<span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)))

knn_airquality &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">data =</span> tidy_airquality <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(<span class="op">!</span><span class="kw">is.na</span>(ozone)), ozone <span class="op">~</span><span class="st"> </span>temp <span class="op">+</span><span class="st"> </span>wind, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>,<span class="dt">tuneGrid =</span> k_values, <span class="dt">metric =</span> <span class="st">&quot;Rsquared&quot;</span>)
knn_results &lt;-<span class="st"> </span>knn_airquality<span class="op">$</span>results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>()
knn_results</code></pre>
<pre><code>## # A tibble: 10 x 7
##       k  RMSE Rsquared   MAE RMSESD RsquaredSD MAESD
##   &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;
## 1     1  22.9    0.564  15.3   3.24     0.0864  2.01
## 2     2  20.6    0.635  14.4   3.30     0.0915  1.89
## 3     3  19.7    0.658  14.0   2.44     0.0731  1.52
## 4     4  19.1    0.665  13.9   2.56     0.0705  1.46
## 5     5  18.9    0.674  13.7   2.72     0.0675  1.40
## 6     6  18.7    0.683  13.4   2.84     0.0718  1.51
## # … with 4 more rows</code></pre>
<p>In what setting will a parametric approach outperform a non-parametric approach such as KNN? The parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of <span class="math inline">\(f\)</span>.</p>
<p>Let’s take our ozone example and compare KNN to linear regression. The linear model produces an <span class="math inline">\(R^2\)</span> of <code>0.57</code>, compared to KNN’s <code>0.64</code>. Is KNN’s lack of interpretability worth the <code>0.07</code> gain in <span class="math inline">\(R^2\)</span>? Probably not.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> split into training / test and compare KNN vs Linear</span></code></pre>
<p>Interestingly, the plot below demonstrates a shortcoming of linear regression as well, and the value of domain knowledge in approaching modeling. <code>ozone</code> values should have a minimum value of <code>0</code>, and yet, our linear regression model predicts negative values for certain observations. One good thing of the KNN approach is that, due to its nature of averaging out observations, it’s impossible for it to run into the same scenario.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> clean plot</span>
knn_results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(Rsquared <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(Rsquared))</code></pre>
<pre><code>## # A tibble: 1 x 7
##       k  RMSE Rsquared   MAE RMSESD RsquaredSD MAESD
##   &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;
## 1     7  18.4    0.691  13.1   3.01     0.0802  1.63</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">lm_airquality &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> tidy_airquality, ozone <span class="op">~</span><span class="st"> </span>temp <span class="op">+</span><span class="st"> </span>wind)

lm_airquality <span class="op">%&gt;%</span><span class="st"> </span>broom<span class="op">::</span><span class="kw">glance</span>()</code></pre>
<pre><code>## # A tibble: 1 x 11
##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.569         0.561  21.9      74.5 2.31e-21     3  -521. 1050. 1061.
## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">tidy_airquality <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather_predictions</span>(knn_airquality, lm_airquality) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> ozone, <span class="dt">y =</span> pred, <span class="dt">colour =</span> model)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>()</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-47-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>When the relationship is truly linear, it is common for a linear regression to outperform KNN. However, as the relationship grows more non-linear, KNN might end up as a better approach. KNN being nonparametric doesn’t make any assumptions about the true form of <span class="math inline">\(f(x)\)</span>, and will often outperform (not always) linear regression in non-linear scenarios.</p>
<p>When does KNN exhibit poor performance? This happens when the number of predictors grows. In a <span class="math inline">\(p=1\)</span> setting with 100 observtions, KNN can probably accurately estimate <span class="math inline">\(f(X)\)</span>. If we increase it to <span class="math inline">\(p=20\)</span>, we run in to the <em>curse of dimensionality</em>. As <span class="math inline">\(p\)</span> increases, the <span class="math inline">\(K\)</span> observations that are nearest to a given test observation <span class="math inline">\(x_0\)</span> may be very far away from <span class="math inline">\(x_0\)</span> in <span class="math inline">\(p\)</span>-dimensional space. This will lead to a poor KNN fit. As a rule of thumb, <strong>parametric methods tend to outperform non-parametric approaches when there is a small number of observations per predictor.</strong></p>
<p>Linear regression also has the benefit of interpretability. We might forego a bit of prediction accuracy for the sake of a simple model with interpretable coefficients and available p-values.</p>
</div>
</div>
<div id="lab-linear-regression" class="section level2">
<h2><span class="header-section-number">3.4</span> Lab: Linear Regression</h2>
<p>Use <code>library()</code> to load libraries.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS, <span class="dt">exclude =</span> <span class="st">&quot;select&quot;</span>)
<span class="kw">library</span>(ISLR)</code></pre>
<p>We will be running a linear regression on <code>MASS::Boston</code> dataset.</p>
<p>First thing to do is convert to a tibble, which provides data types, reasonable printing methods, and tidy structure.</p>
<pre class="sourceCode r"><code class="sourceCode r">boston &lt;-<span class="st"> </span>MASS<span class="op">::</span>Boston <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>()
boston</code></pre>
<pre><code>## # A tibble: 506 x 14
##      crim    zn indus  chas   nox    rm   age   dis   rad   tax ptratio
##     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1 0.00632    18  2.31     0 0.538  6.58  65.2  4.09     1   296    15.3
## 2 0.0273      0  7.07     0 0.469  6.42  78.9  4.97     2   242    17.8
## 3 0.0273      0  7.07     0 0.469  7.18  61.1  4.97     2   242    17.8
## 4 0.0324      0  2.18     0 0.458  7.00  45.8  6.06     3   222    18.7
## 5 0.0690      0  2.18     0 0.458  7.15  54.2  6.06     3   222    18.7
## 6 0.0298      0  2.18     0 0.458  6.43  58.7  6.06     3   222    18.7
## # … with 500 more rows, and 3 more variables: black &lt;dbl&gt;, lstat &lt;dbl&gt;,
## #   medv &lt;dbl&gt;</code></pre>
<p>We are interested in predicting <code>medv</code> (median house value) using 13 predictors, such as <code>rm</code> (number of rooms per house), <code>age</code> (average age of houses), and <code>lstat</code>(percent of households with low socioeconomic status).</p>
<div id="fitting-a-linear-regression" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Fitting a linear regression</h3>
<p>We will use the <code>lm</code> function to fit a simple linear regression, which takes the form <code>lm(data = data, y ~ x)</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">boston_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> boston, medv <span class="op">~</span><span class="st"> </span>lstat)</code></pre>
<p>Printing <code>boston_lm</code> provides some basic information about the model.</p>
<pre class="sourceCode r"><code class="sourceCode r">boston_lm</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat, data = boston)
## 
## Coefficients:
## (Intercept)        lstat  
##       34.55        -0.95</code></pre>
<p>If we want to work with data surrounding the model, compare it to other models, or reference components of it, we need to use the <a href="https://cran.r-project.org/web/packages/broom/vignettes/broom.html"><code>broom</code></a> package.</p>
<p><code>broom::tidy</code> converts the model into a data.frame representation.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(broom)
broom<span class="op">::</span><span class="kw">tidy</span>(boston_lm)</code></pre>
<pre><code>## # A tibble: 2 x 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   34.6      0.563       61.4 3.74e-236
## 2 lstat         -0.950    0.0387     -24.5 5.08e- 88</code></pre>
<p><code>broom::glance</code> will provide a summary in data.frame form.</p>
<pre class="sourceCode r"><code class="sourceCode r">broom<span class="op">::</span><span class="kw">glance</span>(boston_lm)</code></pre>
<pre><code>## # A tibble: 1 x 11
##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.544         0.543  6.22      602. 5.08e-88     2 -1641. 3289. 3302.
## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;</code></pre>
<p><code>broom</code> is powerful for easy output, accessing elements of a model, and comparing multiple models.</p>
<p>This is where ISLR misses the mark. It shows <code>attach()</code>, which we discussed as a bad idea in Chapter 1, and only offers the base R methods on inspecting a model object. It goes into inspecting model elements via the use of <code>names()</code>, <code>coef()</code>, and more, which provide much less utility than <code>broom</code>.</p>
<p>To get confidence interval of coefficients, we can use <code>conf.int</code> argument in <code>broom::tidy</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">broom<span class="op">::</span><span class="kw">tidy</span>(boston_lm, <span class="dt">conf.int =</span> <span class="ot">TRUE</span>, <span class="dt">conf.level =</span> <span class="fl">0.95</span>)</code></pre>
<pre><code>## # A tibble: 2 x 7
##   term        estimate std.error statistic   p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   34.6      0.563       61.4 3.74e-236    33.4     35.7  
## 2 lstat         -0.950    0.0387     -24.5 5.08e- 88    -1.03    -0.874</code></pre>
<p>We can use <code>modelr::add_predictions()</code> or <code>broom::augment()</code> to add predictions and confidence intervals for our model.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> decide on best practices for adding predictions; development effort on `modelr`</span></code></pre>
<p><code>modelr::add_predictions()</code> adds a <code>pred</code> column to each observation.</p>
<pre class="sourceCode r"><code class="sourceCode r">boston <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predictions</span>(boston_lm)</code></pre>
<pre><code>## # A tibble: 506 x 15
##      crim    zn indus  chas   nox    rm   age   dis   rad   tax ptratio
##     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1 0.00632    18  2.31     0 0.538  6.58  65.2  4.09     1   296    15.3
## 2 0.0273      0  7.07     0 0.469  6.42  78.9  4.97     2   242    17.8
## 3 0.0273      0  7.07     0 0.469  7.18  61.1  4.97     2   242    17.8
## 4 0.0324      0  2.18     0 0.458  7.00  45.8  6.06     3   222    18.7
## 5 0.0690      0  2.18     0 0.458  7.15  54.2  6.06     3   222    18.7
## 6 0.0298      0  2.18     0 0.458  6.43  58.7  6.06     3   222    18.7
## # … with 500 more rows, and 4 more variables: black &lt;dbl&gt;, lstat &lt;dbl&gt;,
## #   medv &lt;dbl&gt;, pred &lt;dbl&gt;</code></pre>
<p><code>broom::augment()</code> adds fitted values as well as standard errors and residuals.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">augment</span>(boston_lm)</code></pre>
<pre><code>## # A tibble: 506 x 9
##    medv lstat .fitted .se.fit .resid    .hat .sigma   .cooksd .std.resid
##   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1  24    4.98    29.8   0.406 -5.82  0.00426   6.22 0.00189       -0.939
## 2  21.6  9.14    25.9   0.308 -4.27  0.00246   6.22 0.000582      -0.688
## 3  34.7  4.03    30.7   0.433  3.97  0.00486   6.22 0.00100        0.641
## 4  33.4  2.94    31.8   0.467  1.64  0.00564   6.22 0.000198       0.264
## 5  36.2  5.33    29.5   0.396  6.71  0.00406   6.21 0.00238        1.08 
## 6  28.7  5.21    29.6   0.399 -0.904 0.00413   6.22 0.0000440     -0.146
## # … with 500 more rows</code></pre>
<p>We can use the data from <code>augment()</code> to plot our regression with <code>95%</code> confidence intervals for our predicted values.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">augment</span>(boston_lm) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> lstat)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> medv), <span class="dt">pch=</span><span class="dv">21</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> .fitted)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>.se.fit, <span class="dt">ymax =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>.se.fit), <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>()</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-58-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The fit is decent, but there appears to be some non-linearity in the data.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(boston_lm)</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-59-1.png" width="576" style="display: block; margin: auto;" /><img src="tidy_islr_files/figure-html/unnamed-chunk-59-2.png" width="576" style="display: block; margin: auto;" /><img src="tidy_islr_files/figure-html/unnamed-chunk-59-3.png" width="576" style="display: block; margin: auto;" /><img src="tidy_islr_files/figure-html/unnamed-chunk-59-4.png" width="576" style="display: block; margin: auto;" /></p>
<p>Let’s plot some diagnostics surrounding our model.</p>
<div id="diagnostic-plots" class="section level4">
<h4><span class="header-section-number">3.4.1.1</span> Diagnostic Plots</h4>
<p>We could plot the residual in its original form, but this maintains the scale of the <span class="math inline">\(Y\)</span> response. Two methods exist to scale it in an interpretable fashion, which both scale it using an estimate of the variance of the residual.</p>
<p><em>Standardized residuals</em> scale the residuals in a comparable way by dividing the residual by the regression residual standard error.</p>
<p>However, this allows high-leverage points to be factored in to the standard error. If a particular <span class="math inline">\(y_i\)</span> has high leverage, it will drag the regression line significantly, which affects the estimate of the residual itself.</p>
<p><em>Studentized residuals</em> leave out the residual of <span class="math inline">\(y_i\)</span> when calculating the standard error of a residual for a given observation. Observations with low-leverage will have similar standardized/studentized values, while those with high-leverage will be more affected. By isolating an observation’s leverage from its particular standard error calculation, we can observe how much of an outlier it truly is.</p>
<p>For more context, these class notes from <a href="http://www-stat.wharton.upenn.edu/~waterman/Teaching/701f99/Class04/class04.pdf">Wharton Statistics Department</a> are helpful.</p>
<p><code>augment</code> calculates standardized residuals, but not student. We will add a column to calculate it, as shown in this example doc from <a href="https://twitter.com/jrnld">Jeffrey Arnold</a>, [Outliers and Robust Regression] (<a href="https://uw-pols503.github.io/2016/outliers_robust_regression.html" class="uri">https://uw-pols503.github.io/2016/outliers_robust_regression.html</a>).</p>
<p>In this case, the two types of residuals are virtually the same. I added green points of the standardized residual for comparison. I also scaled the size of points by their leverage (<code>.cooksd</code> column). Finally, I shaded the boundary area between <code>-3</code> and <code>3</code> for the studentized residual. This is a rule of thumb for outlier detection, but observations with magnitude greater than <code>3</code> warrant a model inspection. These could be errors in recording of the data or evidence that the model assumptions are not appropriate.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">augment</span>(boston_lm) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># calculate student residual</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">.student.resid =</span> .resid <span class="op">/</span><span class="st"> </span>.sigma <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>.hat)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> lstat, <span class="dt">y =</span> .student.resid)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> .std.resid, <span class="dt">size=</span>.cooksd), <span class="dt">colour =</span> <span class="st">&quot;khaki2&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="dv">2</span><span class="op">/</span><span class="dv">3</span>, <span class="kw">aes</span>(<span class="dt">size =</span> .cooksd), <span class="dt">colour =</span> <span class="st">&quot;dodgerblue4&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">size =</span> <span class="fl">1.5</span>, <span class="dt">colour =</span> <span class="st">&quot;grey&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> <span class="dv">-3</span>, <span class="dt">ymax =</span> <span class="dv">3</span>),  <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>()</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-60-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>This plot shows a fan-shape for the residuals, which adds evidence of non-linearity in the relationship. We also see high-leverage points on both ends of the x-axis, which significantly drag our regression line. Several points are outside of our outlier zone. All of these above add evidence that our simple linear regression is not appropriate for this dataset.</p>
</div>
</div>
<div id="multiple-linear-regression-1" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Multiple Linear Regression</h3>
<p>Let’s fit a model with all the predictor variables.</p>
<pre class="sourceCode r"><code class="sourceCode r">boston_mlm &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> boston, medv <span class="op">~</span><span class="st"> </span>.)
broom<span class="op">::</span><span class="kw">tidy</span>(boston_mlm, <span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(p.value))</code></pre>
<pre><code>## # A tibble: 14 x 7
##   term   estimate std.error statistic  p.value conf.low conf.high
##   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 age    0.000692   0.0132     0.0524 0.958     -0.0253   0.0266 
## 2 indus  0.0206     0.0615     0.334  0.738     -0.100    0.141  
## 3 chas   2.69       0.862      3.12   0.00193    0.994    4.38   
## 4 tax   -0.0123     0.00376   -3.28   0.00111   -0.0197  -0.00495
## 5 crim  -0.108      0.0329    -3.29   0.00109   -0.173   -0.0434 
## 6 zn     0.0464     0.0137     3.38   0.000778   0.0194   0.0734 
## # … with 8 more rows</code></pre>
<p>How does the model perform?</p>
<pre class="sourceCode r"><code class="sourceCode r">broom<span class="op">::</span><span class="kw">glance</span>(boston_mlm)</code></pre>
<pre><code>## # A tibble: 1 x 11
##   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1     0.741         0.734  4.75      108. 6.72e-135    14 -1499. 3028.
## # … with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;</code></pre>
<p>The VIF for each predictor is mostly low.</p>
<pre class="sourceCode r"><code class="sourceCode r">car<span class="op">::</span><span class="kw">vif</span>(boston_mlm) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>(<span class="dt">rownames =</span> <span class="st">&quot;variable&quot;</span>)</code></pre>
<pre><code>## # A tibble: 13 x 2
##   variable value
##   &lt;chr&gt;    &lt;dbl&gt;
## 1 crim      1.79
## 2 zn        2.30
## 3 indus     3.99
## 4 chas      1.07
## 5 nox       4.39
## 6 rm        1.93
## # … with 7 more rows</code></pre>
<p>To remove predictors, such as those with high p-values, we modify the <code>lm()</code> call. Let’s remove age, which has a p-value of 0.95.</p>
<pre class="sourceCode r"><code class="sourceCode r">boston_mlm &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> boston, medv <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>age)
broom<span class="op">::</span><span class="kw">glance</span>(boston_mlm)</code></pre>
<pre><code>## # A tibble: 1 x 11
##   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1     0.741         0.734  4.74      117. 6.08e-136    13 -1499. 3026.
## # … with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;</code></pre>
<p>We could also incorporate a <code>dplyr::select()</code> call in the <code>data</code> argument for a more readable solution.</p>
<pre class="sourceCode r"><code class="sourceCode r">boston_mlm &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> boston <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>age), medv <span class="op">~</span><span class="st"> </span>.)</code></pre>
</div>
<div id="interaction-terms" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Interaction Terms</h3>
<p>We can incorporate interaction terms in <code>lm()</code> using the <code>term1:term2</code> syntax.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lm</span>(<span class="dt">data =</span> boston, medv <span class="op">~</span><span class="st"> </span>lstat <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>lstat<span class="op">:</span>age)
<span class="co"># equivalent</span>
<span class="kw">lm</span>(<span class="dt">data =</span> boston, medv <span class="op">~</span><span class="st"> </span>lstat<span class="op">*</span>age)</code></pre>
</div>
<div id="non-linear-transformations-of-the-predictors" class="section level3">
<h3><span class="header-section-number">3.4.4</span> Non-linear Transformations of the Predictors</h3>
<p><code>lm()</code> can also accomodate non-linear transformations of the predictors.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> is this best way</span>
lm_boston_squared &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> boston, medv <span class="op">~</span><span class="st"> </span>lstat <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(lstat<span class="op">^</span><span class="dv">2</span>))
lm_boston &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">data =</span> boston, medv <span class="op">~</span><span class="st"> </span>lstat)</code></pre>
<p>We can use <code>anova()</code> with <code>broom::tidy()</code> to measure if it fits better than linear fit.</p>
<pre class="sourceCode r"><code class="sourceCode r">broom<span class="op">::</span><span class="kw">tidy</span>(<span class="kw">anova</span>(lm_boston, lm_boston_squared))</code></pre>
<pre><code>## # A tibble: 2 x 6
##   res.df    rss    df sumsq statistic   p.value
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1    504 19472.    NA   NA        NA  NA       
## 2    503 15347.     1 4125.      135.  7.63e-28</code></pre>
<p>In this case, the full model with the quadratic term fits better.</p>
</div>
</div>
<div id="exercises-1" class="section level2">
<h2><span class="header-section-number">3.5</span> Exercises</h2>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># </span><span class="al">TODO</span><span class="co"> exercises</span></code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["tidy_islr.pdf", "tidy_islr.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
