<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Tidy Introduction To Statistical Learning</title>
  <meta name="description" content="A Tidy Introduction To Statistical Learning">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="A Tidy Introduction To Statistical Learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Tidy Introduction To Statistical Learning" />
  
  
  

<meta name="author" content="Beau Lucas">


<meta name="date" content="2018-06-28">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="statistical-learning.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Tidy Introduction To Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#an-overview-of-statistical-learning"><i class="fa fa-check"></i><b>1.1</b> An Overview of Statistical Learning</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#data-sets-used-in-labs-and-exercises"><i class="fa fa-check"></i><b>1.2</b> Data Sets Used in Labs and Exercises</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#book-website"><i class="fa fa-check"></i><b>1.3</b> Book Website</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-learning.html"><a href="statistical-learning.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#what-is-statistical-learning"><i class="fa fa-check"></i><b>2.1</b> What is Statistical Learning?</a><ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-learning.html"><a href="statistical-learning.html#why-estimate-f"><i class="fa fa-check"></i><b>2.1.1</b> Why Estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-learning.html"><a href="statistical-learning.html#how-do-we-estimate-f"><i class="fa fa-check"></i><b>2.1.2</b> How do we estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-trade-off-between-prediction-accuracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.1.3</b> The Trade-Off Between Prediction Accuracy and Model Interpretability</a></li>
<li class="chapter" data-level="2.1.4" data-path="statistical-learning.html"><a href="statistical-learning.html#supervised-versus-unsupervised-learning"><i class="fa fa-check"></i><b>2.1.4</b> Supervised Versus Unsupervised Learning</a></li>
<li class="chapter" data-level="2.1.5" data-path="statistical-learning.html"><a href="statistical-learning.html#regression-versus-classification-problems"><i class="fa fa-check"></i><b>2.1.5</b> Regression Versus Classification Problems</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.2</b> Assessing Model Accuracy</a><ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.2.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>2.2.2</b> The Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="2.2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-classification-setting"><i class="fa fa-check"></i><b>2.2.3</b> The Classification Setting</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#lab-introduction-to-r"><i class="fa fa-check"></i><b>2.3</b> Lab: Introduction to R</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.1</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.1.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>3.1.1</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="3.1.2" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimate"><i class="fa fa-check"></i><b>3.1.2</b> Assessing the Accuracy of the Coefficient Estimate</a></li>
<li class="chapter" data-level="3.1.3" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-model"><i class="fa fa-check"></i><b>3.1.3</b> Assessing the Accuracy of the Model</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/beaulucas/tidy_islr" target="blank">GitHub Repository</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tidy Introduction To Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Linear Regression</h1>
<hr />
<p>Linear regression is a simple yet very powerful approach in statistical learning. It is important to have a strong understanding of it before moving on to more complex learning methods.</p>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">3.1</span> Simple Linear Regression</h2>
<p>Simple linear regression is predicting a quantitative response <span class="math inline">\(Y\)</span> based off a single predcitor <span class="math inline">\(X\)</span>.</p>
<p>It can be written as below:</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Y \approx \beta_0 + \beta_1X\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>simple linear regression</em>
</p>
</div>
<p><span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> represent the <em>intercept</em> and <em>slope</em> terms and are together known as the <em>coefficients</em>. <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> represent the unknown <em>intercept</em> and <em>slope</em> terms and are together known as the <em>coefficients</em>. We will use our training data to estimate these parameters and thus estimate the response <span class="math inline">\(Y\)</span> based on the value of <span class="math inline">\(X = x\)</span>:</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat y = \hat\beta_0 + \hat\beta_1x\)</span>
</p>
</div>
<div id="estimating-the-coefficients" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Estimating the Coefficients</h3>
<p>We need to use data to estimate these coefficients.</p>
<div>
<p style="text-align:center">
<span class="math inline">\((x_1,y_1), (x_2,y_2),..., (x_n,y_n)\)</span>
</p>
</div>
<p>These represent the training observations, in this case pairs of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> measurements. The goal is to use these measurements to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that the linear model fits our data as close as possible. Measuring <em>closeness</em> can be tackled a number of ways, but <a href="https://en.wikipedia.org/wiki/Least_squares">least squares</a> is the most popular.</p>
<p>If we let <span class="math inline">\(\hat y_i = \hat\beta_0 + \hat\beta_1x_i\)</span> be the prediction of <span class="math inline">\(Y\)</span> at observation <span class="math inline">\(X_i\)</span>, then <span class="math inline">\(e_i = y_i - \hat y_i\)</span> represents the <span class="math inline">\(i\)</span>th <em>residual</em>, the difference between the observed value <span class="math inline">\(y_i\)</span> and the predicted value <span class="math inline">\(\hat y_i\)</span>. Now we can define the <em>residual sum of squares (RSS)</em> as</p>
<div>
<p style="text-align:center">
<span class="math inline">\(RSS = e_1^2 + e_2^2 + ... + e_n^2\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>residual sum of squares</em>
</p>
</div>
<p>or more explicitly as</p>
<div>
<p style="text-align:center">
<span class="math inline">\(RSS = (y_1 - \hat\beta_0 - \hat\beta_1x_2)^2 + (y_2 - \hat\beta_0 - \hat\beta_1x_2)^2 + ... + (y_n - \hat\beta_0 - \hat\beta_1x_n)^2\)</span>
</p>
</div>
<p>Minimizing the RSS (proof can be found <a href="https://en.m.wikipedia.org/wiki/Simple_linear_regression#Derivation_of_simple_regression_estimators">here</a>) using <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> produces:</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\frac{\displaystyle \sum_{i=1}^{n}(x_i-\bar x)(y_i - \bar x)}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2}\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>least squares coefficient estimates (simple linear regression)</em>
</p>
</div>
</div>
<div id="assessing-the-accuracy-of-the-coefficient-estimate" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Assessing the Accuracy of the Coefficient Estimate</h3>
<p>Remember that the true function for <span class="math inline">\(f\)</span> contains a random error term <span class="math inline">\(\epsilon\)</span>. This means the linear relationship can be written as</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Y = \beta_0 + \beta_1X + \epsilon\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>population regression line</em>
</p>
</div>
<p><span class="math inline">\(\beta_0\)</span> is the intercept term (value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X = 0\)</span>). <span class="math inline">\(\beta_1\)</span> is the slope (how much does <span class="math inline">\(Y\)</span> change with one-unit change of <span class="math inline">\(X\)</span>). <span class="math inline">\(\epsilon\)</span> is the error term that captures everything our model doesn’t (unknown variables, measurement error, unknown true relationship).</p>
<p>The population regression line captures the best linear approximation to the true relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In real data, we often don’t know the true relationship and have to rely on a set of observations. Using the observations to estimate the coefficients via least squares produces the <em>least squares line</em>. Let’s simulate and visualize this relationship:</p>
<ul>
<li>simulate <code>n = 200</code> observations</li>
<li>compare the population regression line (<code>sim_y</code>) to a number of possible least squares lines (generated from 10 different training sets of the data)</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># f(x), or Y = 2 + 2x + error</span>

sim_linear &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">b0 =</span> <span class="dv">2</span>,
  <span class="dt">b1 =</span> <span class="dv">2</span>,
  <span class="dt">x =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">200</span>, <span class="dt">mean =</span> <span class="dv">100</span>, <span class="dt">sd =</span> <span class="dv">15</span>),
  <span class="dt">err =</span> <span class="kw">rnorm</span>(<span class="dv">200</span>, <span class="dt">sd =</span> <span class="dv">50</span>),
  <span class="dt">sim_y =</span> b0 <span class="op">+</span><span class="st"> </span>b1 <span class="op">*</span><span class="st"> </span>x,
  <span class="dt">true_y =</span> b0 <span class="op">+</span><span class="st"> </span>b1<span class="op">*</span>x <span class="op">+</span><span class="st"> </span>err
)

<span class="co"># generate 10 training sets</span>
y &lt;-<span class="st"> </span><span class="kw">tibble</span>()
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) {
x &lt;-<span class="st"> </span><span class="kw">sample_frac</span>(sim_linear, <span class="fl">0.1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">iter_set =</span> i)
y &lt;-<span class="st"> </span>y <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_rows</span>(x)
}

<span class="co"># apply linear model to each sample</span>
by_iter &lt;-<span class="st"> </span>y <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(iter_set) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">nest</span>()
lm_model &lt;-<span class="st"> </span><span class="cf">function</span>(df) {
  <span class="kw">lm</span>(true_y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> df)
}
by_iter &lt;-<span class="st"> </span>by_iter <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(data, lm_model),
         <span class="dt">preds =</span> <span class="kw">map2</span>(data, model, add_predictions))

<span class="co"># extract predictions</span>
preds &lt;-<span class="st"> </span><span class="kw">unnest</span>(by_iter, preds)

<span class="kw">ggplot</span>(<span class="dt">data =</span> sim_linear, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> true_y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> preds, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> pred, <span class="dt">colour =</span> iter_set, <span class="dt">group =</span> iter_set), <span class="dt">linetype =</span> <span class="st">&quot;F1&quot;</span>, <span class="dt">size =</span> .<span class="dv">75</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> sim_y), <span class="dt">colour =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">size =</span> <span class="fl">1.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>, <span class="dt">panel.grid.minor =</span> <span class="kw">element_blank</span>(),
        <span class="dt">panel.grid.major =</span> <span class="kw">element_blank</span>(), <span class="dt">axis.line =</span> <span class="kw">element_line</span>(<span class="dt">colour =</span> <span class="st">&quot;grey92&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Each least squares line provides a reasonable estimate&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;y&quot;</span>)</code></pre></div>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-9-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The chart above demonstrates the population regression line (red) surrounded by ten different estimates of the least squares line. Notice how every least squares line (shades of blue) is different. This is because each one is generated from a random sample pulled from the simulated data. For a real-world comparison, the simulated data would be the entire population data which is often impossible to obtain. The observations used to generate the least squares line would be the sample data we have access to. In the same way a sample mean can provide a reasonable estimate of the population mean, fitting a least squares line can provide a reasonable estimate of the population regression line.</p>
<p>This comparison of linear regression to estimating population means touches on the topic of bias. An estimate of <span class="math inline">\(\mu\)</span> using the the sample mean <span class="math inline">\(\hat\mu\)</span> is unbiased. On average, the sample mean will not systemically over or underestimate <span class="math inline">\(\mu\)</span>. If we were to take a large enough estimates of <span class="math inline">\(\mu\)</span>, each produced by a particular set of observations, then this average would exactly equal <span class="math inline">\(\mu\)</span>. This concept applies to our estimates of <span class="math inline">\(\beta_0, \beta_1\)</span> as well.</p>
<p>A question that can be asked is how close on average the sample mean <span class="math inline">\(\hat\mu\)</span> is to <span class="math inline">\(\mu\)</span>. We can compute the <em>standard error</em> of <span class="math inline">\(\hat\mu\)</span> to answer this.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Var(\hat\mu) = SE(\hat\mu)^2 = \sigma^2/n\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>standard error</em>
</p>
</div>
<p>This formula measures the average amount that <span class="math inline">\(\hat\mu\)</span> differs from <span class="math inline">\(\mu\)</span>. As the number of observations <span class="math inline">\(n\)</span> increases, the standard error decreases.</p>
<p>We can also use this to calculate how close <span class="math inline">\(\hat\beta_0, \hat\beta_1\)</span> are to <span class="math inline">\(\beta_0, \beta_1\)</span>.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(SE(\hat\beta_0)^2= \sigma^2 \left[1/n + \frac{\displaystyle \bar x^2}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2} \right]\)</span>
</p>
</div>
<div>
<p style="text-align:center">
<span class="math inline">\(SE(\hat\beta_1)^2=\frac{\displaystyle \sigma^2}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2}\)</span>
</p>
</div>
<p>where <span class="math inline">\(\sigma^2 = Var(\epsilon)\)</span>. For this to work, the assumption has to be made that the error terms <span class="math inline">\(\epsilon_i\)</span> are uncorrelated and all share a common variance. This is often not the case, but it doesn’t mean the formula can’t be used for a decent approximation. <span class="math inline">\(\sigma^2\)</span> is not known, but can be estimated from training observations. This estimate is the <em>residual standard error</em> and is given by formula <span class="math inline">\(RSE = \sqrt{RSS/(n-2}\)</span>.</p>
<p>What can we use these standard error formulas for? A useful technique is to calculate <em>confidence intervals</em> from the standard error. If we wanted to compute a 95% confidence interval for <span class="math inline">\(\beta_0,\beta_1\)</span>, it would take the form below.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat\beta_1 \pm 2 * SE(\hat\beta_1)\)</span>
</p>
</div>
</div>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat\beta_0 \pm 2 * SE(\hat\beta_0)\)</span>
</p>
</div>
</div>
<p>Standard errors can also be used to perform hypotheses tests.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(H_0\)</span>: There is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, or <span class="math inline">\(\beta_1 = 0\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>null hypothesis</em>
</p>
</div>
<div>
<p style="text-align:center">
<span class="math inline">\(H_0\)</span>: There exists a relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, or <span class="math inline">\(\beta_1 \neq 0\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>alternative hypothesis</em>
</p>
</div>
<p>To test the null hypothesis, we need to test whether <span class="math inline">\(\hat\beta_1\)</span> is far enough away from zero to conclude that is it non-zero. How far enough from zero is determined by the value of <span class="math inline">\(\hat\beta_1\)</span> as well as <span class="math inline">\(SE(\hat\beta_1)\)</span>. We compute a <em>t-statistic</em></p>
<div>
<p style="text-align:center">
<span class="math inline">\(t = (\beta_1 - 0)/SE(\hat\beta_1)\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>t-statistic</em>
</p>
</div>
<p>This measures how many standard deviations <span class="math inline">\(\hat\beta_1\)</span> is from 0. If there is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, then <span class="math inline">\(t\)</span> will follow a t-distribution. The t-distribution is similar to the normal distribution, but has slightly heavier tails. Like the normal distribution, we can use this to compute the probability of observing any number equal to or larger than <span class="math inline">\(|t|\)</span>. This probability is the <em>p-value</em>. We can interpret a p-value as the probability we would observe the sample data that produced the <span class="math inline">\(t\)</span>-statistic, given that there is no actual relationship between the predictor <span class="math inline">\(X\)</span> and the response <span class="math inline">\(Y\)</span>. This means that a small p-value supports the inference that there exists a relationship between the predictor and the response. In this case, based on whichever threshold <span class="math inline">\(\alpha\)</span> (common value is 0.05) we set, a small enough p-value would lead us to reject the null hypothesis.</p>
</div>
<div id="assessing-the-accuracy-of-the-model" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Assessing the Accuracy of the Model</h3>
<p>Now that we determined the existence of a relationship, how can we measure how well the model fits the data?</p>
<p>Measuring the quality of a linear regression fit is often handled by two quantities: the <em>residual standard error</em> and the <em>R^2</em> statistic.</p>
<div id="residual-standard-error" class="section level4">
<h4><span class="header-section-number">3.1.3.1</span> Residual Standard Error</h4>
<p>Since every observation has an associated error term <span class="math inline">\(\epsilon\)</span>, having the knowledge of true <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> will still not allow one to perfectly predict <span class="math inline">\(Y\)</span>. The residual standard error estimates the standard deviation of the error term.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(RSE = \sqrt{1/(n-2)*RSS} = \sqrt{1/(n-2)\sum_{i=1}^{n}(y_i - \hat y)^2}\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>residual standard error</em>
</p>
</div>
<p>We can interpret the residual standard error as how much, on average, our predictions deviate from the true value. Whether the value is acceptable in terms of being a successful model depends on the context of the problem. Predicting hardware failure on an airplane would obviously carry much more stringent requirements than predicting the added sales from a change in a company’s advertising budget.</p>
</div>
<div id="r2-statistic" class="section level4">
<h4><span class="header-section-number">3.1.3.2</span> R^2 statistic</h4>
<p>The RSE provides an absolute number. Given that it depends on the scale of <span class="math inline">\(Y\)</span>, comparing RSE values across different domains and datasets isn’t useful. The R^2 statistic solves this problem by measuring in terms of proportion – it measures the variance explained and so always takes a value between 0 and 1.</p>
</div>
<p style="text-align:center">
<span class="math inline">\(R^2 = (TSS - RSS)/TSS = 1 - RSS/TSS\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>R^2 statistic</em>
</p>
</div>
<p>where <span class="math inline">\(TSS = \sum_{i=1}^{n}(y_i-\bar y)^2\)</span> is the <em>total sum of squares</em>. TSS can be thought of the amount of total variability in the response variable before any model is fitted to it. RSS is measured after fitting a model, and measures the amount of unexplained variance remaining in the data. Therefore, R^2 can be thought of as the proportion of variance in the data that is explained by fitting a model with <span class="math inline">\(X\)</span>. While R^2 is more intrepetable, determing what constitutes a R^2 is subjective to the problem. Relationships that are known to be linear with little variance would expect an R^2 very close to 1. In reality, a lot of real-world data is not truly linear and could be heavily influenced by unknown, immeasurable predictors. In such cases a linear approximation would be a rough fit, and a smaller R^2 would not be unexpected.</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-learning.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["tidy_islr.pdf", "tidy_islr.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
