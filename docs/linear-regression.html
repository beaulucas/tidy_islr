<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Linear Regression | A Tidy Introduction To Statistical Learning</title>
  <meta name="description" content="Chapter 3 Linear Regression | A Tidy Introduction To Statistical Learning" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Linear Regression | A Tidy Introduction To Statistical Learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Linear Regression | A Tidy Introduction To Statistical Learning" />
  
  
  

<meta name="author" content="Beau Lucas" />


<meta name="date" content="2019-08-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="statistical-learning.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Tidy Introduction To Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#an-overview-of-statistical-learning"><i class="fa fa-check"></i><b>1.1</b> An Overview of Statistical Learning</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#data-sets-used-in-labs-and-exercises"><i class="fa fa-check"></i><b>1.2</b> Data Sets Used in Labs and Exercises</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#book-website"><i class="fa fa-check"></i><b>1.3</b> Book Website</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-learning.html"><a href="statistical-learning.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#what-is-statistical-learning"><i class="fa fa-check"></i><b>2.1</b> What is Statistical Learning?</a><ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-learning.html"><a href="statistical-learning.html#why-estimate-f"><i class="fa fa-check"></i><b>2.1.1</b> Why Estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-learning.html"><a href="statistical-learning.html#how-do-we-estimate-f"><i class="fa fa-check"></i><b>2.1.2</b> How do we estimate <em><span class="math inline">\(f\)</span></em>?</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-trade-off-between-prediction-accuracy-and-model-interpretability"><i class="fa fa-check"></i><b>2.1.3</b> The Trade-Off Between Prediction Accuracy and Model Interpretability</a></li>
<li class="chapter" data-level="2.1.4" data-path="statistical-learning.html"><a href="statistical-learning.html#supervised-versus-unsupervised-learning"><i class="fa fa-check"></i><b>2.1.4</b> Supervised Versus Unsupervised Learning</a></li>
<li class="chapter" data-level="2.1.5" data-path="statistical-learning.html"><a href="statistical-learning.html#regression-versus-classification-problems"><i class="fa fa-check"></i><b>2.1.5</b> Regression Versus Classification Problems</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#assessing-model-accuracy"><i class="fa fa-check"></i><b>2.2</b> Assessing Model Accuracy</a><ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#measuring-the-quality-of-fit"><i class="fa fa-check"></i><b>2.2.1</b> Measuring the Quality of Fit</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>2.2.2</b> The Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="2.2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#the-classification-setting"><i class="fa fa-check"></i><b>2.2.3</b> The Classification Setting</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#lab-introduction-to-r"><i class="fa fa-check"></i><b>2.3</b> Lab: Introduction to R</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-learning.html"><a href="statistical-learning.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.1</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.1.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>3.1.1</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="3.1.2" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimate"><i class="fa fa-check"></i><b>3.1.2</b> Assessing the Accuracy of the Coefficient Estimate</a></li>
<li class="chapter" data-level="3.1.3" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-model"><i class="fa fa-check"></i><b>3.1.3</b> Assessing the Accuracy of the Model</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>3.2.1</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>3.2.2</b> Some Important Questions</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/beaulucas/tidy_islr" target="blank">GitHub Repository</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tidy Introduction To Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Linear Regression</h1>
<hr />
<p>Linear regression is a simple yet very powerful approach in statistical learning. It is important to have a strong understanding of it before moving on to more complex learning methods.</p>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">3.1</span> Simple Linear Regression</h2>
<p>Simple linear regression is predicting a quantitative response <span class="math inline">\(Y\)</span> based off a single predcitor <span class="math inline">\(X\)</span>.</p>
<p>It can be written as below:</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Y \approx \beta_0 + \beta_1X\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>simple linear regression</em>
</p>
</div>
<p><span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> represent the <em>intercept</em> and <em>slope</em> terms and are together known as the <em>coefficients</em>.
<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> represent the unknown <em>intercept</em> and <em>slope</em> terms and are together known as the <em>coefficients</em>. We will use our training data to estimate these parameters and thus estimate the response <span class="math inline">\(Y\)</span> based on the value of <span class="math inline">\(X = x\)</span>:</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat y = \hat\beta_0 + \hat\beta_1x\)</span>
</p>
</div>
<div id="estimating-the-coefficients" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Estimating the Coefficients</h3>
<p>We need to use data to estimate these coefficients.</p>
<div>
<p style="text-align:center">
<span class="math inline">\((x_1,y_1), (x_2,y_2),..., (x_n,y_n)\)</span>
</p>
</div>
<p>These represent the training observations, in this case pairs of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> measurements. The goal is to use these measurements to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that the linear model fits our data as close as possible. Measuring <em>closeness</em> can be tackled a number of ways, but <a href="https://en.wikipedia.org/wiki/Least_squares">least squares</a> is the most popular.</p>
<p>If we let <span class="math inline">\(\hat y_i = \hat\beta_0 + \hat\beta_1x_i\)</span> be the prediction of <span class="math inline">\(Y\)</span> at observation <span class="math inline">\(X_i\)</span>, then <span class="math inline">\(e_i = y_i - \hat y_i\)</span> represents the <span class="math inline">\(i\)</span>th <em>residual</em>, the difference between the observed value <span class="math inline">\(y_i\)</span> and the predicted value <span class="math inline">\(\hat y_i\)</span>. Now we can define the <em>residual sum of squares (RSS)</em> as</p>
<div>
<p style="text-align:center">
<span class="math inline">\(RSS = e_1^2 + e_2^2 + ... + e_n^2\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>residual sum of squares</em>
</p>
</div>
<p>or more explicitly as</p>
<div>
<p style="text-align:center">
<span class="math inline">\(RSS = (y_1 - \hat\beta_0 - \hat\beta_1x_2)^2 + (y_2 - \hat\beta_0 - \hat\beta_1x_2)^2 + ... + (y_n - \hat\beta_0 - \hat\beta_1x_n)^2\)</span>
</p>
</div>
<p>Minimizing the RSS (proof can be found <a href="https://en.m.wikipedia.org/wiki/Simple_linear_regression#Derivation_of_simple_regression_estimators">here</a>) using <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> produces:</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\frac{\displaystyle \sum_{i=1}^{n}(x_i-\bar x)(y_i - \bar x)}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2}\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>least squares coefficient estimates (simple linear regression)</em>
</p>
</div>
</div>
<div id="assessing-the-accuracy-of-the-coefficient-estimate" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Assessing the Accuracy of the Coefficient Estimate</h3>
<p>Remember that the true function for <span class="math inline">\(f\)</span> contains a random error term <span class="math inline">\(\epsilon\)</span>. This means the linear relationship can be written as</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Y = \beta_0 + \beta_1X + \epsilon\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>population regression line</em>
</p>
</div>
<p><span class="math inline">\(\beta_0\)</span> is the intercept term (value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X = 0\)</span>). <span class="math inline">\(\beta_1\)</span> is the slope (how much does <span class="math inline">\(Y\)</span> change with one-unit change of <span class="math inline">\(X\)</span>). <span class="math inline">\(\epsilon\)</span> is the error term that captures everything our model doesn’t (unknown variables, measurement error, unknown true relationship).</p>
<p>The population regression line captures the best linear approximation to the true relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In real data, we often don’t know the true relationship and have to rely on a set of observations. Using the observations to estimate the coefficients via least squares produces the <em>least squares line</em>. Let’s simulate and visualize this relationship:</p>
<ul>
<li>simulate <code>n = 200</code> observations
<ul>
<li>compare the population regression line (<code>sim_y</code>) to a number of possible least squares lines (generated from 10 different training sets of the data)</li>
</ul></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># f(x), or Y = 2 + 2x + error</span>

sim_linear &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">b0 =</span> <span class="dv">2</span>,
  <span class="dt">b1 =</span> <span class="dv">2</span>,
  <span class="dt">x =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">200</span>, <span class="dt">mean =</span> <span class="dv">100</span>, <span class="dt">sd =</span> <span class="dv">15</span>),
  <span class="dt">err =</span> <span class="kw">rnorm</span>(<span class="dv">200</span>, <span class="dt">sd =</span> <span class="dv">50</span>),
  <span class="dt">sim_y =</span> b0 <span class="op">+</span><span class="st"> </span>b1 <span class="op">*</span><span class="st"> </span>x,
  <span class="dt">true_y =</span> b0 <span class="op">+</span><span class="st"> </span>b1<span class="op">*</span>x <span class="op">+</span><span class="st"> </span>err
)

<span class="co"># generate 10 training sets</span>
y &lt;-<span class="st"> </span><span class="kw">tibble</span>()
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) {
x &lt;-<span class="st"> </span><span class="kw">sample_frac</span>(sim_linear, <span class="fl">0.1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">iter_set =</span> i)
y &lt;-<span class="st"> </span>y <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_rows</span>(x)
}

<span class="co"># apply linear model to each sample</span>
by_iter &lt;-<span class="st"> </span>y <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(iter_set) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">nest</span>()
lm_model &lt;-<span class="st"> </span><span class="cf">function</span>(df) {
  <span class="kw">lm</span>(true_y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> df)
}
by_iter &lt;-<span class="st"> </span>by_iter <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">model =</span> <span class="kw">map</span>(data, lm_model),
         <span class="dt">preds =</span> <span class="kw">map2</span>(data, model, add_predictions))

<span class="co"># extract predictions</span>
preds &lt;-<span class="st"> </span><span class="kw">unnest</span>(by_iter, preds)

<span class="kw">ggplot</span>(<span class="dt">data =</span> sim_linear, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> true_y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> preds, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> pred, <span class="dt">colour =</span> iter_set, <span class="dt">group =</span> iter_set), <span class="dt">linetype =</span> <span class="st">&quot;F1&quot;</span>, <span class="dt">size =</span> <span class="fl">.75</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> sim_y), <span class="dt">colour =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">size =</span> <span class="fl">1.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>, <span class="dt">panel.grid.minor =</span> <span class="kw">element_blank</span>(),
        <span class="dt">panel.grid.major =</span> <span class="kw">element_blank</span>(), <span class="dt">axis.line =</span> <span class="kw">element_line</span>(<span class="dt">colour =</span> <span class="st">&quot;grey92&quot;</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Each least squares line provides a reasonable estimate&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;y&quot;</span>)</code></pre>
<p><img src="tidy_islr_files/figure-html/unnamed-chunk-9-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The chart above demonstrates the population regression line (red) surrounded by ten different estimates of the least squares line. Notice how every least squares line (shades of blue) is different. This is because each one is generated from a random sample pulled from the simulated data. For a real-world comparison, the simulated data would be the entire population data which is often impossible to obtain. The observations used to generate the least squares line would be the sample data we have access to. In the same way a sample mean can provide a reasonable estimate of the population mean, fitting a least squares line can provide a reasonable estimate of the population regression line.</p>
<p>This comparison of linear regression to estimating population means touches on the topic of bias. An estimate of <span class="math inline">\(\mu\)</span> using the the sample mean <span class="math inline">\(\hat\mu\)</span> is unbiased. On average, the sample mean will not systemically over or underestimate <span class="math inline">\(\mu\)</span>. If we were to take a large enough estimates of <span class="math inline">\(\mu\)</span>, each produced by a particular set of observations, then this average would exactly equal <span class="math inline">\(\mu\)</span>. This concept applies to our estimates of <span class="math inline">\(\beta_0, \beta_1\)</span> as well.</p>
<p>A question that can be asked is how close on average the sample mean <span class="math inline">\(\hat\mu\)</span> is to <span class="math inline">\(\mu\)</span>. We can compute the <em>standard error</em> of <span class="math inline">\(\hat\mu\)</span> to answer this.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Var(\hat\mu) = SE(\hat\mu)^2 = \sigma^2/n\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>standard error</em>
</p>
</div>
<p>This formula measures the average amount that <span class="math inline">\(\hat\mu\)</span> differs from <span class="math inline">\(\mu\)</span>. As the number of observations <span class="math inline">\(n\)</span> increases, the standard error decreases.</p>
<p>We can also use this to calculate how close <span class="math inline">\(\hat\beta_0, \hat\beta_1\)</span> are to <span class="math inline">\(\beta_0, \beta_1\)</span>.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(SE(\hat\beta_0)^2= \sigma^2 \left[1/n + \frac{\displaystyle \bar x^2}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2} \right]\)</span>
</p>
</div>
<div>
<p style="text-align:center">
<span class="math inline">\(SE(\hat\beta_1)^2=\frac{\displaystyle \sigma^2}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2}\)</span>
</p>
</div>
<p>where <span class="math inline">\(\sigma^2 = Var(\epsilon)\)</span>. For this to work, the assumption has to be made that the error terms <span class="math inline">\(\epsilon_i\)</span> are uncorrelated and all share a common variance. This is often not the case, but it doesn’t mean the formula can’t be used for a decent approximation. <span class="math inline">\(\sigma^2\)</span> is not known, but can be estimated from training observations. This estimate is the <em>residual standard error</em> and is given by formula <span class="math inline">\(RSE = \sqrt{RSS/(n-2}\)</span>.</p>
<p>What can we use these standard error formulas for? A useful technique is to calculate <em>confidence intervals</em> from the standard error. If we wanted to compute a 95% confidence interval for <span class="math inline">\(\beta_0,\beta_1\)</span>, it would take the form below.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat\beta_1 \pm 2 * SE(\hat\beta_1)\)</span>
</p>
</div>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat\beta_0 \pm 2 * SE(\hat\beta_0)\)</span>
</p>
</div>
<p>Standard errors can also be used to perform hypotheses tests.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(H_0\)</span>: There is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, or <span class="math inline">\(\beta_1 = 0\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>null hypothesis</em>
</p>
</div>
<div>
<p style="text-align:center">
<span class="math inline">\(H_0\)</span>: There exists a relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, or <span class="math inline">\(\beta_1 \neq 0\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>alternative hypothesis</em>
</p>
</div>
<p>To test the null hypothesis, we need to test whether <span class="math inline">\(\hat\beta_1\)</span> is far enough away from zero to conclude that is it non-zero. How far enough from zero is determined by the value of <span class="math inline">\(\hat\beta_1\)</span> as well as <span class="math inline">\(SE(\hat\beta_1)\)</span>. We compute a <em>t-statistic</em></p>
<div>
<p style="text-align:center">
<span class="math inline">\(t = (\beta_1 - 0)/SE(\hat\beta_1)\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>t-statistic</em>
</p>
</div>
<p>This measures how many standard deviations <span class="math inline">\(\hat\beta_1\)</span> is from 0. If there is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, then <span class="math inline">\(t\)</span> will follow a t-distribution. The t-distribution is similar to the normal distribution, but has slightly heavier tails. Like the normal distribution, we can use this to compute the probability of observing any number equal to or larger than <span class="math inline">\(|t|\)</span>. This probability is the <em>p-value</em>. We can interpret a p-value as the probability we would observe the sample data that produced the <span class="math inline">\(t\)</span>-statistic, given that there is no actual relationship between the predictor <span class="math inline">\(X\)</span> and the response <span class="math inline">\(Y\)</span>. This means that a small p-value supports the inference that there exists a relationship between the predictor and the response. In this case, based on whichever threshold <span class="math inline">\(\alpha\)</span> (common value is 0.05) we set, a small enough p-value would lead us to reject the null hypothesis.</p>
</div>
<div id="assessing-the-accuracy-of-the-model" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Assessing the Accuracy of the Model</h3>
<p>Now that we determined the existence of a relationship, how can we measure how well the model fits the data?</p>
<p>Measuring the quality of a linear regression fit is often handled by two quantities: the <em>residual standard error</em> and the <em>R^2</em> statistic.</p>
<div id="residual-standard-error" class="section level4">
<h4><span class="header-section-number">3.1.3.1</span> Residual Standard Error</h4>
<p>Since every observation has an associated error term <span class="math inline">\(\epsilon\)</span>, having the knowledge of true <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> will still not allow one to perfectly predict <span class="math inline">\(Y\)</span>. The residual standard error estimates the standard deviation of the error term.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(RSE = \sqrt{1/(n-2)*RSS} = \sqrt{1/(n-2)\sum_{i=1}^{n}(y_i - \hat y)^2}\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>residual standard error</em>
</p>
</div>
<p>We can interpret the residual standard error as how much, on average, our predictions deviate from the true value. Whether the value is acceptable in terms of being a successful model depends on the context of the problem. Predicting hardware failure on an airplane would obviously carry much more stringent requirements than predicting the added sales from a change in a company’s advertising budget.</p>
</div>
<div id="r2-statistic" class="section level4">
<h4><span class="header-section-number">3.1.3.2</span> R^2 statistic</h4>
<p>The RSE provides an absolute number. Given that it depends on the scale of <span class="math inline">\(Y\)</span>, comparing RSE values across different domains and datasets isn’t useful. The R^2 statistic solves this problem by measuring in terms of proportion – it measures the variance explained and so always takes a value between 0 and 1.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(R^2 = (TSS - RSS)/TSS = 1 - RSS/TSS\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>R^2 statistic</em>
</p>
</div>
<p>where <span class="math inline">\(TSS = \sum_{i=1}^{n}(y_i-\bar y)^2\)</span> is the <em>total sum of squares</em>. TSS can be thought of the amount of total variability in the response variable before any model is fitted to it. RSS is measured after fitting a model, and measures the amount of unexplained variance remaining in the data. Therefore, R^2 can be thought of as the proportion of variance in the data that is explained by fitting a model with <span class="math inline">\(X\)</span>. While R^2 is more intrepetable, determing what constitutes a R^2 is subjective to the problem. Relationships that are known to be linear with little variance would expect an R^2 very close to 1. In reality, a lot of real-world data is not truly linear and could be heavily influenced by unknown, immeasurable predictors. In such cases a linear approximation would be a rough fit, and a smaller R^2 would not be unordinary.</p>
<p>There is a relation between R^2 and the correlation.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(r = Cor(X,Y) = \sum_{i=1}^{n}((x_i-\bar x)(y_i - \bar y))/(\sqrt{\sum_{i=1}^{n}(x_i - \bar x)^2}\sqrt{\sum_{i=1}^{n}(y_i - \bar y)^2})\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>correlation</em>
</p>
</div>
<p>Both measure the linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and within the simple linear regression domain, <span class="math inline">\(r^2 = R^2\)</span>. Once we move into multiple linear regression, in which we are using multiple predictors to predict a response, correlation loses effectiveness at measuring a model in whole as it can only measure the relationship between a single pair of variables.</p>
</div>
</div>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2><span class="header-section-number">3.2</span> Multiple Linear Regression</h2>
<p>Simple linear regression works well when the data involves a single predictor variable. In reality, there are often multiple predictor variables. We will need to extend the simple linear regression model and provide each predictor variable <span class="math inline">\(p\)</span> with a slope coefficient.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>multiple linear regression</em>
</p>
</div>
<div id="estimating-the-regression-coefficients" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Estimating the Regression Coefficients</h3>
<p>Again, we need to estimate the regression coefficients.</p>
<div>
<p style="text-align:center">
<span class="math inline">\(\hat y = \hat\beta_0 + \hat\beta_1X_1 + \hat\beta_2X_2 + ... + \hat\beta_pX_p\)</span>
</p>
</div>
<p>We will utilize the same approach of minimizing the sum of squared residuals (RSS).</p>
<div>
<p style="text-align:center">
<span class="math inline">\(RSS = \sum_{i=1}^{n}(y_i - \hat y_i)^2 = \sum_{i=1}^{n}(y_i - \hat\beta_0 - \hat\beta_1x_{i1} - \hat\beta_2x_{i2} - ... - \hat\beta_px_{ip})^2\)</span>
</p>
</div>
<p>Minimizing these coefficients is more complicated than the simple linear regression setting, and is best represented using linear algebra. See <a href="https://en.wikipedia.org/wiki/Residual_sum_of_squares#Matrix_expression_for_the_OLS_residual_sum_of_squares">this Wikipedia section</a> for more information on the formula.</p>
<p>Interpreting a particular coefficient, (say <span class="math inline">\(\beta_1\)</span>) in a multiple regression model can be thought of as follows: if constant value for all other <span class="math inline">\(\beta_p\)</span> are maintained, what effect would an increase in <span class="math inline">\(beta_1\)</span> have on <span class="math inline">\(Y\)</span>?</p>
<p>A side effect of this is that certain predictors which were deemed significant when contained in a simple linear regression can become insignificant when multiple predictors are involved. For an advertising example, <code>newspaper</code> could be a significant predictor of <code>revenue</code> in the simple linear regression context. However, when combined with <code>tv</code> and <code>radio</code> in a multiple linear regression setting, the effects of increasing <code>newspaper</code> spend while maintaining <code>tv</code> and <code>radio</code> becomes insignificant. This could be due to a correlation of <code>newspaper</code> spend in markets where <code>radio</code> spend is high. Multiple linear regression exposes predictors that act as “surrogates” for others due to correlation.</p>
</div>
<div id="some-important-questions" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Some Important Questions</h3>
<div id="is-there-a-relationship-between-the-response-and-predictors" class="section level4">
<h4><span class="header-section-number">3.2.2.1</span> Is There a Relationship Between the Response and Predictors?</h4>
<p>To check this, we need to check whethere all <span class="math inline">\(p\)</span> coefficients are zero, i.e. <span class="math inline">\(\beta_1 = \beta_2 = ... = \beta_p = 0\)</span>. We test the null hypothesis,</p>
<div>
<p style="text-align:center">
<span class="math inline">\(H_o:\beta_1 = \beta_2 = ... = \beta_p = 0\)</span>
</p>
</div>
<p>against the alternative</p>
<div>
<p style="text-align:center">
<span class="math inline">\(H_a:\)</span> at least one <span class="math inline">\(\beta_j\)</span> is non-zero
</p>
</div>
<p>The hypothesis test is performed by computing the <span class="math inline">\(F-statistic\)</span>,</p>
<div>
<p style="text-align:center">
<span class="math inline">\(F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}\)</span>
</p>
<p class="vocab" style="text-align:right">
<em>correlation</em>
</p>
</div>
<p>If linear model assumptions are correct, one can show that</p>
<div>
<p style="text-align:center">
<span class="math inline">\(E\{RSS/(n-p-1)\} = \sigma^2\)</span>
</p>
</div>
<p>and that, provided <span class="math inline">\(H_o\)</span> is true,</p>
<div>
<p style="text-align:center">
<span class="math inline">\(E\{(TSS-RSS)/p\} = \sigma^2\)</span>
</p>
</div>
<p>In simple terms, if <span class="math inline">\(H_o\)</span> were true and all of the predictors have regression coefficients of 0, we would expect the unexplained variance of the model to be approximately equal to that of the total variance, and both the numerator and the denominator of the F-statistic formula to be equal. When there is no relationship between the response and predictors, the F-statistic will take on a value close to 1. However, as RSS shrinks (the model begins to account for more of the variance), the numerator grows and the denominator shrinks, both causing the F-statistic to increase. We can think of the F-statistic as a ratio between the explained variance and unexplained variance. As the explained variance grows larger than the unexplained portion, the likelihood that we reject the null hypothesis grows.</p>
<p>How large does the F-statistic need to be to reject the null hypothesis? This depends on <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>. As <span class="math inline">\(n\)</span> grows, F-statistics closer to 1 may provide sufficient evidence to reject <span class="math inline">\(H_o\)</span>. If <span class="math inline">\(H_o\)</span> is true and <span class="math inline">\(\epsilon_i\)</span> have a normal distribution, the F-statistic follows an F-distribution. We can compute the p-value for any value of <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> associated with an F-statistic.</p>
<p>Sometimes we want to test whether a particular subset of <span class="math inline">\(q\)</span> of the coefficients are zero.</p>
<p>The null hypothesis could be</p>
<div>
<p style="text-align:center">
<span class="math inline">\(H_o : \beta_{p-q+1} = \beta_{p-q+2} = \beta_p = 0\)</span>
</p>
</div>
<p>In this case we fit a second model that uses all the variables except the last <span class="math inline">\(q\)</span>. We will call the residual sum of squares for the second model <span class="math inline">\(RSS_0\)</span>.</p>
<p>Then, the F-statistic is,</p>
<div>
<p style="text-align:center">
<span class="math inline">\(F = \frac{(RSS_0 - RSS)/q}{RSS(n-p-1)}\)</span>
</p>
</div>
<p>We are testing a model without the <span class="math inline">\(q\)</span> predictors and seeing how it compares to the original model containing all the predictors.</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-learning.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["tidy_islr.pdf", "tidy_islr.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
