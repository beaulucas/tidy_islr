[
["index.html", "A Tidy Introduction To Statistical Learning Preface", " A Tidy Introduction To Statistical Learning Beau Lucas 2020-02-09 Preface This book will serve as a source of notes and exercise solutions for An Introduction to Statistical Learning. My approach will be centered around the tidyverse. This is not a replacement for the book, which should be read front to back by all machine learning enthusiasts. Chapter names will line up, and certain subheadings will also match. Sometimes my notes will contain text lifted straight from the book without modification. This is not an attempt to plagiarize or claim their writing as my own. My goal is for this bookdown project to be a quick stop for machine learning enthusiasts to reference high-level ideas from ISLR in a modern media format. "],
["intro.html", "Chapter 1 Introduction 1.1 An Overview of Statistical Learning 1.2 Data Sets Used in Labs and Exercises 1.3 Book Resources:", " Chapter 1 Introduction 1.1 An Overview of Statistical Learning Statistical learning is focused on supervised and unsupervised modeling and prediction. 1.2 Data Sets Used in Labs and Exercises All data sets used in this book can be found in ISLR and MASS packages, with some also being found in the base R distribution. We will utilize the tidyverse ecosystem to tackle the exercises and labs, as the R code found in the original textbook is outdated. 1.3 Book Resources: The website and free PDF for the book can be found here: Website w/ free PDF YouTube Lectures "],
["statistical-learning.html", "Chapter 2 Statistical Learning 2.1 Packages used in this chapter 2.2 What is Statistical Learning? 2.3 Assessing Model Accuracy 2.4 Lab: Introduction to R 2.5 Exercises", " Chapter 2 Statistical Learning 2.1 Packages used in this chapter library(tidyverse) library(knitr) 2.2 What is Statistical Learning? Methods to estimate functions that connect inputs to outputs. If there exists a quantitative response variable \\(Y\\) and \\(p\\) different predictors (\\(X_1\\), \\(X_2\\), …, \\(X_p\\)), we can write this relationship as: \\(Y = f(X) + ε\\) 2.2.1 Why Estimate \\(f\\)? 2.2.1.1 Prediction We can predict Y using: \\(\\hat{Y} = \\hat{f}(X)\\) Accuracy of \\(Y\\) is dependant on: reducible error \\(\\hat{f}\\) will never be perfect estimate of \\(f\\), and model can always be potentially improved Even if \\(\\hat{f} = f\\), prediction would still have some error irreducible error Because \\(Y\\) is also a function of random \\(ε\\), there will always be variability We cannot reduce the error introduced by \\(ε\\) 2.2.1.2 Inference How does \\(Y\\) respond to changes in \\(X_1, X_2, ..., X_p\\)? 2.2.2 How do we estimate \\(f\\)? Use training data to train method \\(x_ij\\) is value of \\(j\\)th predictor for observation \\(i\\), \\(y_i\\) is value of response variable \\(i = 1, 2, ..., n\\), \\(j = 1, 2, ..., p\\) Using training data, apply statistical learning method estimate unknown function \\(f\\) Most statistical learning methods can be characterized as either parametric or non-parametric 2.2.2.1 Parametric Methods Two-step model-based approach: Make an assumption about functional form of \\(f\\), such as “\\(f\\) is linear in \\(X\\)” Perform procedure that uses training data to train the model * In case of linear model, this procedure estimates parameters \\(β_0, β_1, ..., β_p\\) * Most common approach to fit linear model is (ordinary) least squares This is parametric, as it reduces the problem of estimating \\(f\\) down to one of estimating a set of parameters. Problems that can arise: Model will not match the true unknown form of \\(f\\) If model is made more flexible, which generally requires estimating a greater number of parameters, overfitting can occur 2.2.2.2 Non-parametric Methods Non-parametric methods do not make assumptions about the form of \\(f\\). An advantage of this is that they have the potential to fit a wider range of possible shapes for \\(f\\). A disadvantage is that, because there are no assumptions about the form of \\(f\\), the problem of estimating \\(f\\) is not reduced to a set number of parameters. This means more observations are needed compared to a parametric approach to estimate \\(f\\) accurately. 2.2.3 The Trade-Off Between Prediction Accuracy and Model Interpretability Restrictive models are much more intepretable than flexible ones. Flexible approaches can be so complicated that it is hard to understand how predictors affect the response. If inference is the goal, simple and inflexible methods are easier to interpret. For prediction, accuracy is the biggest concern. However, flexible models are more prone to overfitting. 2.2.4 Supervised Versus Unsupervised Learning Most machine learning methods can be split into supervised or unsupervised categories. Most of this textbook involves supervised learning methods, in which a model that captures the relationship between predictors and response measurements is fitted. The goal is to accurately predict the response variables for future observations, or to understand the relationship between the predictors and response. Unsupervised learning takes place when we have a set of observations and a vector of measurements \\(x_i\\), but no response \\(y_i\\). We can examine the relationship between the variables or between the observations. A popular method of unsupervised learning is cluster analysis, in which observations are grouped into distinct groups based on their vector of measurements \\(x_i\\). An example of this would be a company segmenting survey respondents based on demographic data, in which the goal is to ascertain some idea about potential spending habits without possessing this data. Clustering has some drawbacks. It works best when the groups are significantly distinct from each other. In reality, it is rare for data to exhibit this characteristic. There is often overlap between observations in different groups, and clustering will inevitably place a number of observations in the wrong groups. Further more, visualization of clusters breaks down as the dimensionality of data increases. Most data contains at least several, if not dozens, of variables. It is not always clear-cut whether a problem should be handled with supervised or unsupervised learning. There are some scenarios where only a subset of the observations have response measurements. This is a semi-supervised learning problem, in which a statistical learning method that can utilize all observations is needed. 2.2.5 Regression Versus Classification Problems Variables can be categorized as either quantitative or qualitative. Both qualitative and quantatitive predictors can be used to predict both types of response variables. The more important part of choosing an appropriate statistical learning method is the type of the response variable. 2.3 Assessing Model Accuracy Every data set is different and there is no one statistical learning method that works best for all data sets. It is important for any given data set to find the statistical learning method that produces the best results. This section presents some concepts that are part of that decision-making process. 2.3.1 Measuring the Quality of Fit We need to be able to quantify how well a model’s predictions match the observed data. How close are the model’s predicted response values to the true response values? In regression, mean squared error (MSE) is the most commonly-used measure. A small MSE indicates the predicted responses are very close to the true ones. MSE used on training data is more accurately referred to as the training MSE. We are most concerned with the accuracy of the predictions when we apply our methods to previously unseen data. If you are trying to predict the value of a stock, your concern is how it performs in the future, not on known data from the past. Thus, the goal is then minimizing the test MSE, which measures the accuracy of a model on observations that were not used to train the model. Imagine a set of observations \\((x_0, y_0)\\) that were not used to train the statistical learning method. \\(Ave(y_0 - \\hat{f}(x_0))2\\) The goal is to select the model that minimizes the test MSE shown above. How can we do this? Sometimes, there is an available test data set full of observations that were not used in training the model. The test MSE can be evaluated on these observations, and the learning method which produces the smallest TSE will be chosen. If no test observations are available, picking the method that minimizes the training MSE might seem to be a good idea. However, there is no guarantee that a model with the lowest training MSE also has the lowest test MSE. Models often work in minimizing the training MSE, and can end up with large test MSE. There is a tradeoff in model flexibility, training MSE, and test MSE. A model that is too flexible can closely match the training data, but perform poorly on the test data. There is a sweet spot to find between model flexibility, training MSE, and test MSE that varies for each unique data set. Degrees of freedom is a quantity that summarizes the flexibility of a curve, discused more fully in Chapter 7. The more inflexible a model is, the fewer degrees of freedom. As model flexibility increases, training MSE will inevitably decrease, but test MSE may plateau or even rise. A model with a small training MSE and large test MSE is overfitting the data, picking up patterns on the training data that don’t exist in the test data. Since we expect the training MSE to almost always be lower than the test MSE, overfitting is a specific case when there exists a less flexible model with a smaller test MSE. 2.3.2 The Bias-Variance Trade-Off The expected test MSE can be broken down into the sum of three quantities: the variance of \\(\\hat{f}(x_0)\\) the squared bias of \\(\\hat{f}(x_0)\\) the variance of the error terms ε \\(E(y_0 - \\hat{f}(x_0)^2 = Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2+Var(ε)\\) The formula above defines the expected test MSE, which can be thought of the average test MSE that would be obtained if we repeatedly estimated \\(f\\) and tested each at \\(x_0\\). To minimize expected test MSE, we need to choose a statistical learning method that achieves both low variance and low bias. Since variance and squared bias are nonnegative, the expected test MSE can never be lower than \\(Var(ε)\\), the irreducible error. Variance refers to how much \\(\\hat{f}\\) would change if repeatedly estimated with different training data sets. Methods with high variance can produce large changes in \\(\\hat{f}\\) through small changes in the training data. Generally, the more flexible a model it is, the higher the variance. Following the observations so closely can cause changes in just a single observation of the training data to result in significant changes to \\(\\hat{f}\\). More inflexible models, such as linear regression, are less susceptible to the effects of changing a single observation. Bias is the error introduced from approximating a complicated problem by a much simpler model. Fitting a linear regression to data that is not linear will always lead to high bias, no matter how many observations are in the training set. More flexible models tend to result in less bias. More flexible methods lead to higher variance and lower bias. The rate of change between the quantities determines at which point the test MSE is minimized. Bias tends to decrease at a faster rate in the beginning, causing the test MSE to decline. However, when flexibility reaches a certain point, variance will begin to increase faster than bias is decreasing, causing test MSE to rise. This relationship between bias, variance, and test MSE is known as the bias-variance tradeoff. Here is a good article on it: Understanding the Bias-Variance Tradeoff In real-life scenarios where \\(f\\) is unknown, we cannot explicitly compute the test MSE, bias, or variance. However, there are methods to estimate this, such as cross-validation, which will be discussed in Chapter 5. 2.3.3 The Classification Setting For classification problems where \\(y_i,...,y_n\\) are qualitative, we can quantify the accuracy of our estimate by using the training error rate, the proportion of mistakes that are made when applying our model \\(\\hat{f}\\) to the training observations. \\(1/n\\sum_{i=1}^nI(y_i \\neq \\hat{y_i})\\) training error rate Breaking the formula above down. \\(\\hat{y}_i\\) is the predicted class label for the \\(i\\)th observation using \\(\\hat{f}\\) \\(I(y_i \\neq \\hat{y_i})\\) is an indicator variable that equals 1 if \\(y_i \\neq \\hat{y_i}\\), and 0 if \\(y_i = \\hat{y_i}\\) If \\(I(y_i \\neq \\hat{y_i})\\) = 0, then the \\(i\\)th observation was classified correctly Similar to our regression problems, we are more interested in the model’s performance on test observations not used in training. The formula below gives us the test error rate for a set of observations of the form \\((x_0, y_0)\\). \\(Ave(I(y_0 \\neq \\hat{y_0}))\\) test error rate A good classifier will minimize the above. 2.3.3.1 The Bayes Classifier The test error rate is minimized by the classifier that assigns each observation to the most likely class, given its predictor values. Our decision is then based on finding the value at which the formula below is largest. \\(Pr(Y = j|X = x_0)\\) If the response values are binomial (let’s call them A and B) the classifier simplifies to: \\(Pr(Y = A|X = x_0) &gt; 0.5\\:then\\:A,\\:else\\:B\\) The Bayes decision boundary is the point where the probabilities are equal for both groups. Points on either side of this line are assigned to the group predicted by the classifier. The Bayes error rate averaged over all possible values of \\(X\\) is below. \\(1-E(max_jPr(Y = j|X))\\) Bayes error rate The Bayes error rate is often greater than zero, as observations between classes overlap in real-world data. 2.3.3.2 K-Nearest Neighbors Since the true conditional distribution of \\(Y\\) given \\(X\\) cannot be known in real data, the Bayes classifier is used as a “gold standard” to compare other models to. Many methods attempt to estimate this conditional distribution, and then classify an observation based on the estimated probability. A common method is K-nearest neighbors (KNN). Given a positive integer \\(K\\) and a test observation \\(x_0\\), KNN then does the following: identifies the \\(K\\) points in the training data that are closest to \\(x_0\\), represented by \\(N_0\\) estimates conditional probability for class \\(j\\) as the fraction of the points in \\(N_0\\) whose response values equal \\(j\\): \\(Pr(Y = j| X = x_0) = 1/K\\sum_{i\\in N_0}I(y_i = j)\\) applies Bayes rule and classifies test observation \\(x_0\\) to class with largest probability KNN can be surprisingly robust to the optimal Bayes classifier. The choice in \\(K\\) makes a huge difference. For example, a \\(K\\) = 1 is highly flexible, classifying observations based off of the closest nearby training observation. \\(K\\) = 100 would do the opposite, basing its classification off a large pool of training observations compared to the \\(K\\) = 1 version. The higher \\(K\\) value produces a more linear model. The trade-off between flexibility, training error rate, and test error rate applies to both classification and regression problems. 2.4 Lab: Introduction to R Finally we get to some R code. This chapter of ISLR introduces basic R syntax, and most of it is unchanged in my version. This should all be familiar to anyone who has used R before. We are going to be working with tibbles as our primary data structure throughout this book. Please read here: tibbles 2.4.0.1 Basic Commands Skipping this. 2.4.0.2 Graphics (Plotting) Here we begin to explore the “tidy” approach to R. We will abstain from base R plotting and use ggplot2, which is a more powerful tool. Let’s plot a scatterplot with some basic labels. tbl_rnorm &lt;- tibble( x1 = rnorm(100), y1 = rnorm(100) ) ggplot(tbl_rnorm, aes(x = x1, y = y1)) + geom_point() + labs(title = &quot;plot of x1 vs. y1&quot;, x = &quot;this is the x-axis&quot;, y = &quot;this is the y-axis&quot;) 2.4.0.3 Indexing data We will skip this. 2.4.0.4 Loading data ISLR mentions insuring proper working directory before loading data. Dealing with working directories in R is a bad idea. Fortunately, it’s easily avoidable through the use of RStudio projects, which keep all files used in analysis together and make your work more robust and reproducible. See the RStudio Projects chapter in r4ds for more information. We will opt for the readr (part of the tidyverse) package instead of base R. Take a look at this subsection of r4ds for reasons why: 11.2.1 Compared to base R Below is a reproducible example in which we create a tibble, save it as a .txt file, and then read it in with write_tsv(). The set of read_* functions in readr will be the standrad way to read local files into R. If you are using RStudio projects, there is no need to worry about working directories. # generate dummy data to read in generic_company_tibble &lt;- tibble( x = rnorm(100, mean = 25), y = rnorm(100, mean = 50), z = sample(c(&quot;apple&quot;, &quot;uber&quot;, &quot;facebook&quot;, &quot;twitter&quot;, &quot;tesla&quot;, &quot;google&quot;, &quot;microsoft&quot;), 1) ) tmp &lt;- tempfile() write_tsv(generic_company_tibble, tmp) company_data &lt;- read_tsv(tmp) readr provides a nice summary of the imported tibble. Calling the tibble by name will also give a breakdown of column names, data types, and number of observations. company_data ## # A tibble: 100 x 3 ## x y z ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 27.1 49.9 microsoft ## 2 25.5 50.8 microsoft ## 3 25.0 51.1 microsoft ## 4 25.7 49.0 microsoft ## 5 27.1 49.5 microsoft ## 6 23.1 49.3 microsoft ## # … with 94 more rows 2.4.0.5 Additional Graphical and Numerical Summaries ISLR mentions the attach() function, which allows R to reference column names of dataframes without specifying the dataframe. attach can lead to confusion and errors when working on a project with multiple sources of data. This is a bad practice, and should always be avoided. The book then goes into some explanation of plot(), which we will not be using. 2.5 Exercises For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. The sample size n is extremely large, and the number of predictors p is small (better) given large sample size, a flexible model would be able to capture a trend without being influenced too heavily by a small number of observations. The number of predictors p is extremely large, and the number of observations n is small. (worse) given the small sample size, an inflexible model would do better at not overfitting to a small number of observations (capturing patterns in the training data that dont really exist) The relationship between the predictors and response is highly non-linear. (better) highly flexible methods are highly non-linear and can produce better fits on non-linear data compared to inflexible methods such as linear regression The variance of the error terms, i.e. σ2 = Var(ε), is extremely high. (worse) given the high variance in the data, an inflexible method would overfit to the noise Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p. We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary. (regression; inference) This is a regression problem with both qualitative and quantitative predictors. Inference is the main goal, as the company probably wants a model that is human-readable in order to understand what determines a CEO’s salary. n = 500, p = 3 We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables. (classification; prediction) The goal is to classify whether a product will be a success or failure. Prediction is the goal, as they want to accurately determine if their product will succeed or fail. n = 20, p = 4 We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market. (regression; prediction) The goal is to predict the % change of the exchange rate. We now revisit the bias-variance decomposition Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one. bias_variance &lt;- tibble( flexibility = c(1:5), bias = c(300,200,150,100,50), variance = c(0,25,125,250,500), train_error = c(350,250,200, 125, 50), irreducible_error = 100, test_error = variance + bias + irreducible_error) %&gt;% gather(`bias`, `variance`, `train_error`, `irreducible_error`, `test_error`, key = &quot;measurement&quot;, value = &quot;value&quot;) ggplot(bias_variance, aes(x = flexibility, y = value, colour = measurement)) + geom_smooth(se = FALSE, method = &quot;lm&quot;, formula = y ~ poly(x,3)) + theme_minimal() You will now think of some real-life applications for statistical learning. Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer. predicting diabetes. response: future diabetes predictors: health and body measurements of patient goal: prediction, model complexity and human understanding is not important demographics that determine future education level response: education level predictors: demographic data goal: inference, prediction is important here too but researchers would probably want to understand and share which factors determine the response in order to raise awareness faulty parts in manufacturing response: whether or not part is faulty predictors: various tests on part goal: prediction, it is most important to have an accurate model, especially if faulty parts can lead to deaths Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer. number of riders on public transit over time response: how many riders are expected to use public transit predictors: current transit usage data, local population data, etc. goal: prediction, it is important to prepare for growth in transit usage so governments have enough time to make necessary changes demand for product response: how many units to expect to be sold predictors: current demand data, revenue growth, business expansion, changing in market trends, economy health goal: prediction, figure out in X amount of time demand for product in order to upsize/downsize to appropriate level determine future salary response: future expeceted salary predictors: employment history, education, geolocation, etc. goal: inference, researchers might want to know the most important factors that lead to higher salaries rather than a model that is too complex to understand What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred? Very flexible approach allows you to fit a more flexible function to the data. The advantage is that you have the potential to accurately predict data even as it moves away from linearity. The disadvantage are potential overfitting to the training data, increasing variance (individual observations affect the model to higher degree than non-flexible counterpart), and higher computational costs (as well as less human-readable explanations) When a more flexible approach is preferred: data that is non-linear, large sample size, prediction more important than inference When a less flexible approach is preferred: data that is more linear, smaller sample size, inference more important than prediction Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a nonparametric approach)? What are its disadvantages? A parametric approach assumes a form of \\(f\\) and has to estimate an often known number of parameters (for example, linear regression simply requires estimating \\(p+1\\) coefficients) A non-parametric approach makes no assumptions about the true form of \\(f\\). They simply want to get as close as possible to \\(f\\). This allows them to take on a larger variety of shapes, and accomodate a larger variety of patterns. Advantages of a parametric approach are that they take less observations to generate (the problem is reduced to applying a known form to \\(f\\), such as linear regression), are less inclined to overfit, and generally less computationally intensive. Disadvantages of a parametric approach making assumptions about the form of \\(f\\), which may not match the real form and could lead to a model that doesn’t fit the data well The table below provides a training data set containing six observa- tions, three predictors, and one qualitative response variable. training_set &lt;- tibble( x1 = c(0,2,0,0,-1,1), x2 = c(3,0,1,1,0,1), x3 = c(0,0,3,2,1,1), y = c(&quot;red&quot;,&quot;red&quot;,&quot;red&quot;,&quot;green&quot;,&quot;green&quot;,&quot;red&quot;)) kable(training_set) x1 x2 x3 y 0 3 0 red 2 0 0 red 0 1 3 red 0 1 2 green -1 0 1 green 1 1 1 red Suppose we wish to use this data set to make a prediction for \\(Y\\) when \\(X1 = X2 = X3 = 0\\) using K-nearest neighbors. Compute the Euclidean distance between each observation and the test point, \\(X1 = X2 = X3 = 0\\). The Euclidean Distance for three dimensions can be written as: \\(d = \\sqrt {\\left( {x_1 - x_2 } \\right)^2 + \\left( {y_1 - y_2 } \\right)^2 + \\left( {z_1 - z_2 } \\right)^2 }\\) Let’s write a function that can handle this in R, then use the rowwise() feature of dplyr to apply it across the rows of our tibble. euc_dist &lt;- function(x1, x2) sqrt(sum((x1 - x2) ^ 2)) training_set &lt;- training_set %&gt;% rowwise() %&gt;% mutate(distance = euc_dist(c(x1,x2,x3), c(0,0,0))) %&gt;% ungroup() kable(training_set) x1 x2 x3 y distance 0 3 0 red 3.000000 2 0 0 red 2.000000 0 1 3 red 3.162278 0 1 2 green 2.236068 -1 0 1 green 1.414214 1 1 1 red 1.732051 What is our prediction with K = 1? Why? Let’s find the y value of the single closest (k = 1) training observation. training_set %&gt;% filter(distance == min(distance)) %&gt;% select(y) %&gt;% pull() ## [1] &quot;green&quot; Since the closest observation in the training data is green, K = 1 classifies our test observation as green. What is our prediction with K = 3? Why? First we find the three closest values. Then we measure the breakdown of y responses in this group of three observations. We find that two observations have value of red, and one has green. Given red has the highest probability of the two y values, we assign the training observation as red. training_set %&gt;% top_n(3, -distance) %&gt;% mutate(n = n()) %&gt;% group_by(y) %&gt;% summarise(prop = n()/max(n)) %&gt;% filter(prop == max(prop)) %&gt;% pull(y) ## [1] &quot;red&quot; If Bayes decision boundary is highly non-linear, then do we expect the best value of \\(K\\) to be large or small? Why? - We expect the value of \\(K\\) to decline as the decision boundary grows more non-linear. A smaller value of \\(K\\) is more suspectible to small changes between observations, which is the type of pattern highly non-linear decision boundary would depict. The R exercises are pretty basic after this. I am going to skip them for now. "],
["linear-regression.html", "Chapter 3 Linear Regression 3.1 Packages used in this chapter 3.2 Simple Linear Regression 3.3 Multiple Linear Regression 3.4 Lab: Linear Regression 3.5 Exercises", " Chapter 3 Linear Regression Linear regression is a simple yet very powerful approach in statistical learning. It is important to have a strong understanding of it before moving on to more complex learning methods. 3.1 Packages used in this chapter library(tidyverse) library(modelr) library(knitr) library(kableExtra) 3.2 Simple Linear Regression Simple linear regression is predicting a quantitative response \\(Y\\) based off a single predcitor \\(X\\). It can be written as below: \\(Y \\approx \\beta_0 + \\beta_1X\\) simple linear regression \\(\\beta_0\\) and \\(\\beta_1\\) represent the intercept and slope terms and are together known as the coefficients. \\(\\beta_0\\) and \\(\\beta_1\\) represent the unknown intercept and slope terms and are together known as the coefficients. We will use our training data to estimate these parameters and thus estimate the response \\(Y\\) based on the value of \\(X = x\\): \\(\\hat y = \\hat\\beta_0 + \\hat\\beta_1x\\) 3.2.1 Estimating the Coefficients We need to use data to estimate these coefficients. \\((x_1,y_1), (x_2,y_2),..., (x_n,y_n)\\) These represent the training observations, in this case pairs of \\(X\\) and \\(Y\\) measurements. The goal is to use these measurements to estimate \\(\\beta_0\\) and \\(\\beta_1\\) such that the linear model fits our data as close as possible. Measuring closeness can be tackled a number of ways, but least squares is the most popular. If we let \\(\\hat y_i = \\hat\\beta_0 + \\hat\\beta_1x_i\\) be the prediction of \\(Y\\) at observation \\(X_i\\), then \\(e_i = y_i - \\hat y_i\\) represents the \\(i\\)th residual, the difference between the observed value \\(y_i\\) and the predicted value \\(\\hat y_i\\). Now we can define the residual sum of squares (RSS) as \\(RSS = e_1^2 + e_2^2 + ... + e_n^2\\) residual sum of squares or more explicitly as \\(RSS = (y_1 - \\hat\\beta_0 - \\hat\\beta_1x_2)^2 + (y_2 - \\hat\\beta_0 - \\hat\\beta_1x_2)^2 + ... + (y_n - \\hat\\beta_0 - \\hat\\beta_1x_n)^2\\) Minimizing the RSS (proof can be found here) using \\(\\beta_0\\) and \\(\\beta_1\\) produces: \\(\\frac{\\displaystyle \\sum_{i=1}^{n}(x_i-\\bar x)(y_i - \\bar x)}{\\displaystyle\\sum_{i=1}^{n}(x_i - \\bar x)^2}\\) least squares coefficient estimates (simple linear regression) 3.2.2 Assessing the Accuracy of the Coefficient Estimate Remember that the true function for \\(f\\) contains a random error term \\(\\epsilon\\). This means the linear relationship can be written as \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\) population regression line \\(\\beta_0\\) is the intercept term (value of \\(Y\\) when \\(X = 0\\)). \\(\\beta_1\\) is the slope (how much does \\(Y\\) change with one-unit change of \\(X\\)). \\(\\epsilon\\) is the error term that captures everything our model doesn’t (unknown variables, measurement error, unknown true relationship). The population regression line captures the best linear approximation to the true relationship between \\(X\\) and \\(Y\\). In real data, we often don’t know the true relationship and have to rely on a set of observations. Using the observations to estimate the coefficients via least squares produces the least squares line. Let’s simulate and visualize this relationship: simulate n = 200 observations compare the population regression line (sim_y) to a number of possible least squares lines (generated from 10 different training sets of the data) # f(x), or Y = 2 + 2x + error sim_linear &lt;- tibble( b0 = 2, b1 = 2, x = 1:100 + rnorm(n = 200, mean = 100, sd = 15), err = rnorm(200, sd = 50), sim_y = b0 + b1 * x, true_y = b0 + b1 * x + err ) # generate 10 training sets y &lt;- tibble() for (i in 1:10) { x &lt;- sample_frac(sim_linear, 0.1) %&gt;% mutate(iter_set = i) y &lt;- y %&gt;% bind_rows(x) } # apply linear model to each sample by_iter &lt;- y %&gt;% group_by(iter_set) %&gt;% nest() lm_model &lt;- function(df) { lm(true_y ~ x, data = df) } by_iter &lt;- by_iter %&gt;% mutate( model = map(data, lm_model), preds = map2(data, model, add_predictions) ) # extract predictions preds &lt;- unnest(by_iter, preds) ggplot(data = sim_linear, aes(x = x, y = true_y)) + geom_point(alpha = 1 / 3) + geom_line(data = preds, aes(x = x, y = pred, colour = iter_set, group = iter_set), linetype = &quot;F1&quot;, size = .75) + geom_line(aes(y = sim_y), colour = &quot;red&quot;, size = 1.5) + theme_minimal() + theme( legend.position = &quot;none&quot;, panel.grid.minor = element_blank(), panel.grid.major = element_blank(), axis.line = element_line(colour = &quot;grey92&quot;) ) + labs( title = &quot;Each least squares line provides a reasonable estimate&quot;, y = &quot;y&quot; ) The chart above demonstrates the population regression line (red) surrounded by ten different estimates of the least squares line. Notice how every least squares line (shades of blue) is different. This is because each one is generated from a random sample pulled from the simulated data. For a real-world comparison, the simulated data would be the entire population data which is often impossible to obtain. The observations used to generate the least squares line would be the sample data we have access to. In the same way a sample mean can provide a reasonable estimate of the population mean, fitting a least squares line can provide a reasonable estimate of the population regression line. This comparison of linear regression to estimating population means touches on the topic of bias. An estimate of \\(\\mu\\) using the the sample mean \\(\\hat\\mu\\) is unbiased. On average, the sample mean will not systemically over or underestimate \\(\\mu\\). If we were to take a large enough estimates of \\(\\mu\\), each produced by a particular set of observations, then this average would exactly equal \\(\\mu\\). This concept applies to our estimates of \\(\\beta_0, \\beta_1\\) as well. A question that can be asked is how close on average the sample mean \\(\\hat\\mu\\) is to \\(\\mu\\). We can compute the standard error of \\(\\hat\\mu\\) to answer this. \\(Var(\\hat\\mu) = SE(\\hat\\mu)^2 = \\sigma^2/n\\) standard error This formula measures the average amount that \\(\\hat\\mu\\) differs from \\(\\mu\\). As the number of observations \\(n\\) increases, the standard error decreases. We can also use this to calculate how close \\(\\hat\\beta_0, \\hat\\beta_1\\) are to \\(\\beta_0, \\beta_1\\). \\(SE(\\hat\\beta_0)^2= \\sigma^2 \\left[1/n + \\frac{\\displaystyle \\bar x^2}{\\displaystyle\\sum_{i=1}^{n}(x_i - \\bar x)^2} \\right]\\) \\(SE(\\hat\\beta_1)^2=\\frac{\\displaystyle \\sigma^2}{\\displaystyle\\sum_{i=1}^{n}(x_i - \\bar x)^2}\\) where \\(\\sigma^2 = Var(\\epsilon)\\). For this to work, the assumption has to be made that the error terms \\(\\epsilon_i\\) are uncorrelated and all share a common variance. This is often not the case, but it doesn’t mean the formula can’t be used for a decent approximation. \\(\\sigma^2\\) is not known, but can be estimated from training observations. This estimate is the residual standard error and is given by formula \\(RSE = \\sqrt{RSS/(n-2}\\). What can we use these standard error formulas for? A useful technique is to calculate confidence intervals from the standard error. If we wanted to compute a 95% confidence interval for \\(\\beta_0,\\beta_1\\), it would take the form below. \\(\\hat\\beta_1 \\pm 2 * SE(\\hat\\beta_1)\\) \\(\\hat\\beta_0 \\pm 2 * SE(\\hat\\beta_0)\\) Standard errors can also be used to perform hypotheses tests. \\(H_0\\): There is no relationship between \\(X\\) and \\(Y\\), or \\(\\beta_1 = 0\\) null hypothesis \\(H_0\\): There exists a relationship between \\(X\\) and \\(Y\\), or \\(\\beta_1 \\neq 0\\) alternative hypothesis To test the null hypothesis, we need to test whether \\(\\hat\\beta_1\\) is far enough away from zero to conclude that is it non-zero. How far enough from zero is determined by the value of \\(\\hat\\beta_1\\) as well as \\(SE(\\hat\\beta_1)\\). We compute a t-statistic \\(t = (\\beta_1 - 0)/SE(\\hat\\beta_1)\\) t-statistic This measures how many standard deviations \\(\\hat\\beta_1\\) is from 0. If there is no relationship between \\(X\\) and \\(Y\\), then \\(t\\) will follow a t-distribution. The t-distribution is similar to the normal distribution, but has slightly heavier tails. Like the normal distribution, we can use this to compute the probability of observing any number equal to or larger than \\(|t|\\). This probability is the p-value. We can interpret a p-value as the probability we would observe the sample data that produced the \\(t\\)-statistic, given that there is no actual relationship between the predictor \\(X\\) and the response \\(Y\\). This means that a small p-value supports the inference that there exists a relationship between the predictor and the response. In this case, based on whichever threshold \\(\\alpha\\) (common value is 0.05) we set, a small enough p-value would lead us to reject the null hypothesis. 3.2.3 Assessing the Accuracy of the Model Now that we determined the existence of a relationship, how can we measure how well the model fits the data? Measuring the quality of a linear regression fit is often handled by two quantities: the residual standard error and the R^2 statistic. 3.2.3.1 Residual Standard Error Since every observation has an associated error term \\(\\epsilon\\), having the knowledge of true \\(\\beta_0\\) and \\(\\beta_1\\) will still not allow one to perfectly predict \\(Y\\). The residual standard error estimates the standard deviation of the error term. \\(RSE = \\sqrt{1/(n-2)*RSS} = \\sqrt{1/(n-2)\\sum_{i=1}^{n}(y_i - \\hat y)^2}\\) residual standard error We can interpret the residual standard error as how much, on average, our predictions deviate from the true value. Whether the value is acceptable in terms of being a successful model depends on the context of the problem. Predicting hardware failure on an airplane would obviously carry much more stringent requirements than predicting the added sales from a change in a company’s advertising budget. 3.2.3.2 R^2 statistic The RSE provides an absolute number. Given that it depends on the scale of \\(Y\\), comparing RSE values across different domains and datasets isn’t useful. The R^2 statistic solves this problem by measuring in terms of proportion – it measures the variance explained and so always takes a value between 0 and 1. \\(R^2 = (TSS - RSS)/TSS = 1 - RSS/TSS\\) R^2 statistic where \\(TSS = \\sum_{i=1}^{n}(y_i-\\bar y)^2\\) is the total sum of squares. TSS can be thought of the amount of total variability in the response variable before any model is fitted to it. RSS is measured after fitting a model, and measures the amount of unexplained variance remaining in the data. Therefore, R^2 can be thought of as the proportion of variance in the data that is explained by fitting a model with \\(X\\). While R^2 is more intrepetable, determing what constitutes a R^2 is subjective to the problem. Relationships that are known to be linear with little variance would expect an R^2 very close to 1. In reality, a lot of real-world data is not truly linear and could be heavily influenced by unknown, immeasurable predictors. In such cases a linear approximation would be a rough fit, and a smaller R^2 would not be unordinary. There is a relation between R^2 and the correlation. \\(r = Cor(X,Y) = \\sum_{i=1}^{n}((x_i-\\bar x)(y_i - \\bar y))/(\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar x)^2}\\sqrt{\\sum_{i=1}^{n}(y_i - \\bar y)^2})\\) correlation Both measure the linear relationship between \\(X\\) and \\(Y\\), and within the simple linear regression domain, \\(r^2 = R^2\\). Once we move into multiple linear regression, in which we are using multiple predictors to predict a response, correlation loses effectiveness at measuring a model in whole as it can only measure the relationship between a single pair of variables. 3.3 Multiple Linear Regression Simple linear regression works well when the data involves a single predictor variable. In reality, there are often multiple predictor variables. We will need to extend the simple linear regression model and provide each predictor variable \\(p\\) with a slope coefficient. \\(Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon\\) multiple linear regression 3.3.1 Estimating the Regression Coefficients Again, we need to estimate the regression coefficients. \\(\\hat y = \\hat\\beta_0 + \\hat\\beta_1X_1 + \\hat\\beta_2X_2 + ... + \\hat\\beta_pX_p\\) We will utilize the same approach of minimizing the sum of squared residuals (RSS). \\(RSS = \\sum_{i=1}^{n}(y_i - \\hat y_i)^2 = \\sum_{i=1}^{n}(y_i - \\hat\\beta_0 - \\hat\\beta_1x_{i1} - \\hat\\beta_2x_{i2} - ... - \\hat\\beta_px_{ip})^2\\) Minimizing these coefficients is more complicated than the simple linear regression setting, and is best represented using linear algebra. See this Wikipedia section for more information on the formula. Interpreting a particular coefficient, (say \\(\\beta_1\\)) in a multiple regression model can be thought of as follows: if constant value for all other \\(\\beta_p\\) are maintained, what effect would an increase in \\(beta_1\\) have on \\(Y\\)? A side effect of this is that certain predictors which were deemed significant when contained in a simple linear regression can become insignificant when multiple predictors are involved. For an advertising example, newspaper could be a significant predictor of revenue in the simple linear regression context. However, when combined with tv and radio in a multiple linear regression setting, the effects of increasing newspaper spend while maintaining tv and radio becomes insignificant. This could be due to a correlation of newspaper spend in markets where radio spend is high. Multiple linear regression exposes predictors that act as “surrogates” for others due to correlation. 3.3.2 Some Important Questions 3.3.2.1 Is There a Relationship Between the Response and Predictors? To check this, we need to check whethere all \\(p\\) coefficients are zero, i.e. \\(\\beta_1 = \\beta_2 = ... = \\beta_p = 0\\). We test the null hypothesis, \\(H_o:\\beta_1 = \\beta_2 = ... = \\beta_p = 0\\) against the alternative \\(H_a:\\) at least one \\(\\beta_j\\) is non-zero The hypothesis test is performed by computing the \\(F-statistic\\), \\(F = \\frac{(TSS-RSS)/p}{RSS/(n-p-1)}\\) correlation If linear model assumptions are correct, one can show that \\(E\\{RSS/(n-p-1)\\} = \\sigma^2\\) and that, provided \\(H_o\\) is true, \\(E\\{(TSS-RSS)/p\\} = \\sigma^2\\) In simple terms, if \\(H_o\\) were true and all of the predictors have regression coefficients of 0, we would expect the unexplained variance of the model to be approximately equal to that of the total variance, and both the numerator and the denominator of the F-statistic formula to be equal. When there is no relationship between the response and predictors, the F-statistic will take on a value close to 1. However, as RSS shrinks (the model begins to account for more of the variance), the numerator grows and the denominator shrinks, both causing the F-statistic to increase. We can think of the F-statistic as a ratio between the explained variance and unexplained variance. As the explained variance grows larger than the unexplained portion, the likelihood that we reject the null hypothesis grows. How large does the F-statistic need to be to reject the null hypothesis? This depends on \\(n\\) and \\(p\\). As \\(n\\) grows, F-statistics closer to 1 may provide sufficient evidence to reject \\(H_o\\). If \\(H_o\\) is true and \\(\\epsilon_i\\) have a normal distribution, the F-statistic follows an F-distribution. We can compute the p-value for any value of \\(n\\) and \\(p\\) associated with an F-statistic. # TODO remove this section? Sometimes we want to test whether a particular subset of \\(q\\) of the coefficients are zero. The null hypothesis could be \\(H_o : \\beta_{p-q+1} = \\beta_{p-q+2} = \\beta_p = 0\\) In this case we fit a second model that uses all the variables except the last \\(q\\). We will call the residual sum of squares for the second model \\(RSS_0\\). Then, the F-statistic is, \\(F = \\frac{(RSS_0 - RSS)/q}{RSS(n-p-1)}\\) We are testing a model without the \\(q\\) predictors and seeing how it compares to the original model containing all the predictors. Why do we need to look at overall F-statistics if we have individual p-values of the predictors? There are scenarios where individual predictors, by chance, will have small p-values, even in the absence of any true association. This could lead us to incorrectly diagnose a relationship. The overall F-statistic does not suffer this problem because it adjusts for the number of predictors. The F-statistic approach works when the number of predictors \\(p\\) is small compared to \\(n\\). Sometimes, we have situations where \\(p &gt; n\\). In this situation, ther eare more coefficients \\(\\beta_j\\) to estimate than observations from which to estimate them. Such situations requires different approaches that we haven’t discussed yet (see chapter 6) # TODO add chapter 6 link 3.3.2.2 Deciding on Important Variables The first thing we do in a multiple regression is to compute the F-statistic and determine that at least one of the predictors is related to the response. The task of determining which predictors are associated with the response is referred to as variable selection. We could try out a lot of different models with combinations of predictors, \\(2^p\\), but this is not practical as \\(p\\) grows. There are three ways to approach this task: Forward selection: we begin with the null model, which contains an intercept but no predictors. We then fit \\(p\\) simple linear regressions and add to the null model the variable that results in the lowest RSS. We then repeat the process to determine the lowest RSS of the now two-variable model, continuing until some stopping rule is satisfied. Backward selection: Start with all the variables in the model, remove the variable with the largest p-value. Then, for the new \\((p - 1)\\)-variable model, do the same. Continue until stopping rule is reached (for example, some p-value threshold) Mixed selection: Start with no variables, and proceed with forward selection. If any p-value of added variables pass a threshold once new predictors are added, we remove them. We continue the forward and backward until all variables in model have a sufficiently low p-value. 3.3.2.3 Model Fit Two common methods of model fit are the \\(RSE\\) and \\(R^2\\), the fraction of variance explained. More on \\(R^2\\): Values closer to 1 indicate a better fit Adding more variables can only increase it Adding variables that barely increase it can lead to overfitting Plotting the model can also be useful. 3.3.2.4 Predictions Three sorts of uncertainty within a given model: The coefficient estimates \\(\\hat\\beta_0 + \\hat\\beta_1...,\\hat\\beta_p\\) are estimates for \\(\\beta_0 + \\beta_1...,\\beta_p\\). This inaccuracy is part of the reducible error. We can compute a confidence interval to determine how close \\(\\hat Y\\) is to \\(f(X)\\). Model bias can result from the fact that we are fitting a linear approximation to the true surface of \\(f(X)\\). Even if we knew \\(f(X)\\), we still have random error \\(\\epsilon\\), which is the irreducible error. We can use prediction intervals to estimate how far \\(Y\\) will differ from \\(\\hat Y\\). These will always be larger than confidence intervals, because they incorporate both the reducible + irreducible error. 3.3.3 Other Considerations in the Regression Model So far, all predictors have been quantitative. However, it is common to have qualitative variables as well. Take a look at the ISLR::Credit dataset, which has a mix of both types. tidy_credit &lt;- ISLR::Credit %&gt;% as_tibble() %&gt;% janitor::clean_names() tidy_credit ## # A tibble: 400 x 12 ## id income limit rating cards age education gender student married ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 14.9 3606 283 2 34 11 &quot; Mal… No Yes ## 2 2 106. 6645 483 3 82 15 &quot;Fema… Yes Yes ## 3 3 105. 7075 514 4 71 11 &quot; Mal… No No ## 4 4 149. 9504 681 3 36 11 &quot;Fema… No No ## 5 5 55.9 4897 357 2 68 16 &quot; Mal… No Yes ## 6 6 80.2 8047 569 4 77 10 &quot; Mal… No No ## # … with 394 more rows, and 2 more variables: ethnicity &lt;fct&gt;, balance &lt;int&gt; 3.3.3.1 Predictors with only Two Levels Suppose we wish to investigate difference in credit card balance between males and females, ignoring all other variables. If a qualitative variable (also known as a factor) only has two possible values, then incorporating it into a model is easy. We can create a binomial dummy variable that takes on two values. For gender, this could be a variable that is 0 if observation has value male, and 1 if observation has value female. This variable can then be used in the regression equation. Take note that lm() automatically creates dummy variables when given qualitative predictors. # TODO insert regression equation credit_model &lt;- lm(balance ~ gender, data = tidy_credit) tidy_credit_model &lt;- broom::tidy(credit_model) tidy_credit_model ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 510. 33.1 15.4 2.91e-42 ## 2 genderFemale 19.7 46.1 0.429 6.69e- 1 How to interpret this: males are estimated to carry a balance of $510. Meanwhile, females are expected to carry an additional \\(19.70\\) in debt. Notice the p-value is very high, indicating there is no significant difference between genders. 3.3.4 Qualitative Predictors with More than Two Levels A single dummy variable can not represent all the possible values. We can create additional dummy variables for this. Let’s make a dummy variable from ethnicity column, which takes three distinct values. This will yield two dummy variables. tidy_credit %&gt;% distinct(ethnicity) ## # A tibble: 3 x 1 ## ethnicity ## &lt;fct&gt; ## 1 Caucasian ## 2 Asian ## 3 African American fastDummies package will be used to generate these. In this case, African American serves as the baseline, and dummy variables are created for Caucasian and Asian. There will always be one fewer dummy variable than the number of levels. tidy_credit_dummy &lt;- tidy_credit %&gt;% fastDummies::dummy_cols(select_columns = &quot;ethnicity&quot;, remove_first_dummy = TRUE) %&gt;% janitor::clean_names() tidy_credit_dummy %&gt;% select(starts_with(&quot;ethnicity&quot;)) ## # A tibble: 400 x 3 ## ethnicity ethnicity_asian ethnicity_caucasian ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 Caucasian 0 1 ## 2 Asian 1 0 ## 3 Asian 1 0 ## 4 Asian 1 0 ## 5 Caucasian 0 1 ## 6 Caucasian 0 1 ## # … with 394 more rows We can again run the model with newly created dummy variables. Keep in mind, prior creation is not necessary, as lm will generate them automatically. ethnicity_model &lt;- lm(balance ~ ethnicity_asian + ethnicity_caucasian, data = tidy_credit_dummy) broom::tidy(ethnicity_model) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 531. 46.3 11.5 1.77e-26 ## 2 ethnicity_asian -18.7 65.0 -0.287 7.74e- 1 ## 3 ethnicity_caucasian -12.5 56.7 -0.221 8.26e- 1 3.3.5 Extensions of the Linear Model The linear regression model makes highly restrictive assumptions. Two of the most important are that the relationship between predictors and response are additive and linear. Additive means that the effect of changes in a predictor \\(X_j\\) on the response \\(Y\\) is independet of the values of the other predictors. Linear means that the the change in response \\(Y\\) to a one-unit change in \\(X_j\\) is constant, regardless of the value of \\(X_j\\). Here are some common approaches of extending the linear model. 3.3.5.1 Removing the Additive Assumption The additive property assumes that predictors slope terms are independent of the values of other predictors. However, this is not always the case. Imagine an advertising scenario where the effectiveness of TV spend is affected by the radio spend. This is known as an interaction effect. Imagine we have a model with two predictors, but they are not strictly additive. We could extend this model by adding an interaction term to it. \\(Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1X_2 + \\epsilon\\) Now, the effect of \\(X_1\\) on \\(Y\\) is no longer constant; adjusting \\(X_2\\) will change the impact of \\(X_1\\) on \\(Y\\). An easy scenario is the productivity of a factory. Adding lines and workers both would increase productivity. However, the effect is not purely additive. Adding lines without having workers to operate them would not increase productivity. There is an interaction between workers and lines that needs to be accounted for. The hierarchical principle states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant. It’s also possible for qualitative and quantitative variables to interact with each other. We will again use the Credit data set. Suppose we wish to predict balance using the income (quantitative) and student (qualitative) variables. First, let’s take a look at what it looks like to fit this model without an interaction term. Both income and student are significant. lm_credit &lt;- lm(balance ~ income + student, data = tidy_credit) lm_credit %&gt;% broom::tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 211. 32.5 6.51 2.34e-10 ## 2 income 5.98 0.557 10.8 7.82e-24 ## 3 studentYes 383. 65.3 5.86 9.78e- 9 tidy_credit %&gt;% modelr::add_predictions(lm_credit) %&gt;% ggplot(aes(x = income, y = pred, colour = student)) + geom_line(size = 1.5) + geom_point(aes(y = balance, colour = student), fill = &quot;grey&quot;, pch = 21, alpha = 1 / 2) + theme_minimal() It’s a pretty good fit, and because there is no interaction terms, the lines are parallel. Notice how many more observations there are to fit on for the non-students. Now, let’s add an interaction term. lm_credit_int &lt;- lm(balance ~ income + student + income * student, data = tidy_credit) lm_credit_int %&gt;% broom::tidy() ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 201. 33.7 5.95 5.79e- 9 ## 2 income 6.22 0.592 10.5 6.34e-23 ## 3 studentYes 477. 104. 4.57 6.59e- 6 ## 4 income:studentYes -2.00 1.73 -1.15 2.49e- 1 tidy_credit %&gt;% modelr::add_predictions(lm_credit_int) %&gt;% ggplot(aes(x = income, y = pred, colour = student)) + geom_line(size = 1.5) + geom_point(aes(y = balance, colour = student), fill = &quot;grey&quot;, pch = 21, alpha = 1 / 2) + theme_minimal() The model now takes into account how income and student interact with each other. Interpreting the chart suggests that increases in income among students has a smaller effect on balance than it does to non-students. Does it fit better? models &lt;- list(without_interaction = lm_credit, with_interaction = lm_credit_int) purrr::map_df(models, broom::glance, .id = &quot;model&quot;) %&gt;% select(model, r.squared, statistic, p.value, df) ## # A tibble: 2 x 5 ## model r.squared statistic p.value df ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 without_interaction 0.277 76.2 9.64e-29 3 ## 2 with_interaction 0.280 51.3 4.94e-28 4 Not by much. The model with the interaction term has a slightly higher \\(R^2\\), but the added complexity of the model, combined with the small number of observations of students in the dataset, suggests overfitting. 3.3.5.2 Non-linear Relationships The linear model assumes a linear relationship between the response and predictors. We can extend the linear model to accomodate non-linear relationships using polynomial regression. A way to incorporate non-linear associations into a linear model is to include transformed versions of the predictor in the model. For example, within the Auto dataset, predicting mpg with a second-order polynomial of horsepower would look like this: \\(mpg = \\beta_0 + \\beta_1horsepower + \\beta_2horsepower^2 + \\epsilon\\) Let’s look at the Auto dataset with models of different polynomial degrees overlaid. Clearly, the data is not linear, exhibiting a quadratic shape. tidy_auto &lt;- ISLR::Auto %&gt;% as_tibble() lm_auto &lt;- lm(mpg ~ horsepower, data = tidy_auto) lm_auto2 &lt;- lm(mpg ~ poly(horsepower, 2), data = tidy_auto) lm_auto5 &lt;- lm(mpg ~ poly(horsepower, 5), data = tidy_auto) tidy_auto %&gt;% gather_predictions(lm_auto, lm_auto2, lm_auto5) %&gt;% ggplot(aes(x = horsepower, y = mpg)) + geom_point(alpha = 1 / 3, pch = 21) + geom_line(aes(y = pred, colour = model), size = 1.5) The second-order polynomial does a good job of fitting the data, while the fifth-order seems to be unnecessary. The model performance reflects that: models &lt;- list( linear = lm_auto, second_order = lm_auto2, fifth_order = lm_auto5 ) purrr::map_df(models, broom::glance, .id = &quot;model&quot;) %&gt;% select(model, r.squared, statistic, p.value, df) ## # A tibble: 3 x 5 ## model r.squared statistic p.value df ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 linear 0.606 600. 7.03e-81 2 ## 2 second_order 0.688 428. 5.40e-99 3 ## 3 fifth_order 0.697 177. 1.16e-97 6 The second-order model has significantly higher R^2, and only one more degree of freedom. This approach of extending linear models to accomodate non-linear relationships is known as polynomial regression. 3.3.6 Potential Problems Many problems can occur when fitting a linear model to a data set. Non-linearity of the response-predictor relationship. Correlation of error terms. Non-constant variance of error terms Outliers High-leverage points Collinearity 3.3.6.1 1. Non-linearity of the data The linear model assumes a straight-line relationship between the predictors and the response. If this is not the case, the inference and prediction accuracy of the fit are suspect. We can use a residual plot to visualize when a linear model is placed on to a non-linear relationship. For a simple linear regression model, we plot the residuals \\(e_i = y_i - \\hat{y}_i\\) compared to the predictor. For multiple regression, we plot the residuals versus the predicted values \\(\\hat{y}_i\\). If the relationship is linear, the residuals should exhibit a random pattern. Let’s take the Auto dataset and plot the residuals compared to horsepower for each model we fit. Notice in the model containing no quadratic term is U-shaped, indicating a non-linear relationship. The model that contains horsepower^2 exhibits little pattern in the residuals, indicating a better fit. tidy_auto %&gt;% gather_predictions(lm_auto, lm_auto2, lm_auto5) %&gt;% ggplot(aes(x = horsepower, y = mpg-pred, colour=model)) + geom_point(alpha = 1 / 3) + geom_hline(yintercept = 0, size = 1.5, colour=&quot;grey&quot;) + facet_wrap(~model, nrow=3) If the residual plot indicates that there non-linear associations in the data, a simple approach is to use non-linear transofmrations of the predictors. 3.3.6.2 2. Correlation of Error Terms The linear regression model assumes that the error terms \\(\\epsilon_1,\\epsilon_2,...,\\epsilon_n\\) are uncorrelated. This means that for a given error term \\(e_i\\), no information is provided about the value \\(e_{i+1}\\). The standard errors that are computed for the estimated regression coefficients are based on this assumption. If there is a correlation among the error terms, than the estimated standard errors will tend to underestimate the true standard errors, producing confidence and prediction intervals narrower than they should be. Given the incorrect assumption, a 95% confidence interval may have a much lower probability than 0.95 of containing the true value of the parameter. P-values would also be lower than they should be, giving us an unwarranted sense of confidence in our model. These correlations occur frequently in time series data, which consists of observations obtained at discrete points in time. In many cases, observations that are obtained at adjacent time periods points will have positively correlated errors. We can again plot the residuals as a function of time to see if this is the case. If no correlation, there should be no pattern in the residuals. If error terms exhibit correlation, we may see that adjacent residuals exhibit similar values, known as tracking. This can also happen outside of time series data. The assumption of uncorrelated errors is extremely important for linear regression as well as other statistical methods. # TODO add a residual plot for time series with correlated error terms, similar to pg. 95 tidy_sunspot &lt;- data.frame(y=as.matrix(sunspot.year), ds=time(sunspot.year)) %&gt;% as_tibble() tidy_sunspot sunspot_lm &lt;- lm(data = tidy_sunspot, y ~ ds) tidy_sunspot %&gt;% add_predictions(sunspot_lm) %&gt;% ggplot(aes(x=ds, y=pred-y)) + geom_point() + geom_line() + geom_smooth(method=&quot;lm&quot;, se = FALSE) 3.3.6.3 3. Non-constant Variance of Error Terms Another assumption of linear regression is that the error terms have a constant variance, \\(Var(\\epsilon_i) = \\sigma^2\\). Standard errors, confidence intervals, and hypothesis tests rely upon this assumption. It is common for error terms to exhiti non-constant variance. Non-constant variance in the errors, also known as heteroscedasticity, can be identified from a funnel shape in the residual plot. Let’s take a look at the MASS::cats dataset, which contains observations of various cats sex, body weight, and heart weight. We fit a linear model to it, and then plot the residuals. I’ve added a linear fit to the residual plot itself. Observe how the error terms begin to funnel out as bwt increases, indicating non-constant variance of the error terms. tidy_cats &lt;- MASS::cats %&gt;% as_tibble() %&gt;% janitor::clean_names() lm_cats &lt;- lm(data = tidy_cats, hwt ~ bwt) tidy_cats %&gt;% add_predictions(lm_cats) %&gt;% ggplot(aes(x = bwt, y = hwt - pred)) + geom_point() + geom_smooth(method=&quot;lm&quot;, level = 0.99) # TODO add OLS method to this # get weights of each response # fit a linear model on the residuals tidy_cats_res &lt;- tidy_cats %&gt;% add_predictions(lm_cats) %&gt;% mutate(res = hwt - pred) %&gt;% select(bwt, hwt, res) lm_cats_res &lt;- lm(data = tidy_cats_res, res ~ hwt) cat_weights &lt;- tidy_cats_res %&gt;% add_predictions(lm_cats_res) %&gt;% mutate(res_var = (res-pred)^2) %&gt;% mutate(weight = 1 / res_var) %&gt;% pull(weight) lm_cats_weights &lt;- lm(data = tidy_cats, hwt ~ bwt, weights = cat_weights) tidy_cats %&gt;% gather_predictions(lm_cats, lm_cats_weights) %&gt;% ggplot(aes(x = bwt, y = hwt - pred, colour = model)) + geom_point(aes(y = hwt-pred, colour = model)) + geom_smooth(method=&quot;lm&quot;) When this occurs, there a few ways to remedy it. You could transform the response \\(Y\\) using a function such as \\(logY\\) or \\(\\sqrt{Y}\\). If we have a good idea of the variance of each response, we could fit our model using weighted least squares, which weights proportional to the inverse of the expected variance of an observation. 3.3.6.4 4. Outliers An outlier is a point for which \\(y_i\\) is far from the value predicted by the model. These can arise for a variety of reasons, such as incorrect recording of an observation during data collection. For the msleep dataset below, which contains data on mammal sleep durations, I’ve highlighted two observations that most would consider outliers. This is for the African elephant and Asian elephant mammals, who’s bodyweights are far and away from the rest of mammals. There are others that could be considered outliers as well. Identifying outliers is an often arbitrary process. msleep %&gt;% ggplot(aes(x = awake, y = bodywt, colour = name)) + geom_point() + gghighlight::gghighlight(bodywt&gt;2000) Let’s fit a linear model to predict body weight from how long the animal is awake. Notice how the data that maintains the elephant observations significantly affects the slope, drawing the regression line away from the majority of observations. lm_sleep &lt;- lm(data = msleep, bodywt ~ awake) lm_sleep_filtered &lt;- lm(data = msleep %&gt;% filter(!name %in% c(&#39;African elephant&#39;, &#39;Asian elephant&#39;)), bodywt ~ awake) msleep %&gt;% gather_predictions(lm_sleep, lm_sleep_filtered) %&gt;% ggplot(aes(x = awake, y = bodywt)) + geom_point(pch=21, alpha = 1/3) + geom_line(aes(y = pred, colour = model), size = 1.5) + scale_colour_viridis_d() + theme_minimal() The model excluding the elephant observations has a significantly higher \\(R^2\\), which indicates a better fit. models &lt;- list(with_outliers = lm_sleep, without_outliers = lm_sleep_filtered) purrr::map_df(models, broom::glance, .id = &quot;model&quot;) %&gt;% select(model, r.squared, statistic, p.value, df) ## # A tibble: 2 x 5 ## model r.squared statistic p.value df ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 with_outliers 0.0973 8.73 0.00409 2 ## 2 without_outliers 0.222 22.5 0.00000900 2 Another way of handling an outlier is transforming the response variable. Upon inspection of the scatterplot, it becomes clear that the relationship between bodywt and awake is not linear. If we take the same dataset and apply a log function to response variable bodywt, we see that the outliers no longer exists. msleep %&gt;% ggplot(aes(x = awake, y = log(bodywt))) + geom_smooth(method = &quot;lm&quot;) + geom_point() The model that uses log(bodywt) as the response also has better performance than both models above. lm_sleep_log &lt;- lm(data = msleep, log(bodywt) ~ awake) lm_sleep_log %&gt;% broom::glance() %&gt;% select(r.squared, statistic, p.value, df) ## # A tibble: 1 x 4 ## r.squared statistic p.value df ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0.324 38.7 0.0000000202 2 3.3.6.5 5. High Leverage Points Observations with high leverage have an unusual value for \\(x_i\\). In a simple regression model, these are practically the same as outliers (I could have flipped predictors with response in my mammal sleep model above). High leverage points are observations that significantly move the regression line. However, in multiple regression, it is possible to have an observation that is well within the range of each individual predictor’s values, but unusual terms of the full set of predictors. Let’s look at temp and ozone data from airquality dataset. Plenty of observations have ozone &gt; 100 and temp &lt; 80, but the combination is rare. If we fit models with and without this leverage point, we can see how the regression line moves. tidy_airquality &lt;- airquality %&gt;% as_tibble() %&gt;% janitor::clean_names() tidy_airquality %&gt;% ggplot(aes(x = ozone, y = temp)) + geom_point() + gghighlight::gghighlight(ozone &gt; 100 &amp; temp &lt; 80) lm_temp &lt;- lm(data = tidy_airquality, temp ~ ozone) lm_temp_filtered &lt;- lm(data = tidy_airquality %&gt;% filter(!(ozone &gt; 100 &amp; temp &lt; 80)), temp ~ ozone) Notice the slight change in the fit. This observation isn’t that extreme, but still produces a visible difference in fit. More extreme observations would move this line even further, potentially causing an improper fit. tidy_airquality %&gt;% gather_predictions(lm_temp, lm_temp_filtered) %&gt;% ggplot(aes(x = ozone, y = pred, colour = model)) + geom_line() + geom_point(aes(y = temp), alpha = 1/3, colour = &quot;grey&quot;) The model without the strange observation performs slightly better. In reality, I would probably include it as going from an \\(R^2\\) of 0.488 to 0.506 isn’t worth the manual effort. models &lt;- list(with_leverage = lm_temp, without_leverage = lm_temp_filtered) purrr::map_df(models, broom::glance, .id = &quot;model&quot;) %&gt;% select(model, r.squared, statistic, p.value, df) ## # A tibble: 2 x 5 ## model r.squared statistic p.value df ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 with_leverage 0.488 109. 2.93e-18 2 ## 2 without_leverage 0.506 116. 5.05e-19 2 We can also quantify an observation’s leverage by computing the \\(leverage statistic\\). A large value of this statistic indicates an observation with high leverage. For a simple linear regression, \\(h_i = \\frac{1}{n} + \\frac{x_i - \\bar{x}^2}{\\sum_{i&#39;=1}^{n}}\\) leverage statistic As \\(x_i\\) increases from \\(\\bar{x}\\), \\(h_i\\) increases. Combined with a high residual (outlier), a high-leverage observation could be dangerous. 3.3.6.6 6. Colinearity Collinearity is when two or more predictor variables are closely related to one another. If we look at our tidy_credit tibble, we see that rating and limit are very highly correlated with one another. tidy_credit %&gt;% ggplot(aes(x = rating, y = limit)) + geom_point() The effects of collinearity can make it difficult to separate out the individual effects of collinear variables on the response. Since limit and rating move together, it can be hard to determine how each one separately is associated with the response, balance. # TODO add contour plot # clean up what this link did: https://yetanotheriteration.netlify.com/2018/01/high-collinearity-effect-in-regressions/ Two multiple regression models show the effects of regressing balance on age + limit versus rating + limit. Notice the large p-value for limit, combined with the &gt;10x increase in standard error for its coefficient compared to the original model. It’s effects have been masked by the collinearity it shares with rating. lm_age_limit &lt;- lm(data = tidy_credit, balance ~ age + limit) lm_rating_limit &lt;- lm(data = tidy_credit, balance ~ rating + limit) models &lt;- list(`Balance ~ Age + Limit` = lm_age_limit, `Balance ~ Rating + Limit` = lm_rating_limit) purrr::map_df(models, broom::tidy, .id = &quot;model&quot;) ## # A tibble: 6 x 6 ## model term estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Balance ~ Age + Limit (Intercept) -173. 43.8 -3.96 9.01e- 5 ## 2 Balance ~ Age + Limit age -2.29 0.672 -3.41 7.23e- 4 ## 3 Balance ~ Age + Limit limit 0.173 0.00503 34.5 1.63e-121 ## 4 Balance ~ Rating + Limit (Intercept) -378. 45.3 -8.34 1.21e- 15 ## 5 Balance ~ Rating + Limit rating 2.20 0.952 2.31 2.13e- 2 ## 6 Balance ~ Rating + Limit limit 0.0245 0.0638 0.384 7.01e- 1 The growth in the standard error caused by collinearity caises the t-statistic (\\(B_j\\) divided by it’s standard error) to decline. As a result, we may fail to reject \\(H_0: \\beta_j = 0\\). This means that the power of the hypothesis test – the probability of correctly detecting a non-zero coefficient – is reduced by collinearity. Thus, it is desirable to identify and address collinearity problems while fitting the model. A simple way is to look at the correlation matrix of the predictors. library(corrr) tidy_credit_corr &lt;- tidy_credit %&gt;% select_if(is.numeric) %&gt;% select(-id) %&gt;% corrr::correlate() %&gt;% corrr:::stretch() %&gt;% filter(!is.na(r)) # remove duplicate combinations of x, y tidy_credit_corr &lt;- tidy_credit_corr %&gt;% mutate(sort_var = map2_chr(x, y, ~toString(sort(c(.x, .y))))) %&gt;% distinct(sort_var, .keep_all = TRUE) %&gt;% select(-sort_var) tidy_credit_corr %&gt;% arrange(desc(abs(r))) %&gt;% filter(abs(r) &gt; 0.05) %&gt;% kable() %&gt;% kableExtra::kable_styling() x y r limit rating 0.9968797 rating balance 0.8636252 limit balance 0.8616973 income limit 0.7920883 income rating 0.7913776 income balance 0.4636565 income age 0.1753384 rating age 0.1031650 limit age 0.1008879 cards balance 0.0864563 rating cards 0.0532390 cards education -0.0510842 Here we can see the strongest correlation is between limit and rating, rating and balance, which makes sense. Card issuers give higher limit to those with higher rating, and rating is directly affted by balance. As we move down the table, weaker correlations appear, such as cards and education. Such tables allow one to identify potential collinearity problems between two predictors. However, it is possible for collinearity to exist between three or more variables even if there is no correlation between any of the pairs in that group. This is called multicollinearity. In this case, we compute the variance inflation factor (VIF). The VIF is the ratio of the variance of \\(\\hat\\beta_j\\) when fitting the full model divided by the variance of \\(\\hat\\beta_j\\) if fit on its own. VIF can also be thought of as checking if a predictor can be explained by all the other predictors in the dataset. In this case, that predictor provides no novel information, but can add a significant amount of variance. Such predictors with high VIF are a candidate for removal from the model. Here is a video (Python) explaining how to use VIF to tackle multicollinearity: Variance Inflation Factor (VIF) for Detecting Multicolinearity in Python A rule of thumb is that a VIF value exceeding 10 indicates a problematic amount of collinearity. The VIF for each variable can be computed using the formula \\(VIF(\\hat\\beta_j) = \\frac{1}{1-R^2_{X_j|X_-j}}\\) leverage statistic where \\(R^2_{X_j|X_-j}\\) is the \\(R^2\\) from a regression of \\(X_j\\) onto all of the other predictors. If a variable \\(X_j\\) has a high \\(R^2\\) when regressed onto all other predictor variables (other predictor variables can explain a large amount of \\(X_j\\)’s variance), the VIF will be high. In R, we can use the car package to calculate GVIF for a lm object. car::vif(lm(data = tidy_credit %&gt;% select(-id), balance ~ .)) %&gt;% as_tibble(rownames = &quot;variable&quot;) %&gt;% arrange(desc(GVIF)) ## # A tibble: 10 x 4 ## variable GVIF Df `GVIF^(1/(2*Df))` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 rating 236. 1 15.4 ## 2 limit 234. 1 15.3 ## 3 income 2.79 1 1.67 ## 4 cards 1.45 1 1.20 ## 5 age 1.05 1 1.03 ## 6 married 1.04 1 1.02 ## # … with 4 more rows As we can see here, the most problematic variables are rating and limit, which both exceed VIFs of 200. It seems that rating and limit can be explained by the other predictors in the dataset. There are two simple solutions for this. The first is to drop problematic variables from the regression. Let’s drop rating and see what happens. car::vif(lm(data = tidy_credit %&gt;% select(-id, -rating), balance ~ .)) %&gt;% as_tibble(rownames = &quot;variable&quot;) %&gt;% arrange(desc(GVIF)) ## # A tibble: 9 x 4 ## variable GVIF Df `GVIF^(1/(2*Df))` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 income 2.77 1 1.67 ## 2 limit 2.71 1 1.65 ## 3 age 1.05 1 1.03 ## 4 married 1.03 1 1.02 ## 5 ethnicity 1.03 2 1.01 ## 6 student 1.02 1 1.01 ## # … with 3 more rows Just like that, limit goes from VIF &gt;200 down to ~2.7. Is the fit compromised? The model without rating has an \\(R^2\\) of 0.954, while the model containing it has 0.955. Such negligible change in \\(R_2\\) shows the redundant information provided by the variable. Reducing the multicollinearity gives us an equivalent level fit while reducing the chance for a significant predictor to end up with an insignificant coefficient in our model. This is especially important when extending a pre-existing model on to a new, unknown sample. Another solution would be to combine collinear variables into a single predictor. For example, taking the average of standardized versions of limit and rating and creating a new variable that measures credit worthiness. 3.3.7 The Marketing Plan # TODO We return to the seven questions we set out to answer regarding Advertising data. 3.3.8 Comparison of Linear Regression with K-Nearest Neighbors Linear regression is an example of a parametric approach because it assumes a linear functional form for \\(f(X)\\). The advantages are: estimating small number of coefficients is easy coefficients have simple intrepretations tests of statistical significance are easily performed However, the strong assumptions of the form of \\(f(X)\\) come at a cost. If the functional form is far from the truth, then the model will perform poorly in terms of prediction accuracy. Non-parametric methods do not explicitly assume a parametric form for \\(f(X)\\), and thereby provide an alternative and more flexible approach for performing regression. Let’s consider one of the simplest and best-known non-parametric methods, K-nearest neighbors regression (KNN regression). Given a value for \\(K\\) and a prediction point \\(x_0\\), KNN regression first identifies the \\(K\\) training observations that are closest to \\(x_0\\), represented by \\(\\mathcal{N}_0\\). It then estimates \\(f(x_0)\\) using the average of all the training response in \\(\\mathcal{N}_0\\). In other words, \\(\\hat{f}(x_0) = \\frac{1}{K}\\sum_{x_i\\in\\mathcal{N_0}y_i\\) When \\(K\\) is small, the KNN fits close to the training observations, and provides a rough step function. As \\(K\\) increases, the prediction is averaged over a larger number of observations, producing small regions of constant prediction and smoother fits. The optimal value for \\(K\\) depends on the bias-variance tradeoff. A small value of \\(K\\) provides the most flexible fit, which will have low bias but high variance. The high variance comes from the prediction in a particular region being dependant on a small number of observations. In contrast, larger values of \\(K\\) provide a smoother and less variable fit; incorporating the average of a larger number of points, giving a single observation less weight in the prediction. However, this smoothing can cause bias by masking some of the structure in \\(f(x)\\). In Chapter 5, (TODO add link), we introduce several approaches for estimating test error rates, which can be used identify optimal value of \\(K\\). We can use the caret package to fit a number of \\(k\\)-values with the KNN algorithm, and see which produces the best R^2. This example doesn’t take into account the bias introduced by a larger \\(k\\)-value, and how that might perform on a test dataset. A \\(k\\) of 5 produces the best fit. # TODO plot 3d with decision boundaries for multiple k library(caret) k_values = data.frame(k = c(seq(1:10))) knn_airquality &lt;- train(data = tidy_airquality %&gt;% filter(!is.na(ozone)), ozone ~ temp + wind, method = &quot;knn&quot;,tuneGrid = k_values, metric = &quot;Rsquared&quot;) knn_results &lt;- knn_airquality$results %&gt;% as_tibble() knn_results ## # A tibble: 10 x 7 ## k RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 22.8 0.547 15.2 3.05 0.0772 2.03 ## 2 2 21.2 0.602 14.8 3.54 0.104 2.14 ## 3 3 20.3 0.620 14.2 3.29 0.0997 1.91 ## 4 4 20.1 0.626 14.0 3.68 0.0974 2.12 ## 5 5 19.7 0.636 13.7 3.95 0.0955 2.19 ## 6 6 19.6 0.637 13.6 4.00 0.0959 2.13 ## # … with 4 more rows In what setting will a parametric approach outperform a non-parametric approach such as KNN? The parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of \\(f\\). Let’s take our ozone example and compare KNN to linear regression. The linear model produces an \\(R^2\\) of 0.57, compared to KNN’s 0.64. Is KNN’s lack of interpretability worth the 0.07 gain in \\(R^2\\)? Probably not. # TODO split into training / test and compare KNN vs Linear Interestingly, the plot below demonstrates a shortcoming of linear regression as well, and the value of domain knowledge in approaching modeling. ozone values should have a minimum value of 0, and yet, our linear regression model predicts negative values for certain observations. One good thing of the KNN approach is that, due to its nature of averaging out observations, it’s impossible for it to run into the same scenario. # TODO clean plot knn_results %&gt;% filter(Rsquared == max(Rsquared)) ## # A tibble: 1 x 7 ## k RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 19.4 0.643 13.4 4.21 0.100 2.17 lm_airquality &lt;- lm(data = tidy_airquality, ozone ~ temp + wind) lm_airquality %&gt;% broom::glance() ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.569 0.561 21.9 74.5 2.31e-21 3 -521. 1050. 1061. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; tidy_airquality %&gt;% gather_predictions(knn_airquality, lm_airquality) %&gt;% ggplot(aes(x = ozone, y = pred, colour = model)) + geom_point() + geom_abline() When the relationship is truly linear, it is common for a linear regression to outperform KNN. However, as the relationship grows more non-linear, KNN might end up as a better approach. KNN being nonparametric doesn’t make any assumptions about the true form of \\(f(x)\\), and will often outperform (not always) linear regression in non-linear scenarios. When does KNN exhibit poor performance? This happens when the number of predictors grows. In a \\(p=1\\) setting with 100 observtions, KNN can probably accurately estimate \\(f(X)\\). If we increase it to \\(p=20\\), we run in to the curse of dimensionality. As \\(p\\) increases, the \\(K\\) observations that are nearest to a given test observation \\(x_0\\) may be very far away from \\(x_0\\) in \\(p\\)-dimensional space. This will lead to a poor KNN fit. As a rule of thumb, parametric methods tend to outperform non-parametric approaches when there is a small number of observations per predictor. Linear regression also has the benefit of interpretability. We might forego a bit of prediction accuracy for the sake of a simple model with interpretable coefficients and available p-values. 3.4 Lab: Linear Regression Use library() to load libraries. library(MASS, exclude = &quot;select&quot;) library(ISLR) We will be running a linear regression on MASS::Boston dataset. First thing to do is convert to a tibble, which provides data types, reasonable printing methods, and tidy structure. boston &lt;- MASS::Boston %&gt;% as_tibble() boston ## # A tibble: 506 x 14 ## crim zn indus chas nox rm age dis rad tax ptratio black ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00632 18 2.31 0 0.538 6.58 65.2 4.09 1 296 15.3 397. ## 2 0.0273 0 7.07 0 0.469 6.42 78.9 4.97 2 242 17.8 397. ## 3 0.0273 0 7.07 0 0.469 7.18 61.1 4.97 2 242 17.8 393. ## 4 0.0324 0 2.18 0 0.458 7.00 45.8 6.06 3 222 18.7 395. ## 5 0.0690 0 2.18 0 0.458 7.15 54.2 6.06 3 222 18.7 397. ## 6 0.0298 0 2.18 0 0.458 6.43 58.7 6.06 3 222 18.7 394. ## # … with 500 more rows, and 2 more variables: lstat &lt;dbl&gt;, medv &lt;dbl&gt; We are interested in predicting medv (median house value) using 13 predictors, such as rm (number of rooms per house), age (average age of houses), and lstat(percent of households with low socioeconomic status). 3.4.1 Fitting a linear regression We will use the lm function to fit a simple linear regression, which takes the form lm(data = data, y ~ x). boston_lm &lt;- lm(data = boston, medv ~ lstat) Printing boston_lm provides some basic information about the model. boston_lm ## ## Call: ## lm(formula = medv ~ lstat, data = boston) ## ## Coefficients: ## (Intercept) lstat ## 34.55 -0.95 If we want to work with data surrounding the model, compare it to other models, or reference components of it, we need to use the broom package. broom::tidy converts the model into a data.frame representation. library(broom) broom::tidy(boston_lm) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 34.6 0.563 61.4 3.74e-236 ## 2 lstat -0.950 0.0387 -24.5 5.08e- 88 broom::glance will provide a summary in data.frame form. broom::glance(boston_lm) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.544 0.543 6.22 602. 5.08e-88 2 -1641. 3289. 3302. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; broom is powerful for easy output, accessing elements of a model, and comparing multiple models. This is where ISLR misses the mark. It shows attach(), which we discussed as a bad idea in Chapter 1, and only offers the base R methods on inspecting a model object. It goes into inspecting model elements via the use of names(), coef(), and more, which provide much less utility than broom. To get confidence interval of coefficients, we can use conf.int argument in broom::tidy. broom::tidy(boston_lm, conf.int = TRUE, conf.level = 0.95) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 34.6 0.563 61.4 3.74e-236 33.4 35.7 ## 2 lstat -0.950 0.0387 -24.5 5.08e- 88 -1.03 -0.874 We can use modelr::add_predictions() or broom::augment() to add predictions and confidence intervals for our model. # TODO decide on best practices for adding predictions; development effort on `modelr` modelr::add_predictions() adds a pred column to each observation. boston %&gt;% add_predictions(boston_lm) ## # A tibble: 506 x 15 ## crim zn indus chas nox rm age dis rad tax ptratio black ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00632 18 2.31 0 0.538 6.58 65.2 4.09 1 296 15.3 397. ## 2 0.0273 0 7.07 0 0.469 6.42 78.9 4.97 2 242 17.8 397. ## 3 0.0273 0 7.07 0 0.469 7.18 61.1 4.97 2 242 17.8 393. ## 4 0.0324 0 2.18 0 0.458 7.00 45.8 6.06 3 222 18.7 395. ## 5 0.0690 0 2.18 0 0.458 7.15 54.2 6.06 3 222 18.7 397. ## 6 0.0298 0 2.18 0 0.458 6.43 58.7 6.06 3 222 18.7 394. ## # … with 500 more rows, and 3 more variables: lstat &lt;dbl&gt;, medv &lt;dbl&gt;, ## # pred &lt;dbl&gt; broom::augment() adds fitted values as well as standard errors and residuals. augment(boston_lm) ## # A tibble: 506 x 9 ## medv lstat .fitted .se.fit .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 24 4.98 29.8 0.406 -5.82 0.00426 6.22 0.00189 -0.939 ## 2 21.6 9.14 25.9 0.308 -4.27 0.00246 6.22 0.000582 -0.688 ## 3 34.7 4.03 30.7 0.433 3.97 0.00486 6.22 0.00100 0.641 ## 4 33.4 2.94 31.8 0.467 1.64 0.00564 6.22 0.000198 0.264 ## 5 36.2 5.33 29.5 0.396 6.71 0.00406 6.21 0.00238 1.08 ## 6 28.7 5.21 29.6 0.399 -0.904 0.00413 6.22 0.0000440 -0.146 ## # … with 500 more rows We can use the data from augment() to plot our regression with 95% confidence intervals for our predicted values. augment(boston_lm) %&gt;% ggplot(aes(x = lstat)) + geom_point(aes(y = medv), pch=21) + geom_line(aes(y = .fitted)) + geom_ribbon(aes(ymin = .fitted - 1.96 * .se.fit, ymax = .fitted + 1.96 * .se.fit), alpha = 1/3) + theme_minimal() The fit is decent, but there appears to be some non-linearity in the data. plot(boston_lm) Let’s plot some diagnostics surrounding our model. 3.4.1.1 Diagnostic Plots We could plot the residual in its original form, but this maintains the scale of the \\(Y\\) response. Two methods exist to scale it in an interpretable fashion, which both scale it using an estimate of the variance of the residual. Standardized residuals scale the residuals in a comparable way by dividing the residual by the regression residual standard error. However, this allows high-leverage points to be factored in to the standard error. If a particular \\(y_i\\) has high leverage, it will drag the regression line significantly, which affects the estimate of the residual itself. Studentized residuals leave out the residual of \\(y_i\\) when calculating the standard error of a residual for a given observation. Observations with low-leverage will have similar standardized/studentized values, while those with high-leverage will be more affected. By isolating an observation’s leverage from its particular standard error calculation, we can observe how much of an outlier it truly is. For more context, these class notes from Wharton Statistics Department are helpful. augment calculates standardized residuals, but not student. We will add a column to calculate it, as shown in this example doc from Jeffrey Arnold, [Outliers and Robust Regression] (https://uw-pols503.github.io/2016/outliers_robust_regression.html). In this case, the two types of residuals are virtually the same. I added green points of the standardized residual for comparison. I also scaled the size of points by their leverage (.cooksd column). Finally, I shaded the boundary area between -3 and 3 for the studentized residual. This is a rule of thumb for outlier detection, but observations with magnitude greater than 3 warrant a model inspection. These could be errors in recording of the data or evidence that the model assumptions are not appropriate. augment(boston_lm) %&gt;% # calculate student residual mutate(.student.resid = .resid / .sigma * sqrt(1 - .hat)) %&gt;% ggplot(aes(x = lstat, y = .student.resid)) + geom_point(aes(y = .std.resid, size=.cooksd), colour = &quot;khaki2&quot;) + geom_point(alpha = 2/3, aes(size = .cooksd), colour = &quot;dodgerblue4&quot;) + geom_hline(yintercept = 0, size = 1.5, colour = &quot;grey&quot;) + geom_ribbon(aes(ymin = -3, ymax = 3), alpha = 1/4) + theme_minimal() This plot shows a fan-shape for the residuals, which adds evidence of non-linearity in the relationship. We also see high-leverage points on both ends of the x-axis, which significantly drag our regression line. Several points are outside of our outlier zone. All of these above add evidence that our simple linear regression is not appropriate for this dataset. 3.4.2 Multiple Linear Regression Let’s fit a model with all the predictor variables. boston_mlm &lt;- lm(data = boston, medv ~ .) broom::tidy(boston_mlm, conf.int = TRUE) %&gt;% arrange(desc(p.value)) ## # A tibble: 14 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 age 0.000692 0.0132 0.0524 0.958 -0.0253 0.0266 ## 2 indus 0.0206 0.0615 0.334 0.738 -0.100 0.141 ## 3 chas 2.69 0.862 3.12 0.00193 0.994 4.38 ## 4 tax -0.0123 0.00376 -3.28 0.00111 -0.0197 -0.00495 ## 5 crim -0.108 0.0329 -3.29 0.00109 -0.173 -0.0434 ## 6 zn 0.0464 0.0137 3.38 0.000778 0.0194 0.0734 ## # … with 8 more rows How does the model perform? broom::glance(boston_mlm) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.741 0.734 4.75 108. 6.72e-135 14 -1499. 3028. 3091. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; The VIF for each predictor is mostly low. car::vif(boston_mlm) %&gt;% as_tibble(rownames = &quot;variable&quot;) ## # A tibble: 13 x 2 ## variable value ## &lt;chr&gt; &lt;dbl&gt; ## 1 crim 1.79 ## 2 zn 2.30 ## 3 indus 3.99 ## 4 chas 1.07 ## 5 nox 4.39 ## 6 rm 1.93 ## # … with 7 more rows To remove predictors, such as those with high p-values, we modify the lm() call. Let’s remove age, which has a p-value of 0.95. boston_mlm &lt;- lm(data = boston, medv ~ . - age) broom::glance(boston_mlm) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.741 0.734 4.74 117. 6.08e-136 13 -1499. 3026. 3085. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; We could also incorporate a dplyr::select() call in the data argument for a more readable solution. boston_mlm &lt;- lm(data = boston %&gt;% select(-age), medv ~ .) 3.4.3 Interaction Terms We can incorporate interaction terms in lm() using the term1:term2 syntax. lm(data = boston, medv ~ lstat + age + lstat:age) # equivalent lm(data = boston, medv ~ lstat*age) 3.4.4 Non-linear Transformations of the Predictors lm() can also accomodate non-linear transformations of the predictors. # TODO is this best way lm_boston_squared &lt;- lm(data = boston, medv ~ lstat + I(lstat^2)) lm_boston &lt;- lm(data = boston, medv ~ lstat) We can use anova() with broom::tidy() to measure if it fits better than linear fit. broom::tidy(anova(lm_boston, lm_boston_squared)) ## # A tibble: 2 x 6 ## res.df rss df sumsq statistic p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 504 19472. NA NA NA NA ## 2 503 15347. 1 4125. 135. 7.63e-28 In this case, the full model with the quadratic term fits better. 3.5 Exercises # TODO exercises "],
["classification.html", "Chapter 4 Classification 4.1 Packages used in this chapter 4.2 An Overview of Classification 4.3 Why Not Linear Regression? 4.4 Logistic Regression 4.5 Linear Discriminant Analysis 4.6 A Comparison of Classification Methods 4.7 Lab: Logistic Regression, LDA, QDA, and KNN 4.8 Conclusion 4.9 Exercises", " Chapter 4 Classification 4.1 Packages used in this chapter library(tidyverse) library(tidymodels) library(discrim) library(kknn) library(knitr) library(kableExtra) library(skimr) Linear regression in chapter 3 was concerned with predicting a quantitative response variable. What if the response variable is qualitative? Eye color is an example of a qualitative variable, which takes discrete value such as blue, brown, green. These are also referred to as categorical. The approach of predicting qualitative responses is known as classification. Often, we predict the probability of the occurences of each category of a qualitative variable, and then make a decision based off of that. In this chapter we discuss three of the most widely-used classifiers: logistic regression linear discriminant analysis k-nearest neighbors We discuss more computer-intensive methods in later chapters. 4.2 An Overview of Classification Classification is a common scenario. Person arrives at ER exhibiting particular symptoms. What illness does he have? Money is wired to an external account at a bank. Is this fraud? Email is sent to your account. Is it legit, or spam? Similar to regression, we have a set of training observations that use to build a classifier. We also want the classifier to perform well on both training and test observations. We will use the dataset ISLR::Default. First, let’s convert it to tidy format. default &lt;- ISLR::Default %&gt;% as_tibble() We are interested in the ability to predict whether an individual will default on their credit card payment, based on their credit card balance and annual income. If we look at the summary statistics, we see the data is clean, and that very few people default on their balances. default %&gt;% skimr::skim() Table 4.1: Data summary Name Piped data Number of rows 10000 Number of columns 4 _______________________ Column type frequency: factor 2 numeric 2 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts default 0 1 FALSE 2 No: 9667, Yes: 333 student 0 1 FALSE 2 No: 7056, Yes: 2944 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist balance 0 1 835.37 483.71 0.00 481.73 823.64 1166.31 2654.32 ▆▇▅▁▁ income 0 1 33516.98 13336.64 771.97 21340.46 34552.64 43807.73 73554.23 ▂▇▇▅▁ The scatterplot signals a strong relationship between balance and default. default %&gt;% ggplot(aes(x = balance, y = income, fill = default)) + geom_hex(alpha = 2/3) The boxplot captures the stark difference in balance between those who default and do not. default %&gt;% ggplot(aes(y = balance, fill = default)) + geom_boxplot() 4.3 Why Not Linear Regression? Imagine we were trying to predict the medical outcome of a patient on the basis of their symptoms. Let’s say there are three possible diagnoses: stroke, overdose, and seizure. We could encode these into a quantitative variable \\(Y\\). that takes values from 1 to 3. Using least squares, we could then fit a regression model to predict \\(Y\\). Unfortunately, this coding implies an ordering of the outcomes. It also insists that the difference between levels is quantitative, and equivalent across all sequences of levels. Thus, changing the order of encodings would change relationship among the conditions, producing fundamentally different linear models. There could be a case where a response variables took on a natural ordering, such as mild, moderate, severe. We would also need to believe that the gap between each level is equivalent. Unfortunately, there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is appropriate for linear regression. For cases of binary qualitative response, we can utilize the dummy variable solution seen in Chapter 3. In this case, the order of the encodings is arbitrary. # TODO add latex for encoding Linear regression does work for this binary response scenario. However, it is possible for linear regression to produce estimates outside of the [0, 1] interval, which affects their interpretability as probabilities. When the qualitative response has more than two levels, we need to use classification methods that are appropriate. 4.4 Logistic Regression Let’s consider the default dataset. Rather than modeling this response \\(Y\\) directly, logistic regression models the probability that \\(Y\\) belongs to a particular category. If we estimate using linear regression, we see that some estimated probabilities are negative. We are using the tidymodels package. default &lt;- default %&gt;% mutate(default_bool = if_else(default == &quot;Yes&quot;, 1, 0)) lm_default &lt;- linear_reg() %&gt;% fit(data = default, default_bool ~ balance) default %&gt;% bind_cols(predict(lm_default, default)) %&gt;% ggplot(aes(x = balance)) + geom_line(aes(y = .pred)) + geom_point(aes(y = default_bool, colour = default_bool)) + guides(colour=FALSE) Below is the classification using logistic regression, where are probabilities fall between 0 and 1. logi_default &lt;- logistic_reg(mode = &quot;classification&quot;) %&gt;% fit(data = default, as.factor(default_bool) ~ balance) default %&gt;% bind_cols(predict(logi_default, default, type = &quot;prob&quot;)) %&gt;% ggplot(aes(x = balance)) + geom_line(aes(y = .pred_1)) + geom_point(aes(y = default_bool, colour = default_bool)) + guides(colour=FALSE) Logistic regression in this example is modelling the probability of default, given the value of balance. Pr(default = Yes|balance) These values, which we abbreviate as p(balance), range between 0 and 1. Logistic regression will always produce an S-shaped curve. Regardless of the value of \\(X\\), we will receive a sensible prediction. From this, we can make a classification prediction for default. Depending how conservative we are, the threshold for this could vary. Depending on the domain and context of the classification, a decision boundary around 0.5 or 0.1 might be appropriate. 4.4.1 The Logistic Model The problem of using a linear regression model is evident in the chart above, where probabilities can fall below 0 or greater than 1. To avoid this, we must model \\(p(X)\\) using a function that gives outputs between 0 and 1 for all values of \\(X\\). In logistic regression, we use the logistic function, \\(p(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\\) logistic function To fit the model, we use a method called maximum likelihood. If we manipulate the logistic function, we find that \\(\\frac{p(X)}{1-p(X)} = e^{\\beta_0+\\beta_1X}\\) odds This is called the odds, and takes any value from \\(0\\) to \\(\\infty\\). This is the same type of odds used in sporting events (“9:1 odds to win this match”, etc). If \\(p(X) = 0.9\\), then odds are \\(\\frac{0.9}{1-0.9} = 9\\). If we take the logarithm of the odds, we arrive at \\(log(\\frac{p(X)}{1-p(X)}) = \\beta_0+\\beta_1X\\) log-odds logit The left-hande side is called the log-odds or logit. The logistic regression model has a logit that is linear in \\(X\\). The contrast to linear regression is that increasing \\(X\\) by one-unit changes the log odds by \\(\\beta_1\\) (or the odds by \\(e^{\\beta_1}\\). However, since \\(p(X)\\) and \\(X\\) relationship is not a straight line (see plot above), \\(\\beta_1\\) does not correspond to the the change in \\(p(X)\\) associated with a one-unit increase in \\(X\\). The amount that \\(p(X)\\) changes depends on the current value of \\(X\\). See how the slope approaches 0 more and more slowly as balance increases. Regardless of how much \\(p(X)\\) moves, if \\(\\beta_1\\) is positive then increasing \\(X\\) will be associated with increasing \\(p(X)\\). The opposite is also true. 4.4.2 Estimating the Regression Coefficients The coefficients in the logistic regression equation must be estimated used training data. Linear regression used the least squares approach to estimate the coefficients. It is possible to use non-linear least squares to fit the model, but maximum likelihood is preferred. Maximum likelihood seeks to to find estimates for \\(\\beta_0\\) and \\(\\beta_1\\) such that the predicted probability \\(\\hat{p}(x_i)\\) of default for each individual corresponds as closely as possible to to the individual’s observed default status. We want estimates that produce low probabilities for individuals who did not default, and high probabilities for those who did. We can formalize this with a likelihood function: #TODO add likelihood function, links, etc We can examine the coefficients and other information from our logistic regression model. logi_default %&gt;% broom::tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -10.7 0.361 -29.5 3.62e-191 ## 2 balance 0.00550 0.000220 25.0 1.98e-137 If we look at the terms of our logistic regression, we see that the coefficient for balance is positive. This means that higher balance increases \\(p(Default)\\). A one-unit increase in balance will increase the log odds of defaulting by ~0.0055. The test-statistic also behaves similarly. Coefficients with large statistics indicate evidence against the null hypothesis \\(H_0: \\beta_1 = 0\\). For logistic regression, the null hypothesis implies that \\(p(X) = \\frac{e^{\\beta_0}}{1+e^{\\beta_0}}\\), which means that the probability of defaulting does not depend on balance. Given the miniscule p-value associated with our balance coefficient, we can confidently reject \\(H_0\\). The intercept (\\(\\beta_0\\)) is typically not of interest; it’s main purpose is to adjust the average fitted probabilities to the proportion of ones in the data. 4.4.3 Making Predictions Once we have the coefficients, we simply compute the probability of default for any given observation. Let’s take an individual with a balance of $1000. Using our model terms, we can compute the probability. Let’s extract the terms from the model and plug in a balance of $1000. logi_coef &lt;- logi_default %&gt;% broom::tidy() %&gt;% # widen it and clean up names select(term, estimate) %&gt;% pivot_wider(names_from = term, values_from = estimate) %&gt;% janitor::clean_names() logi_coef %&gt;% mutate(prob_1000 = exp(intercept + balance * 1000) / (1 + exp(intercept + balance * 1000))) ## # A tibble: 1 x 3 ## intercept balance prob_1000 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -10.7 0.00550 0.00575 We find the probability to be less than 1%. We can also incorporate qualitative predictors with the logistic regression model. Here we encode student in to the model. logi_default_student &lt;- logistic_reg(mode = &quot;classification&quot;) %&gt;% fit(data = default, as.factor(default_bool) ~ student) logi_default_student %&gt;% broom::tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -3.50 0.0707 -49.6 0 ## 2 studentYes 0.405 0.115 3.52 0.000431 This model indicates that students have a higher rate of defaulting compared to non-students. 4.4.4 Multiple Logistic Regression We now consider the scenario of multiple predictors. We can rewrite \\(p(X)\\) as \\(p(X) = \\frac{e^{\\beta_0+\\beta_1X_1+...+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+...+\\beta_pX_p}}\\) /p&gt; And again use the maximum likelihood method to estimate the coefficients. Let’s estimate balance using balance, income and student. multiple_logi_default&lt;- logistic_reg(mode = &quot;classification&quot;) %&gt;% fit(data = default, as.factor(default_bool) ~ balance + student + income) multiple_logi_default %&gt;% broom::tidy() ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -10.9 0.492 -22.1 4.91e-108 ## 2 balance 0.00574 0.000232 24.7 4.22e-135 ## 3 studentYes -0.647 0.236 -2.74 6.19e- 3 ## 4 income 0.00000303 0.00000820 0.370 7.12e- 1 Notice that being a student now decreases the chances of default, whereas in our previous model (which only contained student as a predictor), it increased the chances. Why is that? This model is showing that, for a fixed value of income and balance, students actually default less. This is because student and balance are correlated. default %&gt;% ggplot(aes(y = balance, fill = student)) + geom_boxplot() If we plot the distribution of balance across student, we see that students tend to carry larger credit card balances. This example illustrates the dangers of drawing insights from single predictor regressions when other predictors may be relevant. The results from using one predictor can be substantially different compared to using multiple predictors. This phenomenon is known as confounding. 4.4.5 Logistic Regression for &gt;2 Response Classes Sometimes we wish to classify a response variable that has more than two classes. This could be the medical example where a patient outcomes falls into stroke, overdose, and seizure. It is possible to extend the two-class logistic regression model into multiple-class, but this is not used often in practice. A method that is popular for multi-class classification is discriminant analysis. 4.5 Linear Discriminant Analysis Logistic regression models the distribution of response \\(Y\\) given the predictor(s) \\(X\\). In discriminant analysis, we model the distribution of the predictors \\(X\\) in each of the response classes, and then use Bayes’ theorem to flip these around into estimates for \\(Pr(Y = k|X = x)\\). Why do we need this method? Well-separated classes produce unstable parameter estimates for logistic regression models If \\(n\\) is small and distribution of predictors \\(X\\) is normall across the classes, the linear discriminant model is more stable than logistic regression 4.5.1 Using Bayes’ Theorem for Classification Consider the scenario where we want to classify an observation into one of \\(K\\) classes, where \\(K &gt;= 2\\). Let \\(\\pi_k\\) represent the overall or prior probability that a randomly chosen observation comes from the \\(k\\)th class Let \\(f_k(x) = Pr(X = x|Y = k)\\) denote the density function of \\(X\\) for an observation that comes from the \\(k\\)th class. In other words, \\(f_k(x)\\) being large means that there is a high probability that an observation in the \\(k\\)th class has \\(X \\approx x\\). We can use Bayes’ theorem \\[ \\operatorname{Pr}(Y=k | X=x)=\\frac{\\pi_{k} f_{k}(x)}{\\sum_{l=1}^{K} \\pi_{l} f_{l}(x)} \\] And call the left-hand side \\(p_k(X)\\). We can plug in estimates of \\(\\pi_k\\) and \\(f_k(X)\\) into Bayes’ theorem above to get the probability of a certain class, given an observation. Solving for \\(\\pi_k\\) is easy if we have a random sample of \\(Y\\)s from the population. We simply calculate the fraction of observations that fall into a \\(k\\) class. Estimating \\(f_k(X)\\) is more challenging unless we assume simple forms for these densities We refer to \\(p_k(x)\\) as the posterior probability that an observation \\(X = x\\) belongs to the \\(k\\)th class. This is the probability that the observation belongs to the \\(k\\)th class, given the predictor value for that observation. The Bayes’ classifier classifies an observation to the class for which \\(p_k(X)\\) is largest. If we can find a way to estimate \\(f_k(X)\\), we can develop a classifier that approximates the Bayes classifier. 4.5.2 Linear Discriminant Analysis for p = 1 Let’s assume we have one predictor. We need to obtain an estimate for \\(f_k(x)\\) (the density function for \\(X\\) given a class \\(k\\)). This will obtain a value for \\(p_k(x)\\). We will then classify this observation for which \\(p_k(x)\\) is greatest. To estimate \\(f_k(x)\\), we need to make some assumptions about its form. Let’s assume \\(f_k(x)\\) is normal or Gaussian. The normal density takes the form \\[ f_{k}(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma_{k}} \\exp \\left(-\\frac{1}{2 \\sigma_{k}^{2}}\\left(x-\\mu_{k}\\right)^{2}\\right) \\] Plugging this back in to \\(p_k(x)\\), we obtain \\[ p_{k}(x)=\\frac{\\pi_{k} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{1}{2 \\sigma^{2}}\\left(x-\\mu_{k}\\right)^{2}\\right)}{\\sum_{l=1}^{K} \\pi_{l} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{1}{2 \\sigma^{2}}\\left(x-\\mu_{l}\\right)^{2}\\right)} \\] Taking the log and rearranging results in \\[ \\delta_{k}(x)=x \\cdot \\frac{\\mu_{k}}{\\sigma^{2}}-\\frac{\\mu_{k}^{2}}{2 \\sigma^{2}}+\\log \\left(\\pi_{k}\\right) \\] In this case, the Bayes decision boundary corresponds to \\[ x=\\frac{\\mu_{1}^{2}-\\mu_{2}^{2}}{2\\left(\\mu_{1}-\\mu_{2}\\right)}=\\frac{\\mu_{1}+\\mu_{2}}{2} \\] We can simulate some data to show a simple example. In this data we have two classes: \\(\\mu_1 = -1.25, \\mu_2 = 1.25, \\sigma_1^2 = \\sigma_2^2 = 1\\) var_1 = 1 var_2 = var_1 f_1 = tibble(fun = &quot;f_1&quot;, x = rnorm(n = 10000, mean = -1.25, sd = var_1)) f_2 = tibble(fun = &quot;f_2&quot;, x = rnorm(n = 10000, mean = 1.25, sd = var_2)) f_x = bind_rows(f_1, f_2) # add summary statistics f_x &lt;- f_x %&gt;% group_by(fun) %&gt;% mutate(pi = n(), var = var(x), mu = mean(x)) %&gt;% ungroup() %&gt;% mutate(pi = pi / n()) decision_boundary &lt;- f_x %&gt;% group_by(fun) %&gt;% summarise(mu = mean(x)) %&gt;% summarise(decision_boundary = sum(mu) / 2) %&gt;% pull() f_x %&gt;% ggplot(aes(x = x, colour = fun)) + geom_density() + geom_vline(xintercept = decision_boundary, linetype = &quot;dashed&quot;) These two densities overlap, and so given \\(X = x\\), we still have uncertaintly about which class the observation belongs to. If both classes are equally likely for a random observation \\(\\pi_1 = \\pi_2\\), then we see the Bayes classifier assigns the observation to class 1 if \\(x &lt; 0\\) and class 2 otherwise. Even if we are sure that \\(X\\) is drawn from a Gaussian distribution within each class, we still need to estimate \\(\\mu_1,...,\\mu_k\\), \\(\\pi_1,...,\\pi_k\\), and \\(\\sigma^2\\). The linear discriminant analysis method approximates these by plugging in estimates as follows \\(\\hat{\\mu}_k = \\frac{1}{n_k}\\sum_{i:y_i=k}{x_i}\\) * \\(\\hat{\\sigma}^2 = \\frac{1}{n-K}\\sum_{k=1}^{K}\\sum_{i:y_i=k}{(x_i-\\hat{\\mu}_k)^2}\\) * The estimate for \\(\\hat{\\mu}_k\\) is the average of all training observations from the \\(k\\)th class. The estimate for \\(\\hat{\\sigma}^2\\) is the weighted average of the sample variances for each of the K classes. To estimate \\(\\hat{\\pi}_k\\), we simply take the proportion of training observations that belong to the \\(k\\)th class \\(\\hat{\\pi}_k = n_k/n\\) * From these estimates, we can achieve a decision boundary \\[ \\hat{\\delta}_{k}(x)=x \\cdot \\frac{\\hat{\\mu}_{k}}{\\hat{\\sigma}^{2}}-\\frac{\\hat{\\mu}_{k}^{2}}{2 \\hat{\\sigma}^{2}}+\\log \\left(\\hat{\\pi}_{k}\\right) \\] This classifier has linear in the name due to the fact that the discriminant function above are linear functions of \\(x\\). Let’s take a sample from our earlier distribution and see how it performs. library(discrim) f_sample = f_x %&gt;% sample_frac(size = 0.01) lda_f &lt;- discrim::discrim_linear() %&gt;% fit(data = f_sample, as.factor(fun) ~ x) preds &lt;- predict(lda_f, f_sample, type = &quot;class&quot;) f_sample &lt;- f_sample %&gt;% bind_cols(preds) # TODO figure out how to truly extract decision boundary from MASS::lda est_decision &lt;- f_sample %&gt;% arrange(x) %&gt;% filter(.pred_class == &#39;f_2&#39;) %&gt;% slice(1) %&gt;% pull(x) ggplot(f_sample, aes(x = x, fill = fun)) + geom_histogram() + geom_vline(xintercept = est_decision, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 0) Notice the estimated decision boundary (dashed line) being very close to the Bayes decision boundary. 4.5.2.1 Measuring Performance # TODO show lda performance compared to true value 4.5.3 Linear Discriminant Analysis for p &gt; 1 We can extend LDA classifier to multiple predictors. The multivariate Gaussian distribution assumes that each predictor follows a one-dimensional normal distribution, with some correlation between each pair of predictors. Andrew Ng on Multivariate Gaussian Distribution To indicate that a \\(p\\)-dimensional random variable \\(X\\) has a multi-variate Gaussian distribution, we write $ X N(, )$ \\(E(X) = \\mu\\) is the mean of \\(X\\) (a vector with \\(p\\) components) \\(Cov(X) = \\Sigma\\) is the \\(p*p\\) covariance matrix of \\(X\\). The multivariate Gaussian density is defined as \\[ f(x)=\\frac{1}{(2 \\pi)^{p / 2}|\\mathbf{\\Sigma}|^{1 / 2}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\mathbf{\\Sigma}^{-1}(x-\\mu)\\right) \\] In the case of \\(p&gt;1\\) predictors, the LDA classifier assumes that the observations in the \\(k\\)th class are drawn from a multivariate Gaussian distribution \\(N(\\mu_k, \\Sigma)\\), where \\(\\mu_k\\) is a class-specific mean vector, and \\(\\Sigma\\) is the covariance matrix that is common to all \\(K\\) classes. Plugging the density function for the \\(k\\)th class, \\(f_k(X = x)\\), into \\[ \\operatorname{Pr}(Y=k | X=x)=\\frac{\\pi_{k} f_{k}(x)}{\\sum_{l=1}^{K} \\pi_{l} f_{l}(x)} \\] and performing some algebra reveals that the Bayes classifier will assign observation \\(X = x\\) by identifying the class for which \\[ \\delta_{k}(x)=x^{T} \\boldsymbol{\\Sigma}^{-1} \\mu_{k}-\\frac{1}{2} \\mu_{k}^{T} \\boldsymbol{\\Sigma}^{-1} \\mu_{k}+\\log \\pi_{k} \\] is largest. 4.5.3.1 Performing LDA on Default data If we run an LDA model on our default dataset, predicting the probability of default based off of student and balance, we achieve a respectable 3.0% error rate. set.seed(1) default_split &lt;- initial_split(default, prop = 3/4) train_default &lt;- training(default_split) test_default &lt;- testing(default_split) lda_default &lt;- discrim::discrim_linear() %&gt;% fit(data = train_default, default ~ student + balance) preds &lt;- predict(lda_default, test_default, type = &quot;class&quot;) # error rate test_default %&gt;% bind_cols(preds) %&gt;% metrics(truth = default, estimate = .pred_class) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.97 ## 2 kap binary 0.369 While this may seem impressive, let’s remember that only 3.6% of observations in the dataset end up in default. This means that if we assigned a null classifier, which simply predicted every observation to not end in default, our error rate would be 3.6%. This is worse, but not by much, compared to our LDA error rate. # null error rate test_default %&gt;% group_by(default) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(prop = n / sum(n)) ## # A tibble: 2 x 3 ## default n prop ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 No 2410 0.964 ## 2 Yes 90 0.036 Binary decision makers can make to types of errors: Incorrectly assigning an individual who defaults to the “no default” category Incorrectly assigning an individual who doesn’t default to the “default” category. We can identify the breakdown by using a confusion matrix cm &lt;- test_default %&gt;% bind_cols(preds) %&gt;% conf_mat(truth = default, estimate = .pred_class) cm ## Truth ## Prediction No Yes ## No 2402 67 ## Yes 8 23 We see that our LDA only predicted 31 people to default. Of these, 23 actually defaulted. So, only 8 of out of the ~7500 people who did not default were incorrectly labeled. However, of the 90 people in our test set who defaulted, we only predicted this correctly for 23 of them. That means ~75% of individuals who default were incorrectly classified. Having an error rate this high for the problematic class (those who default) is unacceptable. Class-specific performance is an important concept. Sensitivity and specificity characterize the performance of a classifier or screening test. In this case, the sensitivity is the percentage of true defaults who are identified (a low ~25%). The specificity is the percentage of non-defaulters who are correctly identified (7492/7500 ~ 99.9%). Remember that LDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers (assuming Gaussian assumption is correct). The classifier will yield the smallest total number of misclassifications, regardless of which class the errors came from. In this credit card scenario, the credit card company might wish to avoid incorrectly misclassifying a user who defaults. In this case, they value sensitivity. For them, the cost of misclassifying a defaulter is higher than the cost of misclassifying a non-defaulter (which they still desire to avoid). It’s possible to modify LDA for such circumstances. Given the Bayes classifier works by assigning an observation to a class in which the posterior probability \\(p_k(X)\\) is greatest (in the two-class scenario, this decision boundary is at 0.5), we can modify the probability threshold to suit our needs. If we wish to increase our sensitivity, we can lower this threshold. Imagine we lowered the threshold to 0.2. Sure, we would classify more people as defaulters than before (decreasing our specificity) but we would also catch more defaulters we previously missed (increasing our sensitivity). preds &lt;- predict(lda_default, test_default, type = &quot;prob&quot;) # error rate test_default %&gt;% bind_cols(preds) %&gt;% mutate(.pred_class = as.factor(if_else(.pred_Yes &gt; 0.2, &quot;Yes&quot;, &quot;No&quot;))) %&gt;% conf_mat(truth = default, estimate = .pred_class) ## Truth ## Prediction No Yes ## No 2357 37 ## Yes 53 53 Now our sensitivy has increased. Of the 90 people who defaulted, we correctly identified 53, or ~58.8% of them (up from ~25% previously). This came at a cost, as our specificity decreased. This time, we predicted 106 people to default. Of those, 53 actually defaulted. This means that 53 of the 7500 people who didn’t default were incorrectly labelled. This gives us a specificity of (7447/7500 ~ 99.2%) Despite the overall increase in error rate, the lower threshold may be chosen, depending on the context of the problem. To make a decision, an extensive amount of domain knowledge is required. The ROC curve is a popular graphic for displaying the two types of errors for all possible thresholds. “ROC” stands for receiver operating characteristics. The overall performance of a classifier, summarized over all possible thresholds, is given by the area under the (ROC) curve (AUC). An ideal ROC curve will hug the top left corner. Think of it this way: ideal ROC curves are able to increase sensitivity at a much higher rate than reduction in specificity. We can use yardstick:: (part of tidymodels::) to plot an ROC curve. test_default %&gt;% bind_cols(preds) %&gt;% roc_curve(default, .pred_Yes) %&gt;% autoplot() We can think of the sensitivity as the true positive, and 1 - specificity as the false positive. 4.5.4 Quadratic Discriminant Analysis LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution, with a class-specific mean vector and a covariance matrix that is common to all \\(K\\) classes. Quadratic discriminant analysis (QDA) assumes that class has its own covariance matrix. It assumes that each observation from the \\(k\\)th class has the form \\(X \\sim N(\\mu_k, \\Sigma_k)\\), where \\(\\Sigma_k\\) is a covariance matrix for the \\(k\\)th class. Under this assumption, the Bayes classifier assigns an observation \\(X=x\\) to the class for which \\[ \\begin{aligned} \\delta_{k}(x) &amp;=-\\frac{1}{2}\\left(x-\\mu_{k}\\right)^{T} \\boldsymbol{\\Sigma}_{k}^{-1}\\left(x-\\mu_{k}\\right)-\\frac{1}{2} \\log \\left|\\boldsymbol{\\Sigma}_{k}\\right|+\\log \\pi_{k} \\\\ &amp;=-\\frac{1}{2} x^{T} \\boldsymbol{\\Sigma}_{k}^{-1} x+x^{T} \\boldsymbol{\\Sigma}_{k}^{-1} \\mu_{k}-\\frac{1}{2} \\mu_{k}^{T} \\boldsymbol{\\Sigma}_{k}^{-1} \\mu_{k}-\\frac{1}{2} \\log \\left|\\boldsymbol{\\Sigma}_{k}\\right|+\\log \\pi_{k} \\end{aligned} \\] is largest. In this case, we plug in estimates for \\(\\Sigma_k\\), \\(\\mu_k\\), and \\(\\pi_k\\). Notice the quantity \\(x\\) appears as a quadratic function, hence the name. So why would one prefer LDA to QDA, or vice-versa? We again approach the bias-variance trade-off. With \\(p\\) predictors, estimating a class-independent covariance matrix requires estimating \\(p(p+1)/2\\) parameters. For example, a covariance matrix with 4 predictors would require estimating 4(4+1)/2 = 10 parameters. To estimate a covariance matrix for each class, the number of parameters is \\(Kp(p+1)/2\\) paramters. With 50 predictors, this becomes some multiple of 1,275, depending on \\(K\\). The assumption of the common covariance matrix in LDA causes the model to become linear in \\(x\\), which means there are \\(Kp\\) linear coefficients to estimate. As a result, LDA is much less flexible clasifier than QDA, and has lower variance. # TODO understand the parameter calculation The consequence of this is that if LDA’s assumption of a common covariance matrix is significantly off, the LDA can suffer from high bias. In general, LDA tends to be a better bet than QDA when there are relatively few training observations and so reduction of variance is crucial. In contrast, with large data sets, QDA can be recommended as the variance of the classifier is not a major concern, or the assumption of a common covariance matrix for the \\(K\\) classes is clearly not correct. Breaking the assumption of a common covariance matrix can “curve” the decision boundary, and so the use of a more flexible model (QDA) could yield better results. # TODO add QDA example 4.6 A Comparison of Classification Methods Let’s discuss the classification methods we have considered and the scenarios for which one might be superior. Logistic regression LDA QDA K-nearest neighbors There is a connection between LDA and logistic regression, particularyly in the two-class setting with \\(p=1\\) predictor. The difference being that logistic regression estimates coefficients via maximum likelihood, and LDA uses the estimated mean and variance from a normal distribution. The similarity in fitting procedure means that LDA and logistic regression often give similar results. When the assumption that observations are drawn from a Gaussian distribution with a common covariance matrix in each class are in fact true, the LDA can perform better than logistic regression. If the assumptions are in fact false, logistic regression can outperform LDA. KNN, on the other hand, is completely non-parametric. KNN looks at observations “closest” to \\(x\\), and assigns it to the class to which the plurality of these observations belong. No assumptions are made about the shape of the decision boundary. We can expect KNN to outperform both LDA and logistic regression when the decision boundary is highly non-linear. A downside of KNN, even when it does outperform, is its lack of interpretability. KNN does not tell us which predictors are important. QDA serves as a compromise between the non-parametric KNN method and the linear LDA and logistic regression approaches. The assumption of quadratic decision boundary allows it to accurately model a wider range of problems. It’s reduced flexibility compared to KNN allows it to produce a lower variance with a limited number of training observations due to it making some assumptions about the form of the decision boundary. 4.7 Lab: Logistic Regression, LDA, QDA, and KNN # TODO add QDA example 4.7.1 Churn Dataset We will be using the modeldata::wa_churn dataset to test our classification techniques. library(modeldata) data(wa_churn) These data were downloaded from the IBM Watson site (see below) in September 2018. The data contain a factor for whether a customer churned or not. Alternatively, the tenure column presumably contains information on how long the customer has had an account. A survival analysis can be done on this column using the churn outcome as the censoring information. A data dictionary can be found on the source website. Our interest for this dataset is predicting whether a customer will churn or not. If a customer is likely to churn, we can offer them an incentive to stay. These incentives cost us money, so we want to minimize the incentives given out to customers that won’t churn. We will identify a balance between sensitivity and specificity that maximizes the ROI of our incentive. wa_churn %&gt;% skimr::skim() Table 4.2: Data summary Name Piped data Number of rows 7043 Number of columns 20 _______________________ Column type frequency: factor 11 numeric 9 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts churn 0 1 FALSE 2 No: 5174, Yes: 1869 multiple_lines 0 1 FALSE 3 No: 3390, Yes: 2971, No : 682 internet_service 0 1 FALSE 3 Fib: 3096, DSL: 2421, No: 1526 online_security 0 1 FALSE 3 No: 3498, Yes: 2019, No : 1526 online_backup 0 1 FALSE 3 No: 3088, Yes: 2429, No : 1526 device_protection 0 1 FALSE 3 No: 3095, Yes: 2422, No : 1526 tech_support 0 1 FALSE 3 No: 3473, Yes: 2044, No : 1526 streaming_tv 0 1 FALSE 3 No: 2810, Yes: 2707, No : 1526 streaming_movies 0 1 FALSE 3 No: 2785, Yes: 2732, No : 1526 contract 0 1 FALSE 3 Mon: 3875, Two: 1695, One: 1473 payment_method 0 1 FALSE 4 Ele: 2365, Mai: 1612, Ban: 1544, Cre: 1522 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist female 0 1 0.50 0.50 0.00 0.00 0.00 1.00 1.00 ▇▁▁▁▇ senior_citizen 0 1 0.16 0.37 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▂ partner 0 1 0.48 0.50 0.00 0.00 0.00 1.00 1.00 ▇▁▁▁▇ dependents 0 1 0.30 0.46 0.00 0.00 0.00 1.00 1.00 ▇▁▁▁▃ tenure 0 1 32.37 24.56 0.00 9.00 29.00 55.00 72.00 ▇▃▃▃▆ phone_service 0 1 0.90 0.30 0.00 1.00 1.00 1.00 1.00 ▁▁▁▁▇ paperless_billing 0 1 0.59 0.49 0.00 0.00 1.00 1.00 1.00 ▆▁▁▁▇ monthly_charges 0 1 64.76 30.09 18.25 35.50 70.35 89.85 118.75 ▇▅▆▇▅ total_charges 11 1 2283.30 2266.77 18.80 401.45 1397.47 3794.74 8684.80 ▇▂▂▂▁ The dataset contains 7043 observations and 20 variables. The dataset consists mostly of factor variables, all with a small amount of unique levels. We are going to try out logistic regression, LDA, and K-nearest neighbors on this dataset. For now, we will only remove the 11 observations which have missing values for the total_charges column. We will also reorder the churn column levels so that a “success” corresponds with a customer churning. wa_churn &lt;- wa_churn %&gt;% filter(!is.na(total_charges)) %&gt;% mutate(churn = fct_relevel(churn, &quot;No&quot;, &quot;Yes&quot;)) Now let’s take a look at the response variable. wa_churn %&gt;% group_by(churn) %&gt;% tally() %&gt;% mutate(prop = n / sum(n)) ## # A tibble: 2 x 3 ## churn n prop ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 No 5163 0.734 ## 2 Yes 1869 0.266 In this dataset, 73.5% of the observations do not churn. There is some skew, but nothing drastic. We will now prepare the data and apply binary classification techniques to see which model performs best. set.seed(40) split_churn &lt;- initial_split(wa_churn, prop = 3/4) train_churn &lt;- training(split_churn) test_churn &lt;- testing(split_churn) 4.7.2 Logistic Regression Let’s run a logistic regression to predict churn using the available variables. Unlike ISLR, we will use the parsnip::logistic_reg function over glm due to its API design and machine learning workflow provided by its parent package, tidymodels. Models in the {parsnip} package also allow for choice of different computational engines. This reduces cognitive overhead by standardizing the high-level arguments for training a model without rembembering the specifications of different engine. In our case, we will be using the glm engine. logistic_reg() is a way to generate a specification of a model before fitting and allows the model to be created using different packages in R, Stan, keras, or via Spark. logi_churn &lt;- logistic_reg(mode = &quot;classification&quot;) %&gt;% fit(data = train_churn, churn ~ .) broom::tidy(logi_churn) %&gt;% arrange(p.value) ## # A tibble: 24 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 tenure -0.0558 0.00702 -7.95 1.81e-15 ## 2 contractTwo year -1.42 0.206 -6.87 6.48e-12 ## 3 contractOne year -0.655 0.126 -5.21 1.88e- 7 ## 4 paperless_billing 0.397 0.0859 4.62 3.87e- 6 ## 5 total_charges 0.000288 0.0000800 3.59 3.26e- 4 ## 6 online_securityYes -0.533 0.208 -2.56 1.05e- 2 ## # … with 18 more rows The model shows that the smallest p-value is associated with tenure and contract. This makes sense, as users locked into contracts will have a tougher time cancelling their service. We can interpret the negative coefficients as reducing the chance a customer churns. Notice how customers that utilize paperless_billing and electronic payment_method exhibit higher likelihood to churn. They are probably more tech-savvy, younger, and more likely to research other options. 4.7.2.1 Measuring model performance We can use the predict() function with our {tidymodels} workflow. The type parameter specifies whether we want probabilities or classifications returned. The object returned is a tibble with columns of the predicted probability of the observation being in each class. logi_preds &lt;- predict(object = logi_churn, new_data = test_churn, type = &quot;class&quot;) Specifying type = &quot;class&quot; will generate classification predictions based off of a 0.5 threshold (this might not be optimal for the problem). Let’s add them to our test set. test_metrics &lt;- test_churn %&gt;% bind_cols(logi_preds) This adds a column, .pred_class at each observation. We can produce a confusion matrix using conf_mat() function of the {yardstick} package (used for measuring model performance, also part of {tidymodels}). We tell conf_mat() that the direction column is our source of truth, and our classifications are contained in the .pred_class column. cm_churn &lt;- test_metrics %&gt;% conf_mat(truth = churn, estimate = .pred_class) cm_churn ## Truth ## Prediction No Yes ## No 1156 208 ## Yes 142 252 conf_mat objects also have a summary() method that computes various classification metrics. summary(cm_churn) %&gt;% filter(.metric %in% c(&#39;accuracy&#39;, &#39;sens&#39;, &#39;spec&#39;)) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.801 ## 2 sens binary 0.891 ## 3 spec binary 0.548 Our overall accuracy is around ~80.1%. This may seem high, but if we look at the original dataset, the data is skewed. A naive classifier would produce a 73.5% accuracy (the proportion of yes/no churn values).Still, it suggests that we are better off than randomly guessing. Can we account for the skew of data when assessing our model? If we utilize the yardstick::metrics() function on our test data fitted with class predictions, we can get a dataframe containing overall model performance. test_metrics %&gt;% metrics(truth = churn, estimate = .pred_class) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.801 ## 2 kap binary 0.460 Notice the metric of kap. The Kappa statistic compares an observed accuracy with expected accuracy (random chance). This controls for the skewness of our model by comparing our model’s accuracy with the naive classifier. In this case, our kap of 0.460 indicates that our model is 46% better than the naive classifier. We will be using kap to evaluate how our logistic regression fits to other techniques. 4.7.3 Linear discriminant analysis set.seed(40) lda_churn &lt;- discrim_linear() %&gt;% fit(data = train_churn, churn ~ .) Calling the parsnip:: model object gives us information about the model fit. We can see the prior probabilities as well as the coefficients of linear discriminants. These provide the linear combination of the variables used to create the decision rule. If the combination is large, the model will predict an increase. lda_churn$fit$scaling %&gt;% as_tibble(rownames = &quot;term&quot;) %&gt;% arrange(LD1) ## # A tibble: 30 x 2 ## term LD1 ## &lt;chr&gt; &lt;dbl&gt; ## 1 contractOne year -0.519 ## 2 online_securityYes -0.422 ## 3 contractTwo year -0.362 ## 4 tech_supportYes -0.285 ## 5 phone_service -0.220 ## 6 online_backupYes -0.209 ## # … with 24 more rows We see similar important terms as the logistic regression model. Customers on long contracts are less likely to churn. lda_preds &lt;- predict(lda_churn, test_churn, type = &quot;class&quot;) test_churn %&gt;% bind_cols(lda_preds) %&gt;% metrics(truth = churn, estimate = .pred_class) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.787 ## 2 kap binary 0.429 Here we achieve a kap of 42.9%. 4.7.4 K-Nearest Neighbors Next, we will utilize the parsnip::nearest_neighbor() function. set.seed(40) knn_churn &lt;- nearest_neighbor(mode = &quot;classification&quot;) %&gt;% fit(data = train_churn, churn ~ .) knn_preds &lt;- predict(object = knn_churn, new_data = test_churn) test_churn %&gt;% bind_cols(knn_preds) %&gt;% metrics(truth = churn, estimate = .pred_class) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.745 ## 2 kap binary 0.341 K-nearest neighbors performs much worse, with a kap of 34.1%. KNN’s poor performance is most likely due to the predictor variables and true decision boundary not being highly non-linear. In this case, KNN’s flexibility doesn’t match the shape of the data. 4.7.5 Choosing the model Given logistic regression and LDA exhibiting similar performance metrics, we could use either model. However, logistic regression makes less assumptions about the underlying data. Let’s take a look at the ROC curve using yardstick::roc_curve() function. # https://stats.stackexchange.com/questions/188416/discriminant-analysis-vs-logistic-regression logi_preds &lt;- predict(logi_churn, test_churn, type = &quot;prob&quot;) test_churn %&gt;% bind_cols(logi_preds) %&gt;% roc_curve(churn, .pred_Yes) %&gt;% autoplot() Our sensitivity (ability to correctly identify those who will churn) has two visible inflection points where the slope changes, one around ~55% and the other around ~80%. What should we pick as our classifier threshold? 4.7.6 Evaluating the threshold Choosing a classification threshold greatly depends on the problem context. Imagine we want to hand out an incentive to customers we believe will churn. This incentive costs us money, but has a 100% success rate of retaining a customer (turning them from a Yes to No churn). Let’s assume the cost of this incentive for us is $75, but the value of retaining the user is $150. We want to maximize the return we get from this incentive. In this case, every true positive will net us $75 of profit, and every false positive will cost us $75 (the customer doesn’t churn, so giving the incentive was a waste). We want to find the optimal decision boundary where we maximize our return from this incentive. ltv = 150 incentive = 75 logi_class &lt;- predict(logi_churn, test_churn, type = &quot;class&quot;) costs &lt;- test_churn %&gt;% bind_cols(logi_preds) %&gt;% bind_cols(logi_class) %&gt;% roc_curve(churn, .pred_Yes) %&gt;% mutate(profit = sensitivity * (ltv - incentive), cost = (1 - specificity) * incentive, net = profit - cost) costs %&gt;% ggplot(aes(x = sensitivity, y = net)) + geom_line() + geom_line(aes(y = max(net))) We can grab the optimal sensitivity and find the corresponding decision threshold. optimal_sens &lt;- costs %&gt;% filter(net == max(net)) optimal_sens ## # A tibble: 1 x 6 ## .threshold specificity sensitivity profit cost net ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.308 0.757 0.764 57.3 18.3 39.1 We find that our ideal sensitivity of 76.4% corresponds with a specificity of 75.7%. For whatever reason, roc_curve() gives the .threshold column the predicted No churn class (despite specifying an ROC curve with .pred_Yes column). Thus, if we eastimate a customer to have a churn percentage greater or equal to 1 - 0.308 = 0.692, we can offer them this incentive. yes_threshold &lt;- 1 - (optimal_sens$.threshold) test_churn %&gt;% bind_cols(logi_preds) %&gt;% filter(.pred_Yes &gt;= yes_threshold) %&gt;% group_by(churn) %&gt;% tally() %&gt;% mutate(revenue = if_else(churn == &quot;No&quot;, n * -incentive, n * (ltv - incentive))) ## # A tibble: 2 x 3 ## churn n revenue ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 No 39 -2925 ## 2 Yes 103 7725 This incentive program will cost us ~$23k through false positives (handing out incentives to customers that won’t churn), but will net us ~$26k in revenue from true positives. It’s a gross oversimplification, but we expect this program to yield us ~11% profit margin. 4.8 Conclusion We demonstrated how a classification model could be used to solve a real-world problem in which we want to maximize the value of sending incentives to customers that are likely to churn. 4.9 Exercises "],
["resampling-methods.html", "Chapter 5 Resampling Methods 5.1 Cross-Validation 5.2 The Bootstrap", " Chapter 5 Resampling Methods library(tidyverse) library(knitr) library(skimr) library(ISLR) library(tidymodels) Resampling methods involve repeatedly drawing samples from a training set and refitting a model of interest on each sample. This provides additional information about the fitted model. If we wanted to estimate the variability of a linear regression fit, we could repeatedly draw different samples from the training data, fit a linear regression model to each sample, and then measure how our fits differ. This can yield us novel information that would otherwise not be available from fitting a single model using the entirety of the training data. It is computationally expensive to repeatedly fit statistical methods across different subsets of data. In most cases, this cost is not prohibitive. In this chapter, we will look at two of the most common resampling methods, cross-validation and the bootstrap. Cross-validation can be used to estimate the test error associated with a given statistical learning method or to select the appropriate level of flexibility. Evaluating a model’s performance is known as model assessment. Selecting the proper level of flexibility for a model is model selection. The bootstrap is most commonly used to provide a measure of accuracy of a parameter estimate or of a given statistical learning method. 5.1 Cross-Validation Let’s remember the distinction between the test error rate and the training error rate. The test error is the average error that results from using the statistical learning method to predict the response on a new observation (test dataset). In general, the method with the lowest test error is warranted to use. If a designated test set is available, then calculating the test error is trivial. Unfortunately, there are times when a a reasonably-sized test dataset is difficult or impossible to achieve. The training error can be calculated by applying the statistical learning method to the observations used in its training. But this training error is often quite different from the test error rate, and in particular can significantly underestimate the test error rate. Without a large, designated test set, a number of techniques can be used to estimate this quantity using the training data. Some methods make a mathematical adjustment to the training error rate (discussed in Chapter 6 TODO add link). In this chapter, we instead consider a class of methods that estimate the test error rate by holding out a subset of the training observations from the fitting process. 5.1.1 The Validation Set Approach The validation set approach is a strategy to estimate the test error associated with fitting a particular statistical learning method on a set of observations. It involves randomly diving the available set of observations into two parts, a training set and a validation set or hold-out set. The model is fit on the training set, and the model is then used to predict responses for observations in the validation set. The resulting validation set error rate provides an estimate of the test error rate. Let’s try this out on the ISLR::Auto dataset. We can utilize {rsample} (part of the {tidymodels} ecosystem) to handle this. We used this package in the lab of the previous chapter to split our data into training and test sets. The scope of rsample is to provide the basic building blocks for creating and analyzing resamples of a data set but does not include code for modeling or calculating statistics. auto &lt;- ISLR::Auto %&gt;% as_tibble() split_auto &lt;- initial_split(auto, prop = 1/2) train_auto &lt;- training(split_auto) test_auto &lt;- testing(split_auto) We split the data using initial_split(), and then assigned splits to training and validation (test) sets. nrow(train_auto) == nrow(test_auto) ## [1] TRUE Now let’s fit various regression models on the train_auto dataset. We will repeat the sampling process and fit linear regression models for polynomials from 1st to 10th degree. All in all, we create one-hundred fits, each fit and tested on a random, one-half sample of the data. # TODO fix this with purrr and clean up # copied from https://uc-r.github.io/resampling_methods df_mse &lt;- tibble(sample = vector(&quot;integer&quot;, 100), degree = vector(&quot;integer&quot;, 100), mse = vector(&quot;double&quot;, 100)) counter &lt;- 1 for(i in 1:10) { # random sample set.seed(i) split_auto &lt;- initial_split(auto, prop = 1/2) train_auto &lt;- training(split_auto) test_auto &lt;- testing(split_auto) # modeling for(j in 1:10) { lm.fit &lt;- linear_reg() %&gt;% fit(data = train_auto, formula = mpg ~ poly(horsepower, j)) lm_preds &lt;- predict(lm.fit, new_data = test_auto) # calculate mse mse &lt;- test_auto %&gt;% bind_cols(lm_preds) %&gt;% summarise(mse = mean((.pred - mpg) ^ 2)) %&gt;% pull() # add degree &amp; mse values to tibble df_mse[counter, 2] &lt;- j df_mse[counter, 3] &lt;- mse # add sample identifier df_mse[counter, 1] &lt;- i counter &lt;- counter + 1 } next } ggplot(df_mse, aes(x = degree, y = mse, color = factor(sample))) + geom_line(show.legend = FALSE) + geom_point(show.legend = FALSE) + #scale_x_continuous(breaks = scales::pretty_breaks()) ylim(c(10, 30)) While the curves vary in MSE values, they all exhibit a similar shape. All ten curves indicarte that the model with a quadratic term has a significantly smaller MSE than the linear term. All curves also include there isn’t much additionao benefit in extended beyond cubic or higher-order polynomials in the model. However, the variability of the curves and different test MSE estimates generated by each fit can only lead us to conclude that a linear fit is not adequate for this data. There are two drawbacks of the validation set approach: The validation estimate of the test error rate can be highly variable depending on which observations are included in the training set and which are included in the validation set. Only a subset of observations are used to fit the model. Statisticsl methods perform worse when trained on fewer observations. In some cases, this sample size problem can lead the validation set error rate to be an overestimate of the test error rate. 5.1.2 Leave-One-Out Cross-Validation. Leave-one-out cross-validation (LOOCV) is closely related to the validation set approach. LOOCV also involves splitting the set of observations into two parts. However, a single observations \\((x_1, y_1)\\) is used for the validation set. The remaining observations make up the training set. We fit the model on the \\(n - 1\\) training observations, and make a prediction for the excluded observation. Since the held-out observation was not used in the training set, it provides an approximately unbiased estimate for the test error. While it may be unbiased, it is highly variable, causing it to be a poor estimate. We can repeat this procedure by holding out differnet observations, again fitting the model on the remaining \\(n - 1\\) observations, and computing the MSE of the held-out observation. If we repeat this approach \\(n\\) times, we generate \\(n\\) squared errors. The LOOCV estimate for the test MSE is the average of these \\(n\\) test error estimates. \\(CV_(n) = \\frac{1}{n} \\sum_{i=1}^{n}MSE_i\\) LOOCV has a couple of major advantages over the validation set approach. First, it has far less bias. We are repeatedly fitting the statistical learning method to almost all the observations in the entire dataset. As a result, it tends to not overestimate the test error rate as much as the validation set approach does. Secondly, performing LOOCV multiple times will always yield the same results. There is no randomness in the training/validation set splits. Since LOOCV has to fit \\(n\\) times, it can be expensive to implement on large datasets or high-computational statistical learning methods. For least squares linear or polynomial regression, the cost of LOOCV is actually the same as a single model fit if we take advantage of an observation’s leverage. \\(CV_(n) = \\frac{1}{n} \\sum_{i=1}^{n}(\\frac{y_i - \\hat{y}_i}{1-h_i})^2\\) In this case, we divide residuals of a fitted observation by how much that observation influences its own fit. LOOCV can be used with any kind of predictive modeling. 5.1.3 k-Fold Cross-Validation An alternative to LOOCV is k-fold CV. This approach involves randomly dividing the set of observations into k groups, or folds, of approximatelty equal size. The first fold is treated as a valdation set, and the method is fit on the remaining \\(k - 1\\) folds. The mean squared error, \\(MSE_1\\), is then computed on the observations in the held-out fold. This procedure is repeated \\(k\\) times. Each time, a different group of observations is treated as a validation set. This process results in \\(k\\) estimates of the test error, \\(MSE_1, MSE_2, ..., MSE_k\\). The \\(k\\)-fold CV estimate computed by averaging these values, \\(CV_(k) = \\frac{1}{k} \\sum_{i=1}^{k}MSE_i\\) We can also think of LOOCV as a special case of \\(k\\)-fold CV in which \\(k\\) is set equal to \\(n\\). It is common to find \\(k\\)-fold CV operations using \\(k = 5\\) or \\(k = 10\\). Part of using small values of \\(k\\) is computational costs, especially with large datasets and/or expensive learning methods. Later on, we will also discus non-computational advantages of low \\(k\\)-fold CV, which involve the bias-variance tradeoff. With real data, we do not know the true test MSE. With simulated data, we can compute the true test MSE and actually compare how accurate cross-validation is. # TODO v-fold CV When we perform cross-validation, our goal might be to determine how well a statistical learning method can be expected to perform on independent date. In other cases, we are interested in the location of the minimum point in the estimated test MSE curve. This is because we might be performing cross-validation on a number of different statistical learning methods, or the same method with varying levels of flexibility. We can use the minimum point in the estimated test MSE curve to choose an appropriate model. While CV curves can over or underestimate the true test MSE, they generally come close to identifying the correct level of model flexibility. 5.1.4 Bias-Variance Trade-Off for k-fold Cross-Validation \\(k\\)-fold validation with \\(k &lt; n\\) has computational advantages to LOOCV. It also often gives more accurate estimates of the test error rate than does LOOCV due to the bias-variance trade-off. We know that the validation set approach tends to overestimate the test error due to training on only half of the available data. It is not hard to conclude that LOOCV, which uses training sets of \\(n - 1\\) observations (virtually the entire data set), will give approximately unbiased estimates of the test error. \\(k\\)-fold CV, which is trained on more observations than the validation set approach but less than LOOCV, will produce an intermediate level of bias. From the perspective of bias reduction, LOOCV is the winner. However, bias is not the only source of test error. The procedure’s variance is equally as important. LOOCV has higher variance than \\(k\\)-fold with \\(k &lt; n\\). LOOCV always comparing models that are trained on an almost identical set of observations. These outputs are highly correlated, with each other. In contrast, \\(k\\)-fold with values \\(k &lt; n\\) averages \\(k\\) models that share less overlap in observations. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than the test error estimate resulting from \\(k\\)-fold CV. There is a bias-variance trade-off associated with the choice of \\(k\\) in \\(k\\)-fold cross-validation. Empirical evidence shows that \\(k\\) values between 5 and 10 tend to yield test error estimates with optimal bias-variance trade-off. 5.1.5 Cross-Validation on Classification Problems Thus far, we have shown cross-validation as it relates to regression, using MSE as an evaluation metric. But cross-validation is equally as useful in the classification setting. Instead of MSE, we can use the number of misclassified observations. In the classification setting, the LOOCV error rate takes the form \\(CV_{(n)} = \\frac{1}{n} \\sum_{i=1}^{n}Err_i\\) Where \\(Err_i\\) are the number of misclassified observations. The \\(k\\)-fold CV error rate and validation set error rates are defined analogously. As an example, let’s fit various logistic regression models on some two-dimensional classification data. First, we generate two-dimensional data with classes exhibiting visual, but not perfect, separation. set.seed(10) sim_sep &lt;- tibble(x = rnorm(5000, mean = 10), y = rnorm(5000, mean = 20), class=rep(c(-0, 1), c(2500, 2500))) # add some separation sim_sep &lt;- sim_sep %&gt;% mutate(y = if_else(class == 1, y + runif(1, 1, 3), y), class = as.factor(class)) sim_sep %&gt;% ggplot(aes(x=x,y=y,colour=class)) + geom_point() Now let’s fit logistic regression models of varying flexibilities. We can extend our original logistic regression to obtain a non-linear decision boundary by using polynomial functions of the predictors. Below is an example of quadratic logistic regression with two predictors. \\[ \\log \\left(\\frac{p}{1-p}\\right)=\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{1}^{2}+\\beta_{3} X_{2}+\\beta_{4} X_{2}^{2} \\] We will use this extension to fit logistic regression models from degrees 1 through 4. split_sim &lt;- initial_split(sim_sep, prop = 3/4) train_sim &lt;- training(split_sim) test_sim &lt;- testing(split_sim) # TODO loop logi_sim &lt;- logistic_reg(mode = &quot;classification&quot;) %&gt;% fit(train_sim, formula = class ~ x + y) logi_sim_exp2 &lt;-logistic_reg(mode = &quot;classification&quot;) %&gt;% fit(train_sim, formula = class ~ poly(x, 2) + poly(y, 2)) logi_sim_exp3 &lt;- logistic_reg(mode = &quot;classification&quot;) %&gt;% fit(train_sim, formula = class ~ poly(x, 3) + poly(y, 3)) logi_sim_exp4 &lt;- logistic_reg(mode = &quot;classification&quot;) %&gt;% fit(train_sim, formula = class ~ poly(x, 4) + poly(y, 4)) Let’s get the test error rate for each model. # TODO loop fit &lt;- test_sim %&gt;% bind_cols(predict(logi_sim, new_data=.)) %&gt;% metrics(class, .pred_class) fit_exp2 &lt;- test_sim %&gt;% bind_cols(predict(logi_sim_exp2, new_data=.)) %&gt;% metrics(class, .pred_class) fit_exp3 &lt;- test_sim %&gt;% bind_cols(predict(logi_sim_exp3, new_data=.)) %&gt;% metrics(class, .pred_class) fit_exp4 &lt;- test_sim %&gt;% bind_cols(predict(logi_sim_exp4, new_data=.)) %&gt;% metrics(class, .pred_class) bind_rows(list(lin = fit, quad = fit_exp2, cubic = fit_exp3, quadric = fit_exp4), .id = &quot;id&quot;) %&gt;% filter(.metric == &quot;accuracy&quot;) The \\(k\\)-fold CV follows the true test error rate closer than the training error does. While it doesn’t perfectly estimate it, it gives a good idea of ideal model flexibility as well as a less biased estimate of the test error. 5.2 The Bootstrap The bootstrap is used to quantify the uncertainty associated with a given estimator or statistical learning method. "]
]
