---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Linear Regression

-----

Linear regression is a simple yet very powerful approach in statistical learning. It is important to have a strong understanding of it before moving on to more complex learning methods.

## Simple Linear Regression

Simple linear regression is predicting a quantitative response $Y$ based off a single predcitor $X$.

It can be written as below:

<div>
<p style="text-align:center">$Y \approx \beta_0 + \beta_1X$</p>
<p class="vocab" style="text-align:right">*simple linear regression*</p>
</div>

$\beta_0$ and $\beta_1$ represent the *intercept* and *slope* terms and are together known as the *coefficients*.
$\beta_0$ and $\beta_1$ represent the unknown *intercept* and *slope* terms and are together known as the *coefficients*. We will use our training data to estimate these parameters and thus estimate the response $Y$ based on the value of $X = x$:

<div>
<p style="text-align:center">$\hat y = \hat\beta_0 + \hat\beta_1x$</p>
</div>


### Estimating the Coefficients

We need to use data to estimate these coefficients.

<div>
<p style="text-align:center">$(x_1,y_1), (x_2,y_2),..., (x_n,y_n)$</p>
</div>

These represent the training observations, in this case pairs of $X$ and $Y$ measurements. The goal is to use these measurements to estimate $\beta_0$ and $\beta_1$ such that the linear model fits our data as close as possible. Measuring *closeness* can be tackled a number of ways, but [least squares](https://en.wikipedia.org/wiki/Least_squares) is the most popular.

If we let $\hat y_i = \hat\beta_0 + \hat\beta_1x_i$ be the prediction of $Y$ at observation $X_i$, then $e_i = y_i - \hat y_i$ represents the $i$th *residual*, the difference between the observed value $y_i$ and the predicted value $\hat y_i$. Now we can define the *residual sum of squares (RSS)* as 

<div>
<p style="text-align:center">$RSS = e_1^2 + e_2^2 + ... + e_n^2$</p>
<p class="vocab" style="text-align:right">*residual sum of squares*</p>
</div>

or more explicitly as

<div>
<p style="text-align:center">$RSS = (y_1 - \hat\beta_0 - \hat\beta_1x_2)^2 + (y_2 - \hat\beta_0 - \hat\beta_1x_2)^2 + ... + (y_n - \hat\beta_0 - \hat\beta_1x_n)^2$</p>
</div>

Minimizing the RSS (proof can be found [here](https://en.m.wikipedia.org/wiki/Simple_linear_regression#Derivation_of_simple_regression_estimators)) using $\beta_0$ and $\beta_1$ produces:

<div>
<p style="text-align:center">$\frac{\displaystyle \sum_{i=1}^{n}(x_i-\bar x)(y_i - \bar x)}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2}$</p>
<p class="vocab" style="text-align:right">*least squares coefficient estimates (simple linear regression)*</p>
</div>

### Assessing the Accuracy of the Coefficient Estimate

Remember  that the true function for $f$ contains a random error term $\epsilon$. This means the linear relationship can be written as

<div>
<p style="text-align:center">$Y = \beta_0 + \beta_1X + \epsilon$</p>
<p class="vocab" style="text-align:right">*population regression line*</p>
</div>

$\beta_0$ is the intercept term (value of $Y$ when $X = 0$). $\beta_1$ is the slope (how much does $Y$ change with one-unit change of $X$). $\epsilon$ is the error term that captures everything our model doesn't (unknown variables, measurement error, unknown true relationship).

The population regression line captures the best linear approximation to the true relationship between $X$ and $Y$. In real data, we often don't know the true relationship and have to rely on a set of observations. Using the observations to estimate the coefficients via least squares produces the *least squares line*. Let's simulate and visualize this relationship:

  - simulate `n = 200` observations
  - compare the population regression line (`sim_y`) to a number of possible least squares lines (generated from 10 different training sets of the data)

```{r}

# f(x), or Y = 2 + 2x + error

sim_linear <- tibble(
  b0 = 2,
  b1 = 2,
  x = 1:100 + rnorm(n = 200, mean = 100, sd = 15),
  err = rnorm(200, sd = 50),
  sim_y = b0 + b1 * x,
  true_y = b0 + b1*x + err
)

# generate 10 training sets
y <- tibble()
for (i in 1:10) {
x <- sample_frac(sim_linear, 0.1) %>% mutate(iter_set = i)
y <- y %>% bind_rows(x)
}

# apply linear model to each sample
by_iter <- y %>%
  group_by(iter_set) %>%
  nest()
lm_model <- function(df) {
  lm(true_y ~ x, data = df)
}
by_iter <- by_iter %>%
  mutate(model = map(data, lm_model),
         preds = map2(data, model, add_predictions))

# extract predictions
preds <- unnest(by_iter, preds)

ggplot(data = sim_linear, aes(x = x, y = true_y)) +
  geom_point(alpha = 1/3) +
  geom_line(data = preds, aes(x = x, y = pred, colour = iter_set, group = iter_set), linetype = "F1", size = .75) +
  geom_line(aes(y = sim_y), colour = "red", size = 1.5) +
  theme_minimal() +
  theme(legend.position = "none", panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(), axis.line = element_line(colour = "grey92")) +
  labs(title = "Each least squares line provides a reasonable estimate",
       y = "y")
```

The chart above demonstrates the population regression line (red) surrounded by ten different estimates of the least squares line. Notice how every least squares line (shades of blue) is different. This is because each one is generated from a random sample pulled from the simulated data. For a real-world comparison, the simulated data would be the entire population data which is often impossible to obtain. The observations used to generate the least squares line would be the sample data we have access to. In the same way a sample mean can provide a reasonable estimate of the population mean, fitting a least squares line can provide a reasonable estimate of the population regression line.

This comparison of linear regression to estimating population means touches on the topic of bias. An estimate of $\mu$ using the the sample mean $\hat\mu$ is unbiased. On average, the sample mean will not systemically over or underestimate $\mu$. If we were to take a large enough estimates of $\mu$, each produced by a particular set of observations, then this average would exactly equal $\mu$. This concept applies to our estimates of $\beta_0, \beta_1$ as well. 

A question that can be asked is how close on average the sample mean $\hat\mu$ is to $\mu$. We can compute the *standard error* of $\hat\mu$ to answer this.

<div>
<p style="text-align:center">$Var(\hat\mu) = SE(\hat\mu)^2 = \sigma^2/n$</p>
<p class="vocab" style="text-align:right">*standard error*</p>
</div>

This formula measures the average amount that $\hat\mu$ differs from $\mu$. As the number of observations $n$ increases, the standard error decreases.

We can also use this to calculate how close $\hat\beta_0, \hat\beta_1$ are to $\beta_0, \beta_1$.

<div>
<p style="text-align:center">$SE(\hat\beta_0)^2= \sigma^2 \left[1/n + \frac{\displaystyle \bar x^2}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2} \right]$</p>
</div>

<div>
<p style="text-align:center">$SE(\hat\beta_1)^2=\frac{\displaystyle \sigma^2}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2}$</p>
</div>

where $\sigma^2 = Var(\epsilon)$. For this to work, the assumption has to be made that the error terms $\epsilon_i$ are uncorrelated and all share a common variance. This is often not the case, but it doesn't mean the formula can't be used for a decent approximation. $\sigma^2$ is not known, but can be estimated from training observations. This estimate is the *residual standard error* and is given by formula $RSE = \sqrt{RSS/(n-2}$.

What can we use these standard error formulas for? A useful technique is to calculate *confidence intervals* from the standard error. If we wanted to compute a 95% confidence interval for $\beta_0,\beta_1$, it would take the form below.

<div>
<p style="text-align:center">$\hat\beta_1 \pm 2 * SE(\hat\beta_1)$</p>
</div>

<div>
<p style="text-align:center">$\hat\beta_0 \pm 2 * SE(\hat\beta_0)$</p>
</div>

Standard errors can also be used to perform hypotheses tests.

<div>
<p style="text-align:center">$H_0$: There is no relationship between $X$ and $Y$, or $\beta_1 = 0$</p>
<p class="vocab" style="text-align:right">*null hypothesis*</p>
</div>

<div>
<p style="text-align:center">$H_0$: There exists a relationship between $X$ and $Y$, or $\beta_1 \neq 0$</p>
<p class="vocab" style="text-align:right">*alternative hypothesis*</p>
</div>

To test the null hypothesis, we need to test whether $\hat\beta_1$ is far enough away from zero to conclude that is it non-zero. How far enough from zero is determined by the value of $\hat\beta_1$  as well as $SE(\hat\beta_1)$. We compute a *t-statistic*

<div>
<p style="text-align:center">$t = (\beta_1 - 0)/SE(\hat\beta_1)$</p>
<p class="vocab" style="text-align:right">*t-statistic*</p>
</div>

This measures how many standard deviations $\hat\beta_1$ is from 0. If there is no relationship between $X$ and $Y$, then $t$ will follow a t-distribution. The t-distribution is similar to the normal distribution, but has slightly heavier tails. Like the normal distribution, we can use this to compute the probability of observing any number equal to or larger than $|t|$. This probability is the *p-value*. We can interpret a p-value as the probability we would observe the sample data that produced the $t$-statistic, given that there is no actual relationship between the predictor $X$ and the response $Y$. This means that a small p-value supports the inference that there exists a relationship between the predictor and the response. In this case, based on whichever threshold $\alpha$ (common value is 0.05) we set, a small enough p-value would lead us to reject the null hypothesis.

### Assessing the Accuracy of the Model

Now that we determined the existence of a relationship, how can we measure how well the model fits the data?

Measuring the quality of a linear regression fit is often handled by two quantities: the *residual standard error* and the *R^2* statistic.

#### Residual Standard Error

Since every observation has an associated error term $\epsilon$, having the knowledge of true $\beta_0$ and $\beta_1$ will still not allow one to perfectly predict $Y$. The residual standard error estimates the standard deviation of the error term.

<div>
<p style="text-align:center">$RSE = \sqrt{1/(n-2)*RSS} = \sqrt{1/(n-2)\sum_{i=1}^{n}(y_i - \hat y)^2}$</p>
<p class="vocab" style="text-align:right">*residual standard error*</p>
</div>

We can interpret the residual standard error as how much, on average, our predictions deviate from the true value. Whether the value is acceptable in terms of being a successful model depends on the context of the problem. Predicting hardware failure on an airplane would obviously carry much more stringent requirements than predicting the added sales from a change in a company's advertising budget.


#### R^2 statistic

The RSE provides an absolute number. Given that it depends on the scale of $Y$, comparing RSE values across different domains and datasets isn't useful. The R^2 statistic solves this problem by measuring in terms of proportion -- it measures the variance explained and so always takes a value between 0 and 1.

<div>
<p style="text-align:center">$R^2 = (TSS - RSS)/TSS = 1 - RSS/TSS$</p>
<p class="vocab" style="text-align:right">*R^2 statistic*</p>
</div>

where $TSS = \sum_{i=1}^{n}(y_i-\bar y)^2$ is the *total sum of squares*. TSS can be thought of the amount of total variability in the response variable before any model is fitted to it. RSS is measured after fitting a model, and measures the amount of unexplained variance remaining in the data. Therefore, R^2 can be thought of as the proportion of variance in the data that is explained by fitting a model with $X$. While R^2 is more intrepetable, determing what constitutes a R^2 is subjective to the problem. Relationships that are known to be linear with little variance would expect an R^2 very close to 1. In reality, a lot of real-world data is not truly linear and could be heavily influenced by unknown, immeasurable predictors. In such cases a linear approximation would be a rough fit, and a smaller R^2 would not be unordinary.

There is a relation between R^2 and the correlation.

<div>
<p style="text-align:center">$r = Cor(X,Y) = \sum_{i=1}^{n}((x_i-\bar x)(y_i - \bar y))/(\sqrt{\sum_{i=1}^{n}(x_i - \bar x)^2}\sqrt{\sum_{i=1}^{n}(y_i - \bar y)^2})$</p>
<p class="vocab" style="text-align:right">*correlation*</p>
</div>

Both measure the linear relationship between $X$ and $Y$, and within the simple linear regression domain, $r^2 = R^2$. Once we move into multiple linear regression, in which we are using multiple predictors to predict a response, correlation loses effectiveness at measuring a model in whole as it can only measure the relationship between a single pair of variables.

## Multiple Linear Regression

Simple linear regression works well when the data involves a single predictor variable. In reality, there are often multiple predictor variables. We will need to extend the simple linear regression model and provide each predictor variable $p$ with a slope coefficient. 

<div>
<p style="text-align:center">$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon$</p>
<p class="vocab" style="text-align:right">*multiple linear regression*</p>
</div>

### Estimating the Regression Coefficients

Again, we need to estimate the regression coefficients.

<div>
<p style="text-align:center">$\hat y = \hat\beta_0 + \hat\beta_1X_1 + \hat\beta_2X_2 + ... + \hat\beta_pX_p$</p>
</div>

We will utilize the same approach of minimizing the sum of squared residuals (RSS).

<div>
<p style="text-align:center">$RSS = \sum_{i=1}^{n}(y_i - \hat y_i)^2 = \sum_{i=1}^{n}(y_i - \hat\beta_0 - \hat\beta_1x_{i1} - \hat\beta_2x_{i2} - ... - \hat\beta_px_{ip})^2$</p>
</div>

Minimizing these coefficients is more complicated than the simple linear regression setting, and is best represented using linear algebra. See [this Wikipedia section](https://en.wikipedia.org/wiki/Residual_sum_of_squares#Matrix_expression_for_the_OLS_residual_sum_of_squares) for more information on the formula.

Interpreting a particular coefficient, (say $\beta_1$) in a multiple regression model can be thought of as follows: if constant value for all other $\beta_p$ are maintained, what effect would an increase in $beta_1$ have on $Y$?

A side effect of this is that certain predictors which were deemed significant when contained in a simple linear regression can become insignificant when multiple predictors are involved. For an advertising example, `newspaper` could be a significant predictor of `revenue` in the simple linear regression context. However, when combined with `tv` and `radio` in a multiple linear regression setting, the effects of increasing `newspaper` spend while maintaining `tv` and `radio` becomes insignificant. This could be due to a correlation of `newspaper` spend in markets where `radio` spend is high. Multiple linear regression exposes predictors that act as "surrogates" for others due to correlation.

### Some Important Questions

#### Is There a Relationship Between the Response and Predictors?

To check this, we need to check whethere all $p$ coefficients are zero, i.e. $\beta_1 = \beta_2 = ... = \beta_p = 0$. We test the null hypothesis,

<div>
<p style="text-align:center">$H_o:\beta_1 = \beta_2 = ... = \beta_p = 0$</p>
</div>

against the alternative

<div>
<p style="text-align:center">$H_a:$ at least one $\beta_j$ is non-zero</p>
</div>

The hypothesis test is performed by computing the $F-statistic$,

<div>
<p style="text-align:center">$F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}$</p>
<p class="vocab" style="text-align:right">*correlation*</p>
</div>

If linear model assumptions are correct, one can show that

<div>
<p style="text-align:center">$E\{RSS/(n-p-1)\} = \sigma^2$</p>
</div>

and that, provided $H_o$ is true,

<div>
<p style="text-align:center">$E\{(TSS-RSS)/p\} = \sigma^2$</p>
</div>

In simple terms, if $H_o$ were true and all of the predictors have regression coefficients of 0, we would expect the unexplained variance of the model to be approximately equal to that of the total variance, and both the numerator and the denominator of the F-statistic formula to be equal. When there is no relationship between the response and predictors, the F-statistic will take on a value close to 1. However, as RSS shrinks (the model begins to account for more of the variance), the numerator grows and the denominator shrinks, both causing the F-statistic to increase. We can think of the F-statistic as a ratio between the explained variance and unexplained variance. As the explained variance grows larger than the unexplained portion, the likelihood that we reject the null hypothesis grows.

How large does the F-statistic need to be to reject the null hypothesis? This depends on $n$ and $p$. As $n$ grows, F-statistics closer to 1 may provide sufficient evidence to reject $H_o$. If $H_o$ is true and $\epsilon_i$ have a normal distribution, the F-statistic follows an F-distribution. We can compute the p-value for any value of $n$ and $p$ associated with an F-statistic.

Sometimes we want to test whether a particular subset of $q$ of the coefficients are zero.

The null hypothesis could be 

<div>
<p style="text-align:center">$H_o : \beta_{p-q+1} = \beta_{p-q+2} = \beta_p = 0$</p>
</div>

In this case we fit a second model that uses all the variables except the last $q$. We will call the residual sum of squares for the second model $RSS_0$.

Then, the F-statistic is,

<div>
<p style="text-align:center">$F = \frac{(RSS_0 - RSS)/q}{RSS(n-p-1)}$</p>
</div>

We are testing a model without the $q$ predictors and seeing how it compares to the original model containing all the predictors.

