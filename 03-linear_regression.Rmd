# Linear Regression

-----

Linear regression is a simple yet very powerful approach in statistical learning. It is important to have a strong understanding of it before moving on to more complex learning methods.

## Simple Linear Regression

Simple linear regression is predicting a quantitative response $Y$ based off a single predcitor $X$.

It can be written as below:

<div>
<p style="text-align:center">$Y \approx \beta_0 + \beta_1X$</p>
<p class="vocab" style="text-align:right">*simple linear regression*</p>
</div>

$\beta_0$ and $\beta_1$ represent the unknown *intercept* and *slope* terms and are together known as the *coefficients*. We will use our training data to estimate these parameters and thus estimate the response $Y$ based on the value of $X = x$:

<div>
<p style="text-align:center">$\hat y = \hat\beta_0 + \hat\beta_1x$</p>
</div>


### Estimating the Coefficients

We need to use data to estimate these coefficients.

<div>
<p style="text-align:center">$(x_1,y_1), (x_2,y_2),..., (x_n,y_n)$</p>
</div>

These represent the training observations, in this case pairs of $X$ and $Y$ measurements. The goal is to use these measurements to estimate $\beta_0$ and $\beta_1$ such that the linear model fits our data as close as possible. Measuring *closeness* can be tackled a number of ways, but [least squares](https://en.wikipedia.org/wiki/Least_squares) is the most popular.

If we let $\hat y_i = \hat\beta_0 + \hat\beta_1x_i$ be the prediction of $Y$ at observation $X_i$, then $e_i = y_i - \hat y_i$ represents the $i$th *residual*, the difference between the observed value $y_i$ and the predicted value $\hat y_i$. Now we can define the *residual sum of squares (RSS)* as 

<div>
<p style="text-align:center">$RSS = e_1^2 + e_2^2 + ... + e_n^2$</p>
<p class="vocab" style="text-align:right">*residual sum of squares*</p>
</div>

or more explicitly as

<div>
<p style="text-align:center">$RSS = (y_1 - \hat\beta_0 - \hat\beta_1x_2)^2 + (y_2 - \hat\beta_0 - \hat\beta_1x_2)^2 + ... + (y_n - \hat\beta_0 - \hat\beta_1x_n)^2$</p>
</div>

Minimizing the RSS (proof can be found [here](https://en.m.wikipedia.org/wiki/Simple_linear_regression#Derivation_of_simple_regression_estimators)) using $\beta_0$ and $\beta_1$ produces:

<div>
<p style="text-align:center">$\frac{\displaystyle \sum_{i=1}^{n}(x_i-\bar x)(y_i - \bar x)}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2}$</p>
<p class="vocab" style="text-align:right">*least squares coefficient estimates (simple linear regression)*</p>
</div>


