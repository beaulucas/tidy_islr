---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Linear Regression

-----

Linear regression is a simple yet very powerful approach in statistical learning. It is important to have a strong understanding of it before moving on to more complex learning methods.

## Packages used in this chapter

```{r}
library(tidyverse)
library(modelr)
```

## Simple Linear Regression

Simple linear regression is predicting a quantitative response $Y$ based off a single predcitor $X$.

It can be written as below:

<div>
<p style="text-align:center">$Y \approx \beta_0 + \beta_1X$</p>
<p class="vocab" style="text-align:right">*simple linear regression*</p>
</div>

$\beta_0$ and $\beta_1$ represent the *intercept* and *slope* terms and are together known as the *coefficients*.
$\beta_0$ and $\beta_1$ represent the unknown *intercept* and *slope* terms and are together known as the *coefficients*. We will use our training data to estimate these parameters and thus estimate the response $Y$ based on the value of $X = x$:

<div>
<p style="text-align:center">$\hat y = \hat\beta_0 + \hat\beta_1x$</p>
</div>


### Estimating the Coefficients

We need to use data to estimate these coefficients.

<div>
<p style="text-align:center">$(x_1,y_1), (x_2,y_2),..., (x_n,y_n)$</p>
</div>

These represent the training observations, in this case pairs of $X$ and $Y$ measurements. The goal is to use these measurements to estimate $\beta_0$ and $\beta_1$ such that the linear model fits our data as close as possible. Measuring *closeness* can be tackled a number of ways, but [least squares](https://en.wikipedia.org/wiki/Least_squares) is the most popular.

If we let $\hat y_i = \hat\beta_0 + \hat\beta_1x_i$ be the prediction of $Y$ at observation $X_i$, then $e_i = y_i - \hat y_i$ represents the $i$th *residual*, the difference between the observed value $y_i$ and the predicted value $\hat y_i$. Now we can define the *residual sum of squares (RSS)* as 

<div>
<p style="text-align:center">$RSS = e_1^2 + e_2^2 + ... + e_n^2$</p>
<p class="vocab" style="text-align:right">*residual sum of squares*</p>
</div>

or more explicitly as

<div>
<p style="text-align:center">$RSS = (y_1 - \hat\beta_0 - \hat\beta_1x_2)^2 + (y_2 - \hat\beta_0 - \hat\beta_1x_2)^2 + ... + (y_n - \hat\beta_0 - \hat\beta_1x_n)^2$</p>
</div>

Minimizing the RSS (proof can be found [here](https://en.m.wikipedia.org/wiki/Simple_linear_regression#Derivation_of_simple_regression_estimators)) using $\beta_0$ and $\beta_1$ produces:

<div>
<p style="text-align:center">$\frac{\displaystyle \sum_{i=1}^{n}(x_i-\bar x)(y_i - \bar x)}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2}$</p>
<p class="vocab" style="text-align:right">*least squares coefficient estimates (simple linear regression)*</p>
</div>

### Assessing the Accuracy of the Coefficient Estimate

Remember  that the true function for $f$ contains a random error term $\epsilon$. This means the linear relationship can be written as

<div>
<p style="text-align:center">$Y = \beta_0 + \beta_1X + \epsilon$</p>
<p class="vocab" style="text-align:right">*population regression line*</p>
</div>

$\beta_0$ is the intercept term (value of $Y$ when $X = 0$). $\beta_1$ is the slope (how much does $Y$ change with one-unit change of $X$). $\epsilon$ is the error term that captures everything our model doesn't (unknown variables, measurement error, unknown true relationship).

The population regression line captures the best linear approximation to the true relationship between $X$ and $Y$. In real data, we often don't know the true relationship and have to rely on a set of observations. Using the observations to estimate the coefficients via least squares produces the *least squares line*. Let's simulate and visualize this relationship:

  - simulate `n = 200` observations
  - compare the population regression line (`sim_y`) to a number of possible least squares lines (generated from 10 different training sets of the data)

```{r}

# f(x), or Y = 2 + 2x + error

sim_linear <- tibble(
  b0 = 2,
  b1 = 2,
  x = 1:100 + rnorm(n = 200, mean = 100, sd = 15),
  err = rnorm(200, sd = 50),
  sim_y = b0 + b1 * x,
  true_y = b0 + b1 * x + err
)

# generate 10 training sets
y <- tibble()
for (i in 1:10) {
  x <- sample_frac(sim_linear, 0.1) %>% mutate(iter_set = i)
  y <- y %>% bind_rows(x)
}

# apply linear model to each sample
by_iter <- y %>%
  group_by(iter_set) %>%
  nest()
lm_model <- function(df) {
  lm(true_y ~ x, data = df)
}
by_iter <- by_iter %>%
  mutate(
    model = map(data, lm_model),
    preds = map2(data, model, add_predictions)
  )

# extract predictions
preds <- unnest(by_iter, preds)

ggplot(data = sim_linear, aes(x = x, y = true_y)) +
  geom_point(alpha = 1 / 3) +
  geom_line(data = preds, aes(x = x, y = pred, colour = iter_set, group = iter_set), linetype = "F1", size = .75) +
  geom_line(aes(y = sim_y), colour = "red", size = 1.5) +
  theme_minimal() +
  theme(
    legend.position = "none", panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(), axis.line = element_line(colour = "grey92")
  ) +
  labs(
    title = "Each least squares line provides a reasonable estimate",
    y = "y"
  )
```

The chart above demonstrates the population regression line (red) surrounded by ten different estimates of the least squares line. Notice how every least squares line (shades of blue) is different. This is because each one is generated from a random sample pulled from the simulated data. For a real-world comparison, the simulated data would be the entire population data which is often impossible to obtain. The observations used to generate the least squares line would be the sample data we have access to. In the same way a sample mean can provide a reasonable estimate of the population mean, fitting a least squares line can provide a reasonable estimate of the population regression line.

This comparison of linear regression to estimating population means touches on the topic of bias. An estimate of $\mu$ using the the sample mean $\hat\mu$ is unbiased. On average, the sample mean will not systemically over or underestimate $\mu$. If we were to take a large enough estimates of $\mu$, each produced by a particular set of observations, then this average would exactly equal $\mu$. This concept applies to our estimates of $\beta_0, \beta_1$ as well. 

A question that can be asked is how close on average the sample mean $\hat\mu$ is to $\mu$. We can compute the *standard error* of $\hat\mu$ to answer this.

<div>
<p style="text-align:center">$Var(\hat\mu) = SE(\hat\mu)^2 = \sigma^2/n$</p>
<p class="vocab" style="text-align:right">*standard error*</p>
</div>

This formula measures the average amount that $\hat\mu$ differs from $\mu$. As the number of observations $n$ increases, the standard error decreases.

We can also use this to calculate how close $\hat\beta_0, \hat\beta_1$ are to $\beta_0, \beta_1$.

<div>
<p style="text-align:center">$SE(\hat\beta_0)^2= \sigma^2 \left[1/n + \frac{\displaystyle \bar x^2}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2} \right]$</p>
</div>

<div>
<p style="text-align:center">$SE(\hat\beta_1)^2=\frac{\displaystyle \sigma^2}{\displaystyle\sum_{i=1}^{n}(x_i - \bar x)^2}$</p>
</div>

where $\sigma^2 = Var(\epsilon)$. For this to work, the assumption has to be made that the error terms $\epsilon_i$ are uncorrelated and all share a common variance. This is often not the case, but it doesn't mean the formula can't be used for a decent approximation. $\sigma^2$ is not known, but can be estimated from training observations. This estimate is the *residual standard error* and is given by formula $RSE = \sqrt{RSS/(n-2}$.

What can we use these standard error formulas for? A useful technique is to calculate *confidence intervals* from the standard error. If we wanted to compute a 95% confidence interval for $\beta_0,\beta_1$, it would take the form below.

<div>
<p style="text-align:center">$\hat\beta_1 \pm 2 * SE(\hat\beta_1)$</p>
</div>

<div>
<p style="text-align:center">$\hat\beta_0 \pm 2 * SE(\hat\beta_0)$</p>
</div>

Standard errors can also be used to perform hypotheses tests.

<div>
<p style="text-align:center">$H_0$: There is no relationship between $X$ and $Y$, or $\beta_1 = 0$</p>
<p class="vocab" style="text-align:right">*null hypothesis*</p>
</div>

<div>
<p style="text-align:center">$H_0$: There exists a relationship between $X$ and $Y$, or $\beta_1 \neq 0$</p>
<p class="vocab" style="text-align:right">*alternative hypothesis*</p>
</div>

To test the null hypothesis, we need to test whether $\hat\beta_1$ is far enough away from zero to conclude that is it non-zero. How far enough from zero is determined by the value of $\hat\beta_1$  as well as $SE(\hat\beta_1)$. We compute a *t-statistic*

<div>
<p style="text-align:center">$t = (\beta_1 - 0)/SE(\hat\beta_1)$</p>
<p class="vocab" style="text-align:right">*t-statistic*</p>
</div>

This measures how many standard deviations $\hat\beta_1$ is from 0. If there is no relationship between $X$ and $Y$, then $t$ will follow a t-distribution. The t-distribution is similar to the normal distribution, but has slightly heavier tails. Like the normal distribution, we can use this to compute the probability of observing any number equal to or larger than $|t|$. This probability is the *p-value*. We can interpret a p-value as the probability we would observe the sample data that produced the $t$-statistic, given that there is no actual relationship between the predictor $X$ and the response $Y$. This means that a small p-value supports the inference that there exists a relationship between the predictor and the response. In this case, based on whichever threshold $\alpha$ (common value is 0.05) we set, a small enough p-value would lead us to reject the null hypothesis.

### Assessing the Accuracy of the Model

Now that we determined the existence of a relationship, how can we measure how well the model fits the data?

Measuring the quality of a linear regression fit is often handled by two quantities: the *residual standard error* and the *R^2* statistic.

#### Residual Standard Error

Since every observation has an associated error term $\epsilon$, having the knowledge of true $\beta_0$ and $\beta_1$ will still not allow one to perfectly predict $Y$. The residual standard error estimates the standard deviation of the error term.

<div>
<p style="text-align:center">$RSE = \sqrt{1/(n-2)*RSS} = \sqrt{1/(n-2)\sum_{i=1}^{n}(y_i - \hat y)^2}$</p>
<p class="vocab" style="text-align:right">*residual standard error*</p>
</div>

We can interpret the residual standard error as how much, on average, our predictions deviate from the true value. Whether the value is acceptable in terms of being a successful model depends on the context of the problem. Predicting hardware failure on an airplane would obviously carry much more stringent requirements than predicting the added sales from a change in a company's advertising budget.


#### R^2 statistic

The RSE provides an absolute number. Given that it depends on the scale of $Y$, comparing RSE values across different domains and datasets isn't useful. The R^2 statistic solves this problem by measuring in terms of proportion -- it measures the variance explained and so always takes a value between 0 and 1.

<div>
<p style="text-align:center">$R^2 = (TSS - RSS)/TSS = 1 - RSS/TSS$</p>
<p class="vocab" style="text-align:right">*R^2 statistic*</p>
</div>

where $TSS = \sum_{i=1}^{n}(y_i-\bar y)^2$ is the *total sum of squares*. TSS can be thought of the amount of total variability in the response variable before any model is fitted to it. RSS is measured after fitting a model, and measures the amount of unexplained variance remaining in the data. Therefore, R^2 can be thought of as the proportion of variance in the data that is explained by fitting a model with $X$. While R^2 is more intrepetable, determing what constitutes a R^2 is subjective to the problem. Relationships that are known to be linear with little variance would expect an R^2 very close to 1. In reality, a lot of real-world data is not truly linear and could be heavily influenced by unknown, immeasurable predictors. In such cases a linear approximation would be a rough fit, and a smaller R^2 would not be unordinary.

There is a relation between R^2 and the correlation.

<div>
<p style="text-align:center">$r = Cor(X,Y) = \sum_{i=1}^{n}((x_i-\bar x)(y_i - \bar y))/(\sqrt{\sum_{i=1}^{n}(x_i - \bar x)^2}\sqrt{\sum_{i=1}^{n}(y_i - \bar y)^2})$</p>
<p class="vocab" style="text-align:right">*correlation*</p>
</div>

Both measure the linear relationship between $X$ and $Y$, and within the simple linear regression domain, $r^2 = R^2$. Once we move into multiple linear regression, in which we are using multiple predictors to predict a response, correlation loses effectiveness at measuring a model in whole as it can only measure the relationship between a single pair of variables.

## Multiple Linear Regression

Simple linear regression works well when the data involves a single predictor variable. In reality, there are often multiple predictor variables. We will need to extend the simple linear regression model and provide each predictor variable $p$ with a slope coefficient. 

<div>
<p style="text-align:center">$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon$</p>
<p class="vocab" style="text-align:right">*multiple linear regression*</p>
</div>

### Estimating the Regression Coefficients

Again, we need to estimate the regression coefficients.

<div>
<p style="text-align:center">$\hat y = \hat\beta_0 + \hat\beta_1X_1 + \hat\beta_2X_2 + ... + \hat\beta_pX_p$</p>
</div>

We will utilize the same approach of minimizing the sum of squared residuals (RSS).

<div>
<p style="text-align:center">$RSS = \sum_{i=1}^{n}(y_i - \hat y_i)^2 = \sum_{i=1}^{n}(y_i - \hat\beta_0 - \hat\beta_1x_{i1} - \hat\beta_2x_{i2} - ... - \hat\beta_px_{ip})^2$</p>
</div>

Minimizing these coefficients is more complicated than the simple linear regression setting, and is best represented using linear algebra. See [this Wikipedia section](https://en.wikipedia.org/wiki/Residual_sum_of_squares#Matrix_expression_for_the_OLS_residual_sum_of_squares) for more information on the formula.

Interpreting a particular coefficient, (say $\beta_1$) in a multiple regression model can be thought of as follows: if constant value for all other $\beta_p$ are maintained, what effect would an increase in $beta_1$ have on $Y$?

A side effect of this is that certain predictors which were deemed significant when contained in a simple linear regression can become insignificant when multiple predictors are involved. For an advertising example, `newspaper` could be a significant predictor of `revenue` in the simple linear regression context. However, when combined with `tv` and `radio` in a multiple linear regression setting, the effects of increasing `newspaper` spend while maintaining `tv` and `radio` becomes insignificant. This could be due to a correlation of `newspaper` spend in markets where `radio` spend is high. Multiple linear regression exposes predictors that act as "surrogates" for others due to correlation.

### Some Important Questions

#### Is There a Relationship Between the Response and Predictors?

To check this, we need to check whethere all $p$ coefficients are zero, i.e. $\beta_1 = \beta_2 = ... = \beta_p = 0$. We test the null hypothesis,

<div>
<p style="text-align:center">$H_o:\beta_1 = \beta_2 = ... = \beta_p = 0$</p>
</div>

against the alternative

<div>
<p style="text-align:center">$H_a:$ at least one $\beta_j$ is non-zero</p>
</div>

The hypothesis test is performed by computing the $F-statistic$,

<div>
<p style="text-align:center">$F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}$</p>
<p class="vocab" style="text-align:right">*correlation*</p>
</div>

If linear model assumptions are correct, one can show that

<div>
<p style="text-align:center">$E\{RSS/(n-p-1)\} = \sigma^2$</p>
</div>

and that, provided $H_o$ is true,

<div>
<p style="text-align:center">$E\{(TSS-RSS)/p\} = \sigma^2$</p>
</div>

In simple terms, if $H_o$ were true and all of the predictors have regression coefficients of 0, we would expect the unexplained variance of the model to be approximately equal to that of the total variance, and both the numerator and the denominator of the F-statistic formula to be equal. When there is no relationship between the response and predictors, the F-statistic will take on a value close to 1. However, as RSS shrinks (the model begins to account for more of the variance), the numerator grows and the denominator shrinks, both causing the F-statistic to increase. We can think of the F-statistic as a ratio between the explained variance and unexplained variance. As the explained variance grows larger than the unexplained portion, the likelihood that we reject the null hypothesis grows.

How large does the F-statistic need to be to reject the null hypothesis? This depends on $n$ and $p$. As $n$ grows, F-statistics closer to 1 may provide sufficient evidence to reject $H_o$. If $H_o$ is true and $\epsilon_i$ have a normal distribution, the F-statistic follows an F-distribution. We can compute the p-value for any value of $n$ and $p$ associated with an F-statistic.

```{r}
# TODO remove this section?
```

Sometimes we want to test whether a particular subset of $q$ of the coefficients are zero.

The null hypothesis could be 

<div>
<p style="text-align:center">$H_o : \beta_{p-q+1} = \beta_{p-q+2} = \beta_p = 0$</p>
</div>

In this case we fit a second model that uses all the variables except the last $q$. We will call the residual sum of squares for the second model $RSS_0$.

Then, the F-statistic is,

<div>
<p style="text-align:center">$F = \frac{(RSS_0 - RSS)/q}{RSS(n-p-1)}$</p>
</div>

We are testing a model without the $q$ predictors and seeing how it compares to the original model containing all the predictors.

Why do we need to look at overall F-statistics if we have individual p-values of the predictors? There are scenarios where individual predictors, by chance, will have *small* p-values, even in the absence of any true association. This could lead us to incorrectly diagnose a relationship. 

The overall F-statistic does not suffer this problem because it adjusts for the number of predictors.

The F-statistic approach works when the number of predictors $p$ is small compared to $n$. Sometimes, we have situations where $p > n$. In this situation, ther eare more coefficients $\beta_j$ to estimate than observations from which to estimate them. Such situations requires different approaches that we haven't discussed yet (see chapter 6)

```{r}
# TODO add chapter 6 link
```

#### Deciding on Important Variables

The first thing we do in a multiple regression is to compute the F-statistic and determine that at least one of the predictors is related to the response.

The task of determining which predictors are associated with the response is referred to as *variable selection*. We could try out a lot of different models with combinations of predictors, $2^p$, but this is not practical as $p$ grows.

There are three ways to approach this task:

* *Forward selection*: we begin with the *null model*, which contains an intercept but no predictors. We then fit $p$ simple linear regressions and add to the null model the variable that results in the lowest RSS. We then repeat the process to determine the lowest RSS of the now two-variable model, continuing until some stopping rule is satisfied.

* *Backward selection*: Start with all the variables in the model, remove the variable with the largest p-value. Then, for the new $(p - 1)$-variable model, do the same. Continue until stopping rule is reached (for example, some p-value threshold)

* *Mixed selection*: Start with no variables, and proceed with forward selection. If any p-value of added variables pass a threshold once new predictors are added, we remove them. We continue the forward and backward until all variables in model have a sufficiently low p-value.

#### Model Fit

Two common methods of model fit are the $RSE$ and $R^2$, the fraction of variance explained.

More on $R^2$:

* Values closer to `1` indicate a better fit
* Adding more variables can only increase it
  + Adding variables that barely increase it can lead to overfitting

Plotting the model can also be useful.


#### Predictions

Three sorts of uncertainty within a given model:

1. The coefficient estimates $\hat\beta_0 + \hat\beta_1...,\hat\beta_p$ are estimates for $\beta_0 + \beta_1...,\beta_p$. This inaccuracy is part of the *reducible error*. We can compute a confidence interval to determine how close $\hat Y$ is to $f(X)$.

2. *Model bias* can result from the fact that we are fitting a linear approximation to the true surface of $f(X)$.

3. Even if we knew $f(X)$, we still have random error $\epsilon$, which is the *irreducible error*. We can use prediction intervals to estimate how far $Y$ will differ from $\hat Y$. These will always be larger than confidence intervals, because they incorporate both the reducible + irreducible error.

### Other Considerations in the Regression Model

So far, all predictors have been *quantitative*. However, it is common to have *qualitative* variables as well.

Take a look at the `ISLR::Credit` dataset, which has a mix of both types.

```{r}
tidy_credit <- ISLR::Credit %>%
  as_tibble() %>%
  janitor::clean_names()
tidy_credit
```


#### Predictors with only Two Levels

Suppose we wish to investigate difference in credit card balance between males and females, ignoring all other variables. If a *qualitative* variable (also known as a *factor*) only has two possible values, then incorporating it into a model is easy. We can create a binomial dummy variable that takes on two values. For `gender`, this could be a variable that is `0` if observation has value `male`, and `1` if observation has value `female`. This variable can then be used in the regression equation.

Take note that `lm()` automatically creates dummy variables when given qualitative predictors.

```{r}
# TODO insert regression equation
```


```{r lm}
credit_model <- lm(balance ~ gender, data = tidy_credit)
tidy_credit_model <- broom::tidy(credit_model)
tidy_credit_model
```

How to interpret this: males are estimated to carry a balance of `$510`. Meanwhile, females are expected to carry an additional $19.70$ in debt. Notice the p-value is very high, indicating there is no significant difference between genders.

### Qualitative Predictors with More than Two Levels

A single dummy variable can not represent all the possible values. We can create additional dummy variables for this.

Let's make a dummy variable from `ethnicity` column, which takes three distinct values. This will yield two dummy variables.

```{r}
tidy_credit %>% distinct(ethnicity)
```

`fastDummies` package will be used to generate these. In this case, `African American` serves as the baseline, and dummy variables are created for `Caucasian` and `Asian`. **There will always be one fewer dummy variable than the number of levels.**

```{r}
tidy_credit_dummy <- tidy_credit %>%
  fastDummies::dummy_cols(select_columns = "ethnicity", remove_first_dummy = TRUE) %>%
  janitor::clean_names()

tidy_credit_dummy %>%
  select(starts_with("ethnicity"))
```

We can again run the model with newly created dummy variables. Keep in mind, prior creation is not necessary, as `lm` will generate them automatically. 

```{r ethniciy credit lm model}
ethnicity_model <- lm(balance ~ ethnicity_asian + ethnicity_caucasian, data = tidy_credit_dummy)
broom::tidy(ethnicity_model)
```

### Extensions of the Linear Model

The linear regression model makes highly restrictive assumptions. Two of the most important are that the relationship between predictors and response are *additive* and *linear*.

Additive means that the effect of changes in a predictor $X_j$ on the response $Y$ is independet of the values of the other predictors. Linear means that the the change in response $Y$ to a one-unit change in $X_j$ is constant, regardless of the value of $X_j$.

Here are some common approaches of extending the linear model.

#### Removing the Additive Assumption

The additive property assumes that predictors slope terms are independent of the values of other predictors. However, this is not always the case. Imagine an advertising scenario where the effectiveness of TV spend is affected by the radio spend. This is known as an *interaction* effect. Imagine we have a model with two predictors, but they are not strictly additive. We could extend this model by adding an *interaction term* to it.

<div>
<p style="text-align:center">$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_1X_2 + \epsilon$</p>
</div>

Now, the effect of $X_1$ on $Y$ is no longer constant; adjusting $X_2$ will change the impact of $X_1$ on $Y$.

An easy scenario is the productivity of a factory. Adding lines and workers both would increase productivity. However, the effect is not purely additive. Adding lines without having workers to operate them would not increase productivity. There is an interaction between workers and lines that needs to be accounted for.

The *hierarchical principle* states that *if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant*.

It's also possible for qualitative and quantitative variables to interact with each other. We will again use the `Credit` data set. Suppose we wish to predict `balance` using the `income` (quantitative) and `student` (qualitative) variables.

First, let's take a look at what it looks like to fit this model without an interaction term. Both `income` and `student` are significant.

```{r}
lm_credit <- lm(balance ~ income + student, data = tidy_credit)
lm_credit %>% broom::tidy()
```

```{r}
tidy_credit %>%
  modelr::add_predictions(lm_credit) %>%
  ggplot(aes(x = income, y = pred, colour = student)) +
  geom_line(size = 1.5) +
  geom_point(aes(y = balance, colour = student), fill = "grey", pch = 21, alpha = 1 / 2) +
  theme_minimal()
```

It's a pretty good fit, and because there is no interaction terms, the lines are parallel. Notice how many more observations there are to fit on for the non-students.

Now, let's add an interaction term.

```{r}
lm_credit_int <- lm(balance ~ income + student + income * student, data = tidy_credit)
lm_credit_int %>% broom::tidy()
```


```{r}
tidy_credit %>%
  modelr::add_predictions(lm_credit_int) %>%
  ggplot(aes(x = income, y = pred, colour = student)) +
  geom_line(size = 1.5) +
  geom_point(aes(y = balance, colour = student), fill = "grey", pch = 21, alpha = 1 / 2) +
  theme_minimal()
```

The model now takes into account how `income` and `student` interact with each other. Interpreting the chart suggests that increases in `income` among students has a smaller effect on balance than it does to non-students.

Does it fit better?

```{r}
models <- list(without_interaction = lm_credit, with_interaction = lm_credit_int)
purrr::map_df(models, broom::glance, .id = "model") %>%
  select(model, r.squared, statistic, p.value, df)
```

Not by much. The model with the interaction term has a slightly higher $R^2$, but the added complexity of the model, combined with the small number of observations of students in the dataset, suggests overfitting.

#### Non-linear Relationships

The linear model assumes a linear relationship between the response and predictors. We can extend the linear model to accomodate non-linear relationships using *polynomial regression*.

A way to incorporate non-linear associations into a linear model is to include transformed versions of the predictor in the model. For example, within the `Auto` dataset, predicting `mpg` with a second-order polynomial of `horsepower` would look like this:

<div>
<p style="text-align:center">$mpg = \beta_0 + \beta_1horsepower + \beta_2horsepower^2 + \epsilon$</p>
</div>

Let's look at the `Auto` dataset with models of different polynomial degrees overlaid. Clearly, the data is not linear, exhibiting a *quadratic* shape.

```{r}
tidy_auto <- ISLR::Auto %>% as_tibble()

lm_auto <- lm(mpg ~ horsepower, data = tidy_auto)
lm_auto2 <- lm(mpg ~ poly(horsepower, 2), data = tidy_auto)
lm_auto5 <- lm(mpg ~ poly(horsepower, 5), data = tidy_auto)

tidy_auto %>%
  gather_predictions(lm_auto, lm_auto2, lm_auto5) %>%
  ggplot(aes(x = horsepower, y = mpg)) +
  geom_point(alpha = 1 / 3, pch = 21) +
  geom_line(aes(y = pred, colour = model), size = 1.5)
```

The second-order polynomial does a good job of fitting the data, while the fifth-order seems to be unnecessary. The model performance reflects that:

```{r}
models <- list(
  linear = lm_auto,
  second_order = lm_auto2,
  fifth_order = lm_auto5
)
purrr::map_df(models, broom::glance, .id = "model") %>%
  select(model, r.squared, statistic, p.value, df)
```

The second-order model has significantly higher R^2, and only one more degree of freedom. 

This approach of extending linear models to accomodate non-linear relationships is known as polynomial regression. 

### Potential Problems

Many problems can occur when fitting a linear model to a data set.

1. Non-linearity of the response-predictor relationship.
2. Correlation of error terms.
3. Non-constant variance of error terms
4. Outliers
5. High-leverage points
6. Collinearity

#### 1. Non-linearity of the data

The linear model assumes a straight-line relationship between the predictors and the response. If this is not the case, the inference and prediction accuracy of the fit are suspect.

We can use a *residual plot* to visualize when a linear model is placed on to a non-linear relationship. For a simple linear regression model, we plot the residuals $e_i = y_i - \hat{y}_i$ compared to the predictor. For multiple regression, we plot the residuals versus the predicted values $\hat{y}_i$. If the relationship is linear, the residuals should exhibit a random pattern.

Let's take the `Auto` dataset and plot the residuals compared to `horsepower` for each model we fit. Notice in the model containing no quadratic term is U-shaped, indicating a non-linear relationship. The model that contains `horsepower^2` exhibits little pattern in the residuals, indicating a better fit.

```{r}
tidy_auto %>%
  gather_predictions(lm_auto, lm_auto2, lm_auto5) %>%
  ggplot(aes(x = horsepower, y = mpg-pred, colour=model)) +
  geom_point(alpha = 1 / 3) +
  geom_hline(yintercept = 0, size = 1.5, colour="grey") +
  facet_wrap(~model, nrow=3)
```

If the residual plot indicates that there non-linear associations in the data, a simple approach is to use non-linear transofmrations of the predictors.

#### 2. Correlation of Error Terms

The linear regression model assumes that the error terms $\epsilon_1,\epsilon_2,...,\epsilon_n$ are uncorrelated.

This means that for a given error term $e_i$, no information is provided about the value $e_{i+1}$. The standard errors that are computed for the estimated regression coefficients are based on this assumption.

If there is a correlation among the error terms, than the estimated standard errors will tend to underestimate the true standard errors, producing confidence and prediction intervals narrower than they should be. Given the incorrect assumption, a 95% confidence interval may have a much lower probability than 0.95 of containing the true value of the parameter. P-values would also be lower than they should be, giving us an unwarranted sense of confidence in our model.

These correlations occur frequently in *time series* data, which consists of observations obtained at discrete points in time. In many cases, observations that are obtained at adjacent time periods points will have positively correlated errors.

We can again plot the residuals as a function of time to see if this is the case. If no correlation, there should be no pattern in the residuals. If error terms exhibit correlation, we may see that adjacent residuals exhibit similar values, known as *tracking*.

This can also happen outside of time series data. The assumption of uncorrelated errors is extremely important for linear regression as well as other statistical methods.

```{r}
# TODO add a residual plot for time series with correlated error terms, similar to pg. 95
```

```{r sunspot, eval = FALSE}
tidy_sunspot <- data.frame(y=as.matrix(sunspot.year), ds=time(sunspot.year)) %>% as_tibble()
tidy_sunspot
sunspot_lm <- lm(data = tidy_sunspot, y ~ ds)
tidy_sunspot %>%
  add_predictions(sunspot_lm) %>% 
  ggplot(aes(x=ds, y=pred-y)) +
  geom_point() +
  geom_line() +
  geom_smooth(method="lm", se = FALSE)
```

#### 3. Non-constant Variance of Error Terms

Another assumption of linear regression is that the error terms have a constant variance, $Var(\epsilon_i) = \sigma^2$. Standard errors, confidence intervals, and hypothesis tests rely upon this assumption.

It is common for error terms to exhiti non-constant variance. Non-constant variance in the errors, also known as *heteroscedasticity*, can be identified from a *funnel shape* in the residual plot.

Let's take a look at the `MASS::cats` dataset, which contains observations of various cats sex, body weight, and heart weight. We fit a linear model to it, and then plot the residuals.

I've added a linear fit to the residual plot itself. Observe how the error terms begin to funnel out as `bwt` increases, indicating non-constant variance of the error terms. 

```{r}
tidy_cats <- MASS::cats %>% as_tibble() %>% janitor::clean_names()
lm_cats <- lm(data = tidy_cats, hwt ~ bwt)

tidy_cats %>%
  add_predictions(lm_cats) %>%
  ggplot(aes(x = bwt, y = hwt - pred)) +
  geom_point() +
  geom_smooth(method="lm", level = 0.99)
```

```{r, eval = FALSE}
# TODO add OLS method to this
# get weights of each response

# fit a linear model on the residuals
tidy_cats_res <- tidy_cats %>% 
  add_predictions(lm_cats) %>%
  mutate(res = hwt - pred) %>%
  select(bwt, hwt, res)
lm_cats_res <- lm(data = tidy_cats_res, res ~ hwt)

cat_weights <- tidy_cats_res %>%
  add_predictions(lm_cats_res) %>%
  mutate(res_var = (res-pred)^2) %>%
  mutate(weight = 1 / res_var) %>%
  pull(weight)

lm_cats_weights <- lm(data = tidy_cats, hwt ~ bwt, weights = cat_weights)

tidy_cats %>%
  gather_predictions(lm_cats, lm_cats_weights) %>%
  ggplot(aes(x = bwt, y = hwt - pred, colour = model)) +
  geom_point(aes(y = hwt-pred, colour = model)) +
  geom_smooth(method="lm")
```

When this occurs, there a few ways to remedy it. You could transform the response $Y$ using a function such as $logY$ or $\sqrt{Y}$. If we have a good idea of the variance of each response, we could fit our model using *weighted least squares*, which weights proportional to the inverse of the expected variance of an observation.

#### 4. Outliers

An *outlier* is a point for which $y_i$ is far from the value predicted by the model. These can arise for a variety of reasons, such as incorrect recording of an observation during data collection.


For the `msleep` dataset below, which contains data on mammal sleep durations, I've highlighted two observations that most would consider outliers. This is for the `African elephant` and `Asian elephant` mammals, who's bodyweights are far and away from the rest of mammals. There are others that could be considered outliers as well. Identifying outliers is an often arbitrary process.

```{r}
msleep %>%
  ggplot(aes(x = awake, y = bodywt, colour = name)) +
  geom_point() +
  gghighlight::gghighlight(bodywt>2000)
```

Let's fit a linear model to predict body weight from how long the animal is awake. 

Notice how the data that maintains the elephant observations significantly affects the slope, drawing the regression line away from the majority of observations.

```{r}
lm_sleep <- lm(data = msleep, bodywt ~ awake)
lm_sleep_filtered <- lm(data = msleep %>% filter(name != 'African elephant'), bodywt ~ awake)
msleep %>%
  gather_predictions(lm_sleep, lm_sleep_filtered) %>%
  ggplot(aes(x = awake, y = bodywt)) +
  geom_point(pch=21, alpha = 1/3) +
  geom_line(aes(y = pred, colour = model), size = 1.5) +
  scale_colour_viridis_d() +
  theme_minimal()
```

The model excluding the elephant observations has a significantly higher $R^2$, which indicates a better fit.

```{r}
models <- list(with_outliers = lm_sleep,
               without_outliers = lm_sleep_filtered)
purrr::map_df(models, broom::glance, .id = "model") %>%
  select(model, r.squared, statistic, p.value, df)
```

Another way of handling an outlier is transforming the response variable. Upon inspection of the scatterplot, it becomes clear that the relationship between `bodywt` and `awake` is not linear. If we take the same dataset and apply a `log` function to response variable `bodywt`, we see that the outliers no longer exists.

```{r}
msleep %>%
  ggplot(aes(x = awake, y = log(bodywt))) +
  geom_smooth(method = "lm") +
  geom_point()
```

The model that uses `log(bodywt)` as the response also has better performance than both models above.

```{r}
lm_sleep_log <- lm(data = msleep, log(bodywt) ~ awake)
lm_sleep_log %>% broom::glance() %>%
  select(r.squared, statistic, p.value, df)
```


#### 5. High Leverage Points

Observations with *high leverage* have an unusual value for $x_i$.








